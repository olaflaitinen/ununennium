{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Ununennium Documentation","text":"<p>Welcome to the Ununennium documentation. This comprehensive guide covers all aspects of the library, from installation to advanced model deployment.</p>"},{"location":"#documentation-map","title":"Documentation Map","text":"<pre><code>graph TB\n    subgraph \"Getting Started\"\n        QS[Quickstart Tutorial]\n        INST[Installation]\n    end\n\n    subgraph \"Core Concepts\"\n        ARCH[Architecture]\n        DM[Data Model]\n        PIPE[Pipelines]\n    end\n\n    subgraph \"API Reference\"\n        CORE[Core API]\n        IO[Data I/O]\n        MODELS[Models]\n        TRAIN[Training]\n    end\n\n    subgraph \"Tutorials\"\n        T1[Ingest &amp; Tiling]\n        T2[Train/Val/Test]\n        T3[Inference at Scale]\n        T4[Change Detection]\n        T5[Super-Resolution]\n        T6[GAN Recipes]\n        T7[PINN Recipes]\n    end\n\n    subgraph \"Advanced Topics\"\n        GUIDES[Guides]\n        RESEARCH[Research]\n    end\n\n    QS --&gt; ARCH\n    ARCH --&gt; API\n    API --&gt; Tutorials\n    Tutorials --&gt; GUIDES\n</code></pre>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"Section Description Audience Architecture System design and data flow All users API Reference Complete API documentation Developers Tutorials Step-by-step learning paths New users Guides Best practices and deep dives Intermediate Research Mathematical foundations Researchers"},{"location":"#architecture","title":"Architecture","text":"<p>Understand the design and components of Ununennium.</p> <ul> <li>System Overview - High-level architecture, component interactions</li> <li>Data Model - GeoTensor, GeoBatch, CRS handling</li> <li>Pipelines - Data flow from disk to GPU</li> <li>Performance - Benchmarks and optimization</li> <li>Security and Privacy - Data handling best practices</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>Detailed documentation for every public class and function.</p> <ul> <li>Overview - API design principles</li> <li>Core - GeoTensor, GeoBatch, types</li> <li>Data I/O - COG, STAC, Zarr readers</li> <li>Preprocessing - Indices, normalization</li> <li>Training - Trainer, callbacks</li> <li>Models - Model registry, architectures</li> <li>Evaluation - Metrics, validation</li> <li>GAN - Generative adversarial networks</li> <li>PINN - Physics-informed networks</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Step-by-step guides from beginner to advanced.</p> Tutorial Duration Description 00. Quickstart 15 min Zero to trained model 01. Ingest and Tiling 30 min Data preparation 02. Train/Val/Test 45 min Proper experimental design 03. Inference at Scale 30 min Production deployment 04. Change Detection 45 min Multi-temporal analysis 05. Super-Resolution 45 min Resolution enhancement 06. GAN Recipes 60 min Image translation 07. PINN Recipes 60 min Physics-constrained learning"},{"location":"#guides","title":"Guides","text":"<p>Best practices and detailed topic coverage.</p> <ul> <li>Datasets and Splits - Spatial cross-validation</li> <li>Reproducibility - Deterministic experiments</li> <li>Configuration - Config file reference</li> <li>Benchmarking - Performance testing</li> <li>Metrics - Metric selection guide</li> <li>Uncertainty and Calibration - Confidence estimation</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"#research","title":"Research","text":"<p>Mathematical and theoretical foundations.</p> <ul> <li>Math Foundations - Core mathematical concepts</li> <li>Remote Sensing Task Taxonomy - Task classification</li> <li>GAN Theory - Adversarial learning dynamics</li> <li>PINN Theory - Physics-constrained optimization</li> <li>Bibliography - Curated references</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Troubleshooting Guide - Common issues and solutions</li> <li>GitHub Issues - Bug reports</li> <li>GitHub Discussions - Questions</li> </ul>"},{"location":"api/core/","title":"Core API","text":"<p>The core module provides foundational data structures for geospatial machine learning.</p>"},{"location":"api/core/#geotensor","title":"GeoTensor","text":"<p>CRS-aware tensor that preserves geospatial metadata through operations.</p>"},{"location":"api/core/#class-definition","title":"Class Definition","text":"<pre><code>class GeoTensor:\n    \"\"\"A PyTorch tensor with geospatial metadata.\n\n    Parameters\n    ----------\n    data : torch.Tensor\n        The underlying tensor data. Shape: (C, H, W) or (H, W).\n    crs : str | CRS\n        Coordinate Reference System (e.g., \"EPSG:32632\").\n    transform : Affine\n        Affine transform from pixel to world coordinates.\n    nodata : float | None, optional\n        Value representing no data. Default: None.\n\n    Attributes\n    ----------\n    shape : tuple[int, ...]\n        Tensor shape.\n    bounds : BoundingBox\n        Geographic extent.\n    resolution : tuple[float, float]\n        Pixel size in CRS units (x, y).\n    dtype : torch.dtype\n        Data type.\n    device : torch.device\n        Device location.\n    \"\"\"\n</code></pre>"},{"location":"api/core/#constructor","title":"Constructor","text":"<pre><code>from ununennium.core import GeoTensor\nfrom affine import Affine\nimport torch\n\ndata = torch.randn(12, 512, 512)\ntransform = Affine.translation(500000, 4500000) * Affine.scale(10, -10)\n\ntensor = GeoTensor(\n    data=data,\n    crs=\"EPSG:32632\",\n    transform=transform,\n    nodata=-9999.0,\n)\n</code></pre>"},{"location":"api/core/#methods","title":"Methods","text":""},{"location":"api/core/#todevice","title":"<code>to(device)</code>","text":"<p>Move tensor to specified device.</p> <pre><code>gpu_tensor = tensor.to(\"cuda:0\")\ncpu_tensor = tensor.to(\"cpu\")\n</code></pre>"},{"location":"api/core/#numpy","title":"<code>numpy()</code>","text":"<p>Convert to NumPy array.</p> <pre><code>array = tensor.numpy()  # Returns np.ndarray\n</code></pre>"},{"location":"api/core/#reprojecttarget_crs-resolutionnone-methodbilinear","title":"<code>reproject(target_crs, resolution=None, method=\"bilinear\")</code>","text":"<p>Reproject to different CRS.</p> <pre><code>wgs84 = tensor.reproject(\n    target_crs=\"EPSG:4326\",\n    resolution=(0.0001, 0.0001),\n    method=\"bilinear\",\n)\n</code></pre> Parameter Type Description <code>target_crs</code> <code>str</code> Target CRS identifier <code>resolution</code> <code>tuple[float, float]</code> Output resolution <code>method</code> <code>str</code> Resampling method"},{"location":"api/core/#cropbounds","title":"<code>crop(bounds)</code>","text":"<p>Crop to bounding box.</p> <pre><code>from ununennium.core import BoundingBox\n\nbounds = BoundingBox(502000, 4496000, 504000, 4498000)\ncropped = tensor.crop(bounds)\n</code></pre>"},{"location":"api/core/#resampleresolution-methodbilinear","title":"<code>resample(resolution, method=\"bilinear\")</code>","text":"<p>Resample to new resolution.</p> <pre><code>resampled = tensor.resample(\n    resolution=(20.0, 20.0),\n    method=\"cubic\",\n)\n</code></pre>"},{"location":"api/core/#geobatch","title":"GeoBatch","text":"<p>Batch container for training with consistent metadata.</p>"},{"location":"api/core/#class-definition_1","title":"Class Definition","text":"<pre><code>class GeoBatch:\n    \"\"\"A batch of samples for training.\n\n    Parameters\n    ----------\n    images : torch.Tensor\n        Image batch. Shape: (B, C, H, W).\n    masks : torch.Tensor | None, optional\n        Label batch. Shape: (B, H, W) or (B, C, H, W).\n    crs : str | CRS\n        Common CRS for all samples.\n    transforms : list[Affine] | None, optional\n        Per-sample transforms.\n\n    Attributes\n    ----------\n    batch_size : int\n        Number of samples.\n    device : torch.device\n        Device location.\n    \"\"\"\n</code></pre>"},{"location":"api/core/#constructor_1","title":"Constructor","text":"<pre><code>from ununennium.core import GeoBatch\nimport torch\n\nbatch = GeoBatch(\n    images=torch.randn(8, 12, 256, 256),\n    masks=torch.randint(0, 10, (8, 256, 256)),\n    crs=\"EPSG:32632\",\n)\n</code></pre>"},{"location":"api/core/#methods_1","title":"Methods","text":""},{"location":"api/core/#todevice_1","title":"<code>to(device)</code>","text":"<pre><code>gpu_batch = batch.to(\"cuda:0\")\n</code></pre>"},{"location":"api/core/#pin_memory","title":"<code>pin_memory()</code>","text":"<p>Pin memory for faster GPU transfer.</p> <pre><code>pinned = batch.pin_memory()\n</code></pre>"},{"location":"api/core/#boundingbox","title":"BoundingBox","text":"<p>Geographic extent container.</p>"},{"location":"api/core/#class-definition_2","title":"Class Definition","text":"<pre><code>class BoundingBox:\n    \"\"\"Axis-aligned bounding box in CRS coordinates.\n\n    Parameters\n    ----------\n    left : float\n        Minimum x coordinate.\n    bottom : float\n        Minimum y coordinate.\n    right : float\n        Maximum x coordinate.\n    top : float\n        Maximum y coordinate.\n    \"\"\"\n</code></pre>"},{"location":"api/core/#properties","title":"Properties","text":"Property Type Description <code>width</code> <code>float</code> <code>right - left</code> <code>height</code> <code>float</code> <code>top - bottom</code> <code>center</code> <code>tuple[float, float]</code> Center point"},{"location":"api/core/#methods_2","title":"Methods","text":""},{"location":"api/core/#intersectsother","title":"<code>intersects(other)</code>","text":"<p>Check if bounding boxes overlap.</p> <pre><code>if bbox1.intersects(bbox2):\n    print(\"Boxes overlap\")\n</code></pre>"},{"location":"api/core/#intersectionother","title":"<code>intersection(other)</code>","text":"<p>Compute intersection.</p> <pre><code>overlap = bbox1.intersection(bbox2)\n</code></pre>"},{"location":"api/core/#unionother","title":"<code>union(other)</code>","text":"<p>Compute union.</p> <pre><code>combined = bbox1.union(bbox2)\n</code></pre>"},{"location":"api/core/#bufferdistance","title":"<code>buffer(distance)</code>","text":"<p>Expand by distance.</p> <pre><code>expanded = bbox.buffer(100)  # 100 CRS units\n</code></pre>"},{"location":"api/core/#type-definitions","title":"Type Definitions","text":"<pre><code>from ununennium.core.types import (\n    CRS,              # Union[str, pyproj.CRS]\n    Transform,        # affine.Affine\n    Resolution,       # tuple[float, float]\n    Shape,            # tuple[int, ...]\n)\n</code></pre>"},{"location":"api/core/#exceptions","title":"Exceptions","text":"<pre><code>from ununennium.core.exceptions import (\n    CRSError,         # Invalid CRS\n    BoundsError,      # Invalid bounds\n    TransformError,   # Invalid transform\n)\n</code></pre>"},{"location":"api/core/#example-error-handling","title":"Example Error Handling","text":"<pre><code>from ununennium.core import GeoTensor\nfrom ununennium.core.exceptions import CRSError\n\ntry:\n    tensor = GeoTensor(data, crs=\"INVALID\")\nexcept CRSError as e:\n    print(f\"CRS error: {e}\")\n</code></pre>"},{"location":"api/data-io/","title":"Data I/O API","text":"<p>The I/O module provides readers and writers for geospatial data formats.</p>"},{"location":"api/data-io/#geotiff-operations","title":"GeoTIFF Operations","text":""},{"location":"api/data-io/#read_geotiff","title":"read_geotiff","text":"<p>Read a GeoTIFF file as a GeoTensor.</p> <pre><code>def read_geotiff(\n    path: str | Path,\n    bands: list[int] | None = None,\n    window: BoundingBox | None = None,\n    overview_level: int | None = None,\n) -&gt; GeoTensor:\n    \"\"\"Read GeoTIFF file.\n\n    Parameters\n    ----------\n    path : str | Path\n        Path to GeoTIFF file or URL (http://, s3://).\n    bands : list[int] | None, optional\n        1-indexed band numbers to read. Default: all bands.\n    window : BoundingBox | None, optional\n        Spatial window to read. Default: entire image.\n    overview_level : int | None, optional\n        Overview level (0=full res). Default: auto-select.\n\n    Returns\n    -------\n    GeoTensor\n        Loaded data with metadata.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from ununennium.io import read_geotiff\n    &gt;&gt;&gt; tensor = read_geotiff(\"sentinel2.tif\")\n    &gt;&gt;&gt; tensor.shape\n    (12, 10980, 10980)\n    \"\"\"\n</code></pre>"},{"location":"api/data-io/#write_geotiff","title":"write_geotiff","text":"<p>Write a GeoTensor to GeoTIFF.</p> <pre><code>def write_geotiff(\n    tensor: GeoTensor,\n    path: str | Path,\n    compress: str = \"lzw\",\n    tiled: bool = True,\n    blocksize: int = 512,\n    cog: bool = True,\n) -&gt; None:\n    \"\"\"Write GeoTensor to GeoTIFF.\n\n    Parameters\n    ----------\n    tensor : GeoTensor\n        Data to write.\n    path : str | Path\n        Output path.\n    compress : str, optional\n        Compression method. Default: \"lzw\".\n    tiled : bool, optional\n        Write as tiled TIFF. Default: True.\n    blocksize : int, optional\n        Tile size. Default: 512.\n    cog : bool, optional\n        Write as Cloud Optimized GeoTIFF. Default: True.\n    \"\"\"\n</code></pre>"},{"location":"api/data-io/#stac-client","title":"STAC Client","text":"<p>Query SpatioTemporal Asset Catalog (STAC) APIs.</p>"},{"location":"api/data-io/#class-definition","title":"Class Definition","text":"<pre><code>class STACClient:\n    \"\"\"Client for STAC API queries.\n\n    Parameters\n    ----------\n    url : str\n        STAC API endpoint URL.\n    headers : dict | None, optional\n        Additional HTTP headers.\n    \"\"\"\n</code></pre>"},{"location":"api/data-io/#methods","title":"Methods","text":""},{"location":"api/data-io/#search","title":"<code>search</code>","text":"<pre><code>def search(\n    self,\n    bbox: tuple[float, float, float, float] | None = None,\n    datetime: str | None = None,\n    collections: list[str] | None = None,\n    query: dict | None = None,\n    limit: int = 100,\n) -&gt; Iterator[Item]:\n    \"\"\"Search STAC catalog.\n\n    Parameters\n    ----------\n    bbox : tuple, optional\n        Bounding box (west, south, east, north).\n    datetime : str, optional\n        Temporal query (e.g., \"2023-01-01/2023-12-31\").\n    collections : list[str], optional\n        Collection IDs to search.\n    query : dict, optional\n        Additional query parameters.\n    limit : int, optional\n        Maximum items per page. Default: 100.\n\n    Yields\n    ------\n    Item\n        STAC items matching query.\n    \"\"\"\n</code></pre>"},{"location":"api/data-io/#example","title":"Example","text":"<pre><code>from ununennium.io import STACClient\n\nclient = STACClient(\"https://earth-search.aws.element84.com/v1\")\n\nitems = client.search(\n    bbox=(9.0, 45.0, 10.0, 46.0),\n    datetime=\"2023-06-01/2023-06-30\",\n    collections=[\"sentinel-2-l2a\"],\n    query={\"eo:cloud_cover\": {\"lt\": 20}},\n)\n\nfor item in items:\n    tensor = client.read_item(item, assets=[\"B04\", \"B03\", \"B02\"])\n    print(f\"Loaded: {tensor.shape}\")\n</code></pre>"},{"location":"api/data-io/#zarr-operations","title":"Zarr Operations","text":""},{"location":"api/data-io/#read_zarr","title":"read_zarr","text":"<pre><code>def read_zarr(\n    path: str | Path,\n    chunks: tuple[int, ...] | None = None,\n) -&gt; GeoTensor:\n    \"\"\"Read Zarr store as GeoTensor.\n\n    Parameters\n    ----------\n    path : str | Path\n        Path to Zarr store.\n    chunks : tuple, optional\n        Chunk size for lazy loading.\n\n    Returns\n    -------\n    GeoTensor\n        Lazy-loaded data.\n    \"\"\"\n</code></pre>"},{"location":"api/data-io/#write_zarr","title":"write_zarr","text":"<pre><code>def write_zarr(\n    tensor: GeoTensor,\n    path: str | Path,\n    chunks: tuple[int, ...] = (1, 512, 512),\n    compressor: str = \"zstd\",\n) -&gt; None:\n    \"\"\"Write GeoTensor to Zarr store.\n\n    Parameters\n    ----------\n    tensor : GeoTensor\n        Data to write.\n    path : str | Path\n        Output path.\n    chunks : tuple, optional\n        Chunk size. Default: (1, 512, 512).\n    compressor : str, optional\n        Compression codec. Default: \"zstd\".\n    \"\"\"\n</code></pre>"},{"location":"api/data-io/#streaming-operations","title":"Streaming Operations","text":""},{"location":"api/data-io/#stream_geotiff","title":"stream_geotiff","text":"<pre><code>def stream_geotiff(\n    path: str | Path,\n    chunk_size: int = 1024,\n    overlap: int = 0,\n) -&gt; Iterator[GeoTensor]:\n    \"\"\"Stream large GeoTIFF in chunks.\n\n    Parameters\n    ----------\n    path : str | Path\n        Path to GeoTIFF.\n    chunk_size : int, optional\n        Chunk size in pixels. Default: 1024.\n    overlap : int, optional\n        Overlap between chunks. Default: 0.\n\n    Yields\n    ------\n    GeoTensor\n        Chunk with metadata.\n    \"\"\"\n</code></pre>"},{"location":"api/data-io/#example_1","title":"Example","text":"<pre><code>from ununennium.io import stream_geotiff\n\nfor chunk in stream_geotiff(\"large_raster.tif\", chunk_size=512):\n    # Process chunk\n    result = model(chunk.data.unsqueeze(0))\n</code></pre>"},{"location":"api/data-io/#format-support","title":"Format Support","text":"Format Read Write Streaming Cloud-Native GeoTIFF Yes Yes Yes Partial COG Yes Yes Yes Yes Zarr Yes Yes Yes Yes NetCDF Yes No Partial No HDF5 Yes No Partial No"},{"location":"api/evaluation/","title":"Evaluation API","text":"<p>The evaluation module provides metrics for model assessment and calibration analysis.</p>"},{"location":"api/evaluation/#segmentation-metrics","title":"Segmentation Metrics","text":""},{"location":"api/evaluation/#iou-intersection-over-union","title":"IoU (Intersection over Union)","text":"<pre><code>class IoU(Metric):\n    \"\"\"Intersection over Union metric.\n\n    Parameters\n    ----------\n    num_classes : int\n        Number of classes.\n    ignore_index : int | None, optional\n        Class index to ignore. Default: None.\n    average : str, optional\n        Averaging mode: \"micro\", \"macro\", \"weighted\". Default: \"macro\".\n    \"\"\"\n</code></pre> \\[ \\text{IoU}_c = \\frac{\\text{TP}_c}{\\text{TP}_c + \\text{FP}_c + \\text{FN}_c} \\]"},{"location":"api/evaluation/#dice-coefficient","title":"Dice Coefficient","text":"<pre><code>class Dice(Metric):\n    \"\"\"Dice coefficient (F1 score).\n\n    Parameters\n    ----------\n    num_classes : int\n        Number of classes.\n    smooth : float, optional\n        Smoothing factor. Default: 1e-6.\n    \"\"\"\n</code></pre> \\[ \\text{Dice}_c = \\frac{2 \\cdot \\text{TP}_c}{2 \\cdot \\text{TP}_c + \\text{FP}_c + \\text{FN}_c} \\]"},{"location":"api/evaluation/#example","title":"Example","text":"<pre><code>from ununennium.metrics import IoU, Dice\n\niou = IoU(num_classes=10)\ndice = Dice(num_classes=10)\n\n# Update with predictions\niou.update(predictions, targets)\ndice.update(predictions, targets)\n\n# Compute final metrics\nprint(f\"mIoU: {iou.compute():.4f}\")\nprint(f\"mDice: {dice.compute():.4f}\")\n</code></pre>"},{"location":"api/evaluation/#classification-metrics","title":"Classification Metrics","text":""},{"location":"api/evaluation/#accuracy","title":"Accuracy","text":"<pre><code>class Accuracy(Metric):\n    \"\"\"Classification accuracy.\n\n    Parameters\n    ----------\n    task : str\n        Task type: \"binary\", \"multiclass\", \"multilabel\".\n    num_classes : int | None, optional\n        Number of classes (required for multiclass).\n    \"\"\"\n</code></pre>"},{"location":"api/evaluation/#f1score","title":"F1Score","text":"<pre><code>class F1Score(Metric):\n    \"\"\"F1 score.\n\n    Parameters\n    ----------\n    num_classes : int\n        Number of classes.\n    average : str, optional\n        Averaging: \"micro\", \"macro\", \"weighted\". Default: \"macro\".\n    \"\"\"\n</code></pre>"},{"location":"api/evaluation/#calibration-metrics","title":"Calibration Metrics","text":""},{"location":"api/evaluation/#expected-calibration-error-ece","title":"Expected Calibration Error (ECE)","text":"<pre><code>class ECE(Metric):\n    \"\"\"Expected Calibration Error.\n\n    Parameters\n    ----------\n    n_bins : int, optional\n        Number of confidence bins. Default: 15.\n    \"\"\"\n</code></pre> \\[ \\text{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{n} \\left| \\text{acc}(B_m) - \\text{conf}(B_m) \\right| \\] <p>where \\(B_m\\) is the set of samples in bin \\(m\\).</p>"},{"location":"api/evaluation/#brier-score","title":"Brier Score","text":"<pre><code>class BrierScore(Metric):\n    \"\"\"Brier score for probabilistic predictions.\n\n    Parameters\n    ----------\n    num_classes : int\n        Number of classes.\n    \"\"\"\n</code></pre> \\[ \\text{Brier} = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} (p_{ic} - y_{ic})^2 \\]"},{"location":"api/evaluation/#example_1","title":"Example","text":"<pre><code>from ununennium.metrics import ECE, BrierScore\n\nece = ECE(n_bins=15)\nbrier = BrierScore(num_classes=10)\n\n# probabilities: (N, C) - class probabilities\n# targets: (N,) - ground truth class indices\nece.update(probabilities, targets)\nbrier.update(probabilities, targets)\n\nprint(f\"ECE: {ece.compute():.4f}\")\nprint(f\"Brier: {brier.compute():.4f}\")\n</code></pre>"},{"location":"api/evaluation/#detection-metrics","title":"Detection Metrics","text":""},{"location":"api/evaluation/#mean-average-precision-map","title":"Mean Average Precision (mAP)","text":"<pre><code>class MeanAP(Metric):\n    \"\"\"Mean Average Precision.\n\n    Parameters\n    ----------\n    iou_thresholds : list[float], optional\n        IoU thresholds. Default: [0.5, 0.75].\n    \"\"\"\n</code></pre>"},{"location":"api/evaluation/#change-detection-metrics","title":"Change Detection Metrics","text":""},{"location":"api/evaluation/#kappa-coefficient","title":"Kappa Coefficient","text":"<pre><code>class KappaCoefficient(Metric):\n    \"\"\"Cohen's Kappa for change detection.\n\n    Parameters\n    ----------\n    num_classes : int\n        Number of classes (including no-change).\n    \"\"\"\n</code></pre> \\[ \\kappa = \\frac{p_o - p_e}{1 - p_e} \\] <p>where \\(p_o\\) is observed agreement and \\(p_e\\) is expected agreement.</p>"},{"location":"api/evaluation/#super-resolution-metrics","title":"Super-Resolution Metrics","text":""},{"location":"api/evaluation/#psnr","title":"PSNR","text":"<pre><code>class PSNR(Metric):\n    \"\"\"Peak Signal-to-Noise Ratio.\"\"\"\n</code></pre> \\[ \\text{PSNR} = 10 \\cdot \\log_{10}\\left(\\frac{\\text{MAX}^2}{\\text{MSE}}\\right) \\]"},{"location":"api/evaluation/#ssim","title":"SSIM","text":"<pre><code>class SSIM(Metric):\n    \"\"\"Structural Similarity Index.\"\"\"\n</code></pre>"},{"location":"api/evaluation/#metric-aggregation","title":"Metric Aggregation","text":""},{"location":"api/evaluation/#metriccollection","title":"MetricCollection","text":"<pre><code>from ununennium.metrics import MetricCollection, IoU, Dice\n\nmetrics = MetricCollection([\n    IoU(num_classes=10),\n    Dice(num_classes=10),\n])\n\n# Update all metrics at once\nmetrics.update(predictions, targets)\n\n# Compute all metrics\nresults = metrics.compute()\n# {\"IoU\": 0.78, \"Dice\": 0.85}\n</code></pre>"},{"location":"api/evaluation/#bootstrap-confidence-intervals","title":"Bootstrap Confidence Intervals","text":"<pre><code>from ununennium.metrics import bootstrap_ci\n\n# Compute 95% confidence interval\nmean, (low, high) = bootstrap_ci(iou, n_bootstrap=1000, confidence=0.95)\nprint(f\"mIoU: {mean:.4f} [{low:.4f}, {high:.4f}]\")\n</code></pre>"},{"location":"api/gan/","title":"GAN API","text":"<p>The GAN module provides generative adversarial network architectures for image translation and super-resolution.</p>"},{"location":"api/gan/#pix2pix","title":"Pix2Pix","text":"<p>Paired image-to-image translation.</p>"},{"location":"api/gan/#class-definition","title":"Class Definition","text":"<pre><code>class Pix2Pix(nn.Module):\n    \"\"\"Pix2Pix GAN for paired image translation.\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    ngf : int, optional\n        Generator feature maps. Default: 64.\n    ndf : int, optional\n        Discriminator feature maps. Default: 64.\n    n_layers : int, optional\n        PatchGAN discriminator depth. Default: 3.\n    lambda_l1 : float, optional\n        L1 reconstruction weight. Default: 100.0.\n    \"\"\"\n</code></pre>"},{"location":"api/gan/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    subgraph \"Generator (U-Net)\"\n        E1[Encoder 1] --&gt; E2[Encoder 2] --&gt; E3[Encoder 3]\n        E3 --&gt; D3[Decoder 3] --&gt; D2[Decoder 2] --&gt; D1[Decoder 1]\n        E1 -.-&gt; D1\n        E2 -.-&gt; D2\n    end\n\n    subgraph \"Discriminator (PatchGAN)\"\n        C1[Conv 1] --&gt; C2[Conv 2] --&gt; C3[Conv 3] --&gt; OUT[Patch Output]\n    end\n</code></pre>"},{"location":"api/gan/#training","title":"Training","text":"<p>The total generator loss:</p> \\[ \\mathcal{L}_G = \\mathcal{L}_{GAN}(G, D) + \\lambda \\cdot \\mathcal{L}_{L1}(G) \\] <pre><code>from ununennium.models.gan import Pix2Pix\n\nmodel = Pix2Pix(\n    in_channels=2,   # SAR: VV, VH\n    out_channels=3,  # Optical: RGB\n    ngf=64,\n    ndf=64,\n    lambda_l1=100.0,\n)\n\n# Training step\ng_loss, d_loss = model.training_step(sar_batch, optical_batch)\n</code></pre>"},{"location":"api/gan/#cyclegan","title":"CycleGAN","text":"<p>Unpaired image-to-image translation.</p>"},{"location":"api/gan/#class-definition_1","title":"Class Definition","text":"<pre><code>class CycleGAN(nn.Module):\n    \"\"\"CycleGAN for unpaired image translation.\n\n    Parameters\n    ----------\n    in_channels_a : int\n        Channels for domain A.\n    in_channels_b : int\n        Channels for domain B.\n    ngf : int, optional\n        Generator feature maps. Default: 64.\n    ndf : int, optional\n        Discriminator feature maps. Default: 64.\n    n_residual : int, optional\n        Number of residual blocks. Default: 9.\n    lambda_cycle : float, optional\n        Cycle consistency weight. Default: 10.0.\n    lambda_identity : float, optional\n        Identity loss weight. Default: 0.5.\n    \"\"\"\n</code></pre>"},{"location":"api/gan/#architecture_1","title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"Domain A to B\"\n        A[Image A] --&gt; GAB[Generator A\u2192B] --&gt; B_fake[Fake B]\n        B_fake --&gt; GBA_cycle[Generator B\u2192A] --&gt; A_rec[Reconstructed A]\n    end\n\n    subgraph \"Domain B to A\"\n        B[Image B] --&gt; GBA[Generator B\u2192A] --&gt; A_fake[Fake A]\n        A_fake --&gt; GAB_cycle[Generator A\u2192B] --&gt; B_rec[Reconstructed B]\n    end\n\n    subgraph \"Discriminators\"\n        B_fake --&gt; DB[Discriminator B]\n        A_fake --&gt; DA[Discriminator A]\n    end\n</code></pre>"},{"location":"api/gan/#cycle-consistency-loss","title":"Cycle Consistency Loss","text":"\\[ \\mathcal{L}_{cyc} = \\mathbb{E}\\left[\\|G_{BA}(G_{AB}(a)) - a\\|_1\\right] + \\mathbb{E}\\left[\\|G_{AB}(G_{BA}(b)) - b\\|_1\\right] \\] <pre><code>from ununennium.models.gan import CycleGAN\n\nmodel = CycleGAN(\n    in_channels_a=3,  # Summer images\n    in_channels_b=3,  # Winter images\n    lambda_cycle=10.0,\n)\n\nlosses = model.training_step(summer_batch, winter_batch)\n</code></pre>"},{"location":"api/gan/#esrgan","title":"ESRGAN","text":"<p>Enhanced Super-Resolution GAN.</p>"},{"location":"api/gan/#class-definition_2","title":"Class Definition","text":"<pre><code>class ESRGAN(nn.Module):\n    \"\"\"ESRGAN for super-resolution.\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of input channels.\n    scale_factor : int, optional\n        Upscaling factor. Default: 4.\n    n_rrdb : int, optional\n        Number of RRDB blocks. Default: 23.\n    \"\"\"\n</code></pre>"},{"location":"api/gan/#perceptual-loss","title":"Perceptual Loss","text":"\\[ \\mathcal{L}_{percep} = \\sum_{l} \\frac{1}{C_l H_l W_l} \\|\\phi_l(I_{HR}) - \\phi_l(G(I_{LR}))\\|_1 \\] <p>where \\(\\phi_l\\) are VGG feature maps.</p>"},{"location":"api/gan/#gan-losses","title":"GAN Losses","text":""},{"location":"api/gan/#adversarialloss","title":"AdversarialLoss","text":"<pre><code>class AdversarialLoss(nn.Module):\n    \"\"\"GAN adversarial loss.\n\n    Parameters\n    ----------\n    mode : str\n        Loss mode: \"vanilla\", \"lsgan\", \"wgan\", \"hinge\".\n    \"\"\"\n</code></pre> Mode Discriminator Loss Generator Loss <code>vanilla</code> \\(-\\log(D(x)) - \\log(1-D(G(z)))\\) \\(-\\log(D(G(z)))\\) <code>lsgan</code> \\((D(x)-1)^2 + D(G(z))^2\\) \\((D(G(z))-1)^2\\) <code>hinge</code> \\(\\max(0, 1-D(x)) + \\max(0, 1+D(G(z)))\\) \\(-D(G(z))\\)"},{"location":"api/gan/#perceptualloss","title":"PerceptualLoss","text":"<pre><code>class PerceptualLoss(nn.Module):\n    \"\"\"VGG-based perceptual loss.\n\n    Parameters\n    ----------\n    layers : list[str]\n        VGG layers to use.\n    weights : list[float] | None, optional\n        Per-layer weights.\n    \"\"\"\n</code></pre>"},{"location":"api/gan/#training-utilities","title":"Training Utilities","text":""},{"location":"api/gan/#gantrainer","title":"GANTrainer","text":"<pre><code>from ununennium.training import GANTrainer\n\ntrainer = GANTrainer(\n    generator=model.generator,\n    discriminator=model.discriminator,\n    g_optimizer=g_optimizer,\n    d_optimizer=d_optimizer,\n    n_critic=1,  # D updates per G update\n)\n\nhistory = trainer.fit(train_loader, epochs=100)\n</code></pre>"},{"location":"api/models/","title":"Models API","text":"<p>The models module provides a registry of neural network architectures for Earth observation tasks.</p>"},{"location":"api/models/#model-registry","title":"Model Registry","text":""},{"location":"api/models/#create_model","title":"create_model","text":"<pre><code>def create_model(\n    name: str,\n    in_channels: int = 3,\n    num_classes: int = 10,\n    pretrained: bool = False,\n    **kwargs,\n) -&gt; nn.Module:\n    \"\"\"Create model from registry.\n\n    Parameters\n    ----------\n    name : str\n        Model name (e.g., \"unet_resnet50\").\n    in_channels : int, optional\n        Number of input channels. Default: 3.\n    num_classes : int, optional\n        Number of output classes. Default: 10.\n    pretrained : bool, optional\n        Load pretrained weights. Default: False.\n    **kwargs\n        Model-specific parameters.\n\n    Returns\n    -------\n    nn.Module\n        Initialized model.\n    \"\"\"\n</code></pre>"},{"location":"api/models/#list_models","title":"list_models","text":"<pre><code>def list_models(task: str | None = None) -&gt; list[str]:\n    \"\"\"List available models.\n\n    Parameters\n    ----------\n    task : str | None, optional\n        Filter by task: \"segmentation\", \"classification\", \"detection\".\n\n    Returns\n    -------\n    list[str]\n        Available model names.\n    \"\"\"\n</code></pre>"},{"location":"api/models/#example","title":"Example","text":"<pre><code>from ununennium.models import create_model, list_models\n\n# List segmentation models\nprint(list_models(task=\"segmentation\"))\n# [\"unet_resnet50\", \"unet_efficientnet_b4\", \"deeplabv3_resnet101\", ...]\n\n# Create model\nmodel = create_model(\n    \"unet_resnet50\",\n    in_channels=12,\n    num_classes=10,\n    pretrained=True,\n)\n</code></pre>"},{"location":"api/models/#segmentation-architectures","title":"Segmentation Architectures","text":""},{"location":"api/models/#u-net","title":"U-Net","text":"<pre><code>class UNet(nn.Module):\n    \"\"\"U-Net architecture with configurable backbone.\n\n    Parameters\n    ----------\n    backbone : str\n        Encoder backbone name.\n    in_channels : int\n        Number of input channels.\n    num_classes : int\n        Number of output classes.\n    decoder_channels : list[int], optional\n        Decoder channel sizes. Default: [256, 128, 64, 32].\n    \"\"\"\n</code></pre> <p>Available U-Net variants:</p> Model Name Backbone Parameters <code>unet_resnet18</code> ResNet-18 14M <code>unet_resnet50</code> ResNet-50 32M <code>unet_efficientnet_b4</code> EfficientNet-B4 19M"},{"location":"api/models/#deeplabv3","title":"DeepLabV3+","text":"<pre><code>class DeepLabV3Plus(nn.Module):\n    \"\"\"DeepLabV3+ with atrous spatial pyramid pooling.\n\n    Parameters\n    ----------\n    backbone : str\n        Encoder backbone name.\n    in_channels : int\n        Number of input channels.\n    num_classes : int\n        Number of output classes.\n    output_stride : int, optional\n        Output stride. Default: 16.\n    aspp_dilations : list[int], optional\n        ASPP dilation rates. Default: [6, 12, 18].\n    \"\"\"\n</code></pre>"},{"location":"api/models/#backbones","title":"Backbones","text":""},{"location":"api/models/#available-backbones","title":"Available Backbones","text":"Name Type Pretrained <code>resnet18</code> CNN ImageNet <code>resnet50</code> CNN ImageNet <code>resnet101</code> CNN ImageNet <code>efficientnet_b0</code> CNN ImageNet <code>efficientnet_b4</code> CNN ImageNet <code>vit_base_patch16</code> ViT ImageNet-21k <code>swin_tiny</code> Swin ImageNet"},{"location":"api/models/#custom-backbone-registration","title":"Custom Backbone Registration","text":"<pre><code>from ununennium.models import register_backbone\n\n@register_backbone(\"my_backbone\")\nclass MyBackbone(nn.Module):\n    def __init__(self, in_channels: int = 3):\n        super().__init__()\n        # Implementation\n\n    def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n        # Return multi-scale features\n        return [f1, f2, f3, f4]\n</code></pre>"},{"location":"api/models/#task-heads","title":"Task Heads","text":""},{"location":"api/models/#segmentationhead","title":"SegmentationHead","text":"<pre><code>class SegmentationHead(nn.Module):\n    \"\"\"Segmentation output head.\n\n    Parameters\n    ----------\n    in_channels : int\n        Input feature channels.\n    num_classes : int\n        Number of output classes.\n    kernel_size : int, optional\n        Final convolution kernel size. Default: 3.\n    \"\"\"\n</code></pre>"},{"location":"api/models/#classificationhead","title":"ClassificationHead","text":"<pre><code>class ClassificationHead(nn.Module):\n    \"\"\"Classification output head.\n\n    Parameters\n    ----------\n    in_channels : int\n        Input feature channels.\n    num_classes : int\n        Number of output classes.\n    pooling : str, optional\n        Pooling type: \"avg\", \"max\". Default: \"avg\".\n    \"\"\"\n</code></pre>"},{"location":"api/models/#model-configuration","title":"Model Configuration","text":""},{"location":"api/models/#from-config-dictionary","title":"From Config Dictionary","text":"<pre><code>config = {\n    \"name\": \"unet_resnet50\",\n    \"in_channels\": 12,\n    \"num_classes\": 10,\n    \"decoder_channels\": [256, 128, 64, 32],\n}\n\nmodel = create_model(**config)\n</code></pre>"},{"location":"api/models/#from-yaml","title":"From YAML","text":"<pre><code>model:\n  name: unet_resnet50\n  in_channels: 12\n  num_classes: 10\n  pretrained: true\n</code></pre> <pre><code>import yaml\nfrom ununennium.models import create_model\n\nwith open(\"config.yaml\") as f:\n    config = yaml.safe_load(f)\n\nmodel = create_model(**config[\"model\"])\n</code></pre>"},{"location":"api/overview/","title":"API Overview","text":"<p>This document provides an overview of the Ununennium API design principles and module organization.</p>"},{"location":"api/overview/#api-design-principles","title":"API Design Principles","text":"Principle Description Explicit over implicit CRS and transforms are never guessed Fail fast Invalid inputs raise immediately Composable Components work together cleanly Type-safe Full type annotations for IDE support Documented Every public API has docstrings"},{"location":"api/overview/#module-organization","title":"Module Organization","text":"<pre><code>graph TB\n    subgraph \"ununennium\"\n        CORE[core]\n        IO[io]\n        PREP[preprocessing]\n        AUG[augmentation]\n        TILE[tiling]\n        DS[datasets]\n        MOD[models]\n        LOSS[losses]\n        MET[metrics]\n        TRAIN[training]\n        EXP[export]\n        SENS[sensors]\n    end\n\n    CORE --&gt; IO\n    IO --&gt; PREP\n    PREP --&gt; AUG\n    AUG --&gt; TILE\n    TILE --&gt; DS\n    DS --&gt; MOD\n    MOD --&gt; LOSS\n    LOSS --&gt; MET\n    MET --&gt; TRAIN\n    TRAIN --&gt; EXP\n</code></pre>"},{"location":"api/overview/#quick-reference","title":"Quick Reference","text":""},{"location":"api/overview/#core","title":"Core","text":"<pre><code>from ununennium.core import GeoTensor, GeoBatch, BoundingBox\n</code></pre> Class Purpose <code>GeoTensor</code> CRS-aware tensor wrapper <code>GeoBatch</code> Batch of samples for training <code>BoundingBox</code> Geographic extent container"},{"location":"api/overview/#io","title":"I/O","text":"<pre><code>from ununennium.io import read_geotiff, write_geotiff, STACClient\n</code></pre> Function/Class Purpose <code>read_geotiff</code> Load GeoTIFF as GeoTensor <code>write_geotiff</code> Save GeoTensor to GeoTIFF <code>STACClient</code> Query STAC catalogs"},{"location":"api/overview/#models","title":"Models","text":"<pre><code>from ununennium.models import create_model, list_models\nfrom ununennium.models.gan import Pix2Pix, CycleGAN\nfrom ununennium.models.pinn import PINN\n</code></pre> Function Purpose <code>create_model</code> Factory for registered models <code>list_models</code> List available architectures"},{"location":"api/overview/#training","title":"Training","text":"<pre><code>from ununennium.training import Trainer, CheckpointCallback, EarlyStoppingCallback\n</code></pre> Class Purpose <code>Trainer</code> Training loop management <code>CheckpointCallback</code> Save model checkpoints <code>EarlyStoppingCallback</code> Stop on plateau"},{"location":"api/overview/#import-conventions","title":"Import Conventions","text":"<pre><code># Recommended: Import specific functions\nfrom ununennium.io import read_geotiff\nfrom ununennium.models import create_model\n\n# Alternative: Module-level import\nimport ununennium as uu\ntensor = uu.io.read_geotiff(\"image.tif\")\n</code></pre>"},{"location":"api/overview/#error-handling","title":"Error Handling","text":"<p>All errors inherit from base exception types:</p> <pre><code>from ununennium.core.exceptions import (\n    UnunenniumError,      # Base exception\n    CRSError,             # Invalid CRS\n    DataError,            # Data loading issues\n    ConfigError,          # Configuration problems\n)\n</code></pre>"},{"location":"api/overview/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>Core - GeoTensor, GeoBatch, types</li> <li>Data I/O - File and cloud I/O</li> <li>Preprocessing - Indices, normalization</li> <li>Training - Trainer and callbacks</li> <li>Models - Architecture registry</li> <li>Evaluation - Metrics and validation</li> <li>GAN - Generative adversarial networks</li> <li>PINN - Physics-informed networks</li> </ul>"},{"location":"api/pinn/","title":"PINN API","text":"<p>The PINN module provides physics-informed neural network components for scientific applications.</p>"},{"location":"api/pinn/#overview","title":"Overview","text":"<p>Physics-Informed Neural Networks (PINNs) incorporate physical laws as constraints in the training objective.</p> <pre><code>graph LR\n    subgraph \"PINN Architecture\"\n        INPUT[Coordinates x, t] --&gt; NET[Neural Network]\n        NET --&gt; OUTPUT[Solution u]\n\n        OUTPUT --&gt; DATA[Data Loss]\n        OUTPUT --&gt; PDE[PDE Residual]\n        OUTPUT --&gt; BC[Boundary Conditions]\n    end\n\n    DATA --&gt; TOTAL[Total Loss]\n    PDE --&gt; TOTAL\n    BC --&gt; TOTAL\n</code></pre>"},{"location":"api/pinn/#total-loss","title":"Total Loss","text":"\\[ \\mathcal{L} = \\lambda_d \\mathcal{L}_{data} + \\lambda_p \\mathcal{L}_{pde} + \\lambda_b \\mathcal{L}_{bc} + \\lambda_i \\mathcal{L}_{ic} \\]"},{"location":"api/pinn/#pinn-class","title":"PINN Class","text":""},{"location":"api/pinn/#class-definition","title":"Class Definition","text":"<pre><code>class PINN(nn.Module):\n    \"\"\"Physics-Informed Neural Network.\n\n    Parameters\n    ----------\n    network : nn.Module\n        Neural network approximator.\n    equation : PDEEquation\n        PDE to enforce.\n    lambda_pde : float, optional\n        PDE residual weight. Default: 1.0.\n    lambda_bc : float, optional\n        Boundary condition weight. Default: 1.0.\n    lambda_ic : float, optional\n        Initial condition weight. Default: 1.0.\n    \"\"\"\n</code></pre>"},{"location":"api/pinn/#methods","title":"Methods","text":""},{"location":"api/pinn/#compute_loss","title":"<code>compute_loss</code>","text":"<pre><code>def compute_loss(\n    self,\n    x_data: torch.Tensor,\n    u_data: torch.Tensor,\n    x_collocation: torch.Tensor,\n    x_bc: torch.Tensor | None = None,\n    u_bc: torch.Tensor | None = None,\n    x_ic: torch.Tensor | None = None,\n    u_ic: torch.Tensor | None = None,\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Compute all loss components.\n\n    Returns\n    -------\n    dict[str, torch.Tensor]\n        Loss components: \"data\", \"pde\", \"bc\", \"ic\", \"total\".\n    \"\"\"\n</code></pre>"},{"location":"api/pinn/#example","title":"Example","text":"<pre><code>from ununennium.models.pinn import PINN, DiffusionEquation, MLP\n\n# Define network\nnetwork = MLP(layers=[2, 128, 128, 128, 1], activation=\"tanh\")\n\n# Define PDE\nequation = DiffusionEquation(diffusivity=0.1)\n\n# Create PINN\npinn = PINN(\n    network=network,\n    equation=equation,\n    lambda_pde=10.0,\n)\n\n# Compute loss\nlosses = pinn.compute_loss(x_data, u_data, x_collocation)\ntotal_loss = losses[\"total\"]\n</code></pre>"},{"location":"api/pinn/#network-architectures","title":"Network Architectures","text":""},{"location":"api/pinn/#mlp","title":"MLP","text":"<pre><code>class MLP(nn.Module):\n    \"\"\"Multi-layer perceptron for PINN.\n\n    Parameters\n    ----------\n    layers : list[int]\n        Layer sizes including input and output.\n    activation : str, optional\n        Activation function: \"tanh\", \"relu\", \"silu\". Default: \"tanh\".\n    \"\"\"\n</code></pre>"},{"location":"api/pinn/#fouriermlp","title":"FourierMLP","text":"<pre><code>class FourierMLP(nn.Module):\n    \"\"\"MLP with Fourier feature encoding.\n\n    Parameters\n    ----------\n    in_features : int\n        Input dimension.\n    layers : list[int]\n        Hidden and output layer sizes.\n    sigma : float, optional\n        Fourier feature scale. Default: 1.0.\n    n_frequencies : int, optional\n        Number of Fourier frequencies. Default: 256.\n    \"\"\"\n</code></pre> <p>Fourier features help learn high-frequency solutions:</p> \\[ \\gamma(x) = \\left[\\cos(2\\pi B x), \\sin(2\\pi B x)\\right]^T \\] <p>where \\(B\\) is sampled from \\(\\mathcal{N}(0, \\sigma^2)\\).</p>"},{"location":"api/pinn/#pde-equations","title":"PDE Equations","text":""},{"location":"api/pinn/#diffusionequation","title":"DiffusionEquation","text":"<pre><code>class DiffusionEquation(PDEEquation):\n    \"\"\"Heat/diffusion equation.\n\n    PDE: du/dt = D * nabla^2(u)\n\n    Parameters\n    ----------\n    diffusivity : float\n        Diffusion coefficient D.\n    \"\"\"\n</code></pre> \\[ \\frac{\\partial u}{\\partial t} = D \\nabla^2 u \\]"},{"location":"api/pinn/#advectionequation","title":"AdvectionEquation","text":"<pre><code>class AdvectionEquation(PDEEquation):\n    \"\"\"Advection equation.\n\n    PDE: du/dt + v * grad(u) = 0\n\n    Parameters\n    ----------\n    velocity : tuple[float, float]\n        Velocity field (vx, vy).\n    \"\"\"\n</code></pre> \\[ \\frac{\\partial u}{\\partial t} + \\mathbf{v} \\cdot \\nabla u = 0 \\]"},{"location":"api/pinn/#advectiondiffusionequation","title":"AdvectionDiffusionEquation","text":"<pre><code>class AdvectionDiffusionEquation(PDEEquation):\n    \"\"\"Combined advection-diffusion.\n\n    Parameters\n    ----------\n    diffusivity : float\n        Diffusion coefficient.\n    velocity : tuple[float, float]\n        Velocity field.\n    \"\"\"\n</code></pre> \\[ \\frac{\\partial u}{\\partial t} + \\mathbf{v} \\cdot \\nabla u = D \\nabla^2 u \\]"},{"location":"api/pinn/#collocation-samplers","title":"Collocation Samplers","text":""},{"location":"api/pinn/#uniformsampler","title":"UniformSampler","text":"<pre><code>class UniformSampler:\n    \"\"\"Uniform random sampling in domain.\n\n    Parameters\n    ----------\n    bounds : list[tuple[float, float]]\n        Bounds for each dimension [(min, max), ...].\n    n_points : int\n        Number of points to sample.\n    \"\"\"\n</code></pre>"},{"location":"api/pinn/#latinhypercubesampler","title":"LatinHypercubeSampler","text":"<pre><code>class LatinHypercubeSampler:\n    \"\"\"Latin Hypercube Sampling for better coverage.\n\n    Parameters\n    ----------\n    bounds : list[tuple[float, float]]\n        Bounds for each dimension.\n    n_points : int\n        Number of points to sample.\n    \"\"\"\n</code></pre>"},{"location":"api/pinn/#example_1","title":"Example","text":"<pre><code>from ununennium.models.pinn import UniformSampler, LatinHypercubeSampler\n\n# Uniform sampling\nuniform = UniformSampler(bounds=[(0, 1), (0, 1)], n_points=10000)\nx_uniform = uniform.sample()\n\n# LHS for better coverage\nlhs = LatinHypercubeSampler(bounds=[(0, 1), (0, 1)], n_points=1000)\nx_lhs = lhs.sample()\n</code></pre>"},{"location":"api/pinn/#boundary-conditions","title":"Boundary Conditions","text":""},{"location":"api/pinn/#dirichletbc","title":"DirichletBC","text":"<pre><code>class DirichletBC:\n    \"\"\"Dirichlet (fixed value) boundary condition.\n\n    Parameters\n    ----------\n    value : float | Callable\n        Boundary value or function.\n    \"\"\"\n</code></pre>"},{"location":"api/pinn/#neumannbc","title":"NeumannBC","text":"<pre><code>class NeumannBC:\n    \"\"\"Neumann (gradient) boundary condition.\n\n    Parameters\n    ----------\n    value : float | Callable\n        Normal derivative value.\n    \"\"\"\n</code></pre>"},{"location":"api/pinn/#eo-application-example","title":"EO Application Example","text":""},{"location":"api/pinn/#sea-surface-temperature-interpolation","title":"Sea Surface Temperature Interpolation","text":"<pre><code>from ununennium.models.pinn import PINN, AdvectionDiffusionEquation, FourierMLP\n\n# SST follows advection-diffusion in the ocean\nequation = AdvectionDiffusionEquation(\n    diffusivity=100.0,      # m^2/s\n    velocity=(0.1, 0.05),   # m/s\n)\n\nnetwork = FourierMLP(\n    in_features=3,  # x, y, t\n    layers=[256, 256, 256, 1],\n    sigma=10.0,\n)\n\npinn = PINN(network=network, equation=equation, lambda_pde=1.0)\n\n# Train on sparse SST observations\n# Predict on dense grid\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocessing API","text":"<p>The preprocessing module provides spectral index computation, normalization, and radiometric operations.</p>"},{"location":"api/preprocessing/#spectral-indices","title":"Spectral Indices","text":""},{"location":"api/preprocessing/#common-indices","title":"Common Indices","text":"Index Formula Description NDVI \\(\\frac{NIR - Red}{NIR + Red}\\) Vegetation index NDWI \\(\\frac{Green - NIR}{Green + NIR}\\) Water index NBR \\(\\frac{NIR - SWIR}{NIR + SWIR}\\) Burn ratio EVI \\(2.5 \\cdot \\frac{NIR - Red}{NIR + 6 \\cdot Red - 7.5 \\cdot Blue + 1}\\) Enhanced vegetation"},{"location":"api/preprocessing/#compute_index","title":"compute_index","text":"<pre><code>def compute_index(\n    data: torch.Tensor | GeoTensor,\n    index: str,\n    bands: dict[str, int],\n    epsilon: float = 1e-8,\n) -&gt; torch.Tensor | GeoTensor:\n    \"\"\"Compute spectral index.\n\n    Parameters\n    ----------\n    data : torch.Tensor | GeoTensor\n        Input data with shape (C, H, W) or (B, C, H, W).\n    index : str\n        Index name: \"ndvi\", \"ndwi\", \"nbr\", \"evi\", \"savi\".\n    bands : dict[str, int]\n        Mapping of band names to indices (0-indexed).\n    epsilon : float, optional\n        Small value to prevent division by zero.\n\n    Returns\n    -------\n    torch.Tensor | GeoTensor\n        Computed index with shape (H, W) or (B, H, W).\n    \"\"\"\n</code></pre>"},{"location":"api/preprocessing/#example","title":"Example","text":"<pre><code>from ununennium.preprocessing import compute_index\n\n# Sentinel-2 band mapping\nbands = {\"red\": 3, \"nir\": 7, \"green\": 2, \"blue\": 1, \"swir\": 11}\n\nndvi = compute_index(tensor, \"ndvi\", bands)\nprint(f\"NDVI range: [{ndvi.min():.2f}, {ndvi.max():.2f}]\")\n</code></pre>"},{"location":"api/preprocessing/#normalization","title":"Normalization","text":""},{"location":"api/preprocessing/#normalize","title":"normalize","text":"<pre><code>def normalize(\n    data: torch.Tensor | GeoTensor,\n    method: str = \"minmax\",\n    per_channel: bool = True,\n    **kwargs,\n) -&gt; torch.Tensor | GeoTensor:\n    \"\"\"Normalize data.\n\n    Parameters\n    ----------\n    data : torch.Tensor | GeoTensor\n        Input data.\n    method : str, optional\n        Normalization method. Default: \"minmax\".\n    per_channel : bool, optional\n        Normalize each channel independently. Default: True.\n    **kwargs\n        Method-specific parameters.\n\n    Methods\n    -------\n    minmax : Scale to [0, 1]\n    zscore : Standardize to mean=0, std=1\n    percentile : Scale using percentiles (robust to outliers)\n    \"\"\"\n</code></pre>"},{"location":"api/preprocessing/#normalization-methods","title":"Normalization Methods","text":"Method Formula Parameters <code>minmax</code> \\(\\frac{x - \\min}{\\max - \\min}\\) None <code>zscore</code> \\(\\frac{x - \\mu}{\\sigma}\\) None <code>percentile</code> \\(\\frac{x - p_{low}}{p_{high} - p_{low}}\\) <code>p=(2, 98)</code>"},{"location":"api/preprocessing/#example_1","title":"Example","text":"<pre><code>from ununennium.preprocessing import normalize\n\n# Robust normalization\nnormalized = normalize(\n    tensor,\n    method=\"percentile\",\n    p=(2, 98),\n    per_channel=True,\n)\n</code></pre>"},{"location":"api/preprocessing/#standardization","title":"Standardization","text":""},{"location":"api/preprocessing/#standardize","title":"standardize","text":"<pre><code>def standardize(\n    data: torch.Tensor | GeoTensor,\n    mean: list[float] | None = None,\n    std: list[float] | None = None,\n    sensor: str | None = None,\n) -&gt; torch.Tensor | GeoTensor:\n    \"\"\"Standardize data with mean and std.\n\n    Parameters\n    ----------\n    data : torch.Tensor | GeoTensor\n        Input data.\n    mean : list[float] | None, optional\n        Per-channel means.\n    std : list[float] | None, optional\n        Per-channel stds.\n    sensor : str | None, optional\n        Sensor name for predefined statistics.\n    \"\"\"\n</code></pre>"},{"location":"api/preprocessing/#predefined-statistics","title":"Predefined Statistics","text":"<pre><code># Use sensor-specific statistics\nstandardized = standardize(tensor, sensor=\"sentinel2_l2a\")\n</code></pre> Sensor Bands Source <code>sentinel2_l2a</code> 12 ESA Level-2A <code>landsat8_c2</code> 7 USGS Collection 2 <code>modis_mod09ga</code> 7 MODIS daily"},{"location":"api/preprocessing/#cloud-masking","title":"Cloud Masking","text":""},{"location":"api/preprocessing/#apply_cloud_mask","title":"apply_cloud_mask","text":"<pre><code>def apply_cloud_mask(\n    data: torch.Tensor | GeoTensor,\n    mask: torch.Tensor,\n    fill_value: float = float(\"nan\"),\n) -&gt; torch.Tensor | GeoTensor:\n    \"\"\"Apply cloud mask to data.\n\n    Parameters\n    ----------\n    data : torch.Tensor | GeoTensor\n        Input data.\n    mask : torch.Tensor\n        Boolean mask (True = cloud).\n    fill_value : float, optional\n        Value for masked pixels. Default: NaN.\n    \"\"\"\n</code></pre>"},{"location":"api/preprocessing/#compute_cloud_mask","title":"compute_cloud_mask","text":"<pre><code>def compute_cloud_mask(\n    data: torch.Tensor | GeoTensor,\n    sensor: str,\n    threshold: float = 0.5,\n) -&gt; torch.Tensor:\n    \"\"\"Compute cloud mask using sensor-specific algorithm.\n\n    Parameters\n    ----------\n    data : torch.Tensor | GeoTensor\n        Input data with required bands.\n    sensor : str\n        Sensor identifier.\n    threshold : float, optional\n        Cloud probability threshold. Default: 0.5.\n    \"\"\"\n</code></pre>"},{"location":"api/preprocessing/#harmonization","title":"Harmonization","text":""},{"location":"api/preprocessing/#harmonize","title":"harmonize","text":"<pre><code>def harmonize(\n    data: torch.Tensor | GeoTensor,\n    source: str,\n    target: str,\n) -&gt; torch.Tensor | GeoTensor:\n    \"\"\"Harmonize data between sensors.\n\n    Parameters\n    ----------\n    data : torch.Tensor | GeoTensor\n        Input data from source sensor.\n    source : str\n        Source sensor identifier.\n    target : str\n        Target sensor identifier.\n\n    Returns\n    -------\n    torch.Tensor | GeoTensor\n        Harmonized data matching target sensor.\n    \"\"\"\n</code></pre>"},{"location":"api/preprocessing/#example_2","title":"Example","text":"<pre><code>from ununennium.preprocessing import harmonize\n\n# Convert Landsat-8 to Sentinel-2 spectral response\ns2_like = harmonize(landsat_tensor, source=\"landsat8\", target=\"sentinel2\")\n</code></pre>"},{"location":"api/training/","title":"Training API","text":"<p>The training module provides a flexible trainer with callbacks and distributed training support.</p>"},{"location":"api/training/#trainer","title":"Trainer","text":"<p>Main training loop abstraction.</p>"},{"location":"api/training/#class-definition","title":"Class Definition","text":"<pre><code>class Trainer:\n    \"\"\"Training loop manager.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Model to train.\n    optimizer : Optimizer\n        PyTorch optimizer.\n    loss_fn : nn.Module | Callable\n        Loss function.\n    train_loader : DataLoader\n        Training data loader.\n    val_loader : DataLoader | None, optional\n        Validation data loader.\n    callbacks : list[Callback] | None, optional\n        Training callbacks.\n    mixed_precision : bool, optional\n        Enable AMP. Default: False.\n    gradient_accumulation_steps : int, optional\n        Accumulation steps. Default: 1.\n    gradient_clip_norm : float | None, optional\n        Gradient clipping norm.\n    distributed : bool, optional\n        Enable DDP. Default: False.\n    device : str | torch.device, optional\n        Training device. Default: auto-detect.\n    \"\"\"\n</code></pre>"},{"location":"api/training/#methods","title":"Methods","text":""},{"location":"api/training/#fit","title":"<code>fit</code>","text":"<pre><code>def fit(\n    self,\n    epochs: int,\n    start_epoch: int = 0,\n) -&gt; dict[str, list[float]]:\n    \"\"\"Train the model.\n\n    Parameters\n    ----------\n    epochs : int\n        Number of epochs to train.\n    start_epoch : int, optional\n        Starting epoch (for resumption). Default: 0.\n\n    Returns\n    -------\n    dict[str, list[float]]\n        Training history with metrics.\n    \"\"\"\n</code></pre>"},{"location":"api/training/#validate","title":"<code>validate</code>","text":"<pre><code>def validate(self) -&gt; dict[str, float]:\n    \"\"\"Run validation loop.\n\n    Returns\n    -------\n    dict[str, float]\n        Validation metrics.\n    \"\"\"\n</code></pre>"},{"location":"api/training/#example","title":"Example","text":"<pre><code>from ununennium.training import Trainer, CheckpointCallback\nfrom ununennium.models import create_model\nfrom ununennium.losses import DiceLoss\nimport torch\n\nmodel = create_model(\"unet_resnet50\", in_channels=12, num_classes=10)\n\ntrainer = Trainer(\n    model=model,\n    optimizer=torch.optim.AdamW(model.parameters(), lr=1e-4),\n    loss_fn=DiceLoss(),\n    train_loader=train_loader,\n    val_loader=val_loader,\n    callbacks=[\n        CheckpointCallback(\"checkpoints/\", monitor=\"val_iou\"),\n    ],\n    mixed_precision=True,\n    gradient_accumulation_steps=4,\n)\n\nhistory = trainer.fit(epochs=100)\n</code></pre>"},{"location":"api/training/#callbacks","title":"Callbacks","text":""},{"location":"api/training/#base-callback","title":"Base Callback","text":"<pre><code>class Callback:\n    \"\"\"Base callback class.\n\n    Methods\n    -------\n    on_train_start(trainer)\n    on_train_end(trainer)\n    on_epoch_start(trainer, epoch)\n    on_epoch_end(trainer, epoch, logs)\n    on_batch_start(trainer, batch_idx)\n    on_batch_end(trainer, batch_idx, logs)\n    \"\"\"\n</code></pre>"},{"location":"api/training/#checkpointcallback","title":"CheckpointCallback","text":"<pre><code>class CheckpointCallback(Callback):\n    \"\"\"Save model checkpoints.\n\n    Parameters\n    ----------\n    path : str | Path\n        Directory to save checkpoints.\n    monitor : str, optional\n        Metric to monitor. Default: \"val_loss\".\n    mode : str, optional\n        \"min\" or \"max\". Default: \"min\".\n    save_best_only : bool, optional\n        Only save best model. Default: True.\n    save_last : bool, optional\n        Always save latest. Default: True.\n    \"\"\"\n</code></pre>"},{"location":"api/training/#earlystoppingcallback","title":"EarlyStoppingCallback","text":"<pre><code>class EarlyStoppingCallback(Callback):\n    \"\"\"Stop training when metric stops improving.\n\n    Parameters\n    ----------\n    monitor : str, optional\n        Metric to monitor. Default: \"val_loss\".\n    patience : int, optional\n        Epochs to wait. Default: 10.\n    mode : str, optional\n        \"min\" or \"max\". Default: \"min\".\n    min_delta : float, optional\n        Minimum change threshold. Default: 1e-4.\n    \"\"\"\n</code></pre>"},{"location":"api/training/#lrschedulercallback","title":"LRSchedulerCallback","text":"<pre><code>class LRSchedulerCallback(Callback):\n    \"\"\"Learning rate scheduler.\n\n    Parameters\n    ----------\n    scheduler : LRScheduler\n        PyTorch scheduler instance.\n    step_on : str, optional\n        When to step: \"epoch\" or \"batch\". Default: \"epoch\".\n    \"\"\"\n</code></pre>"},{"location":"api/training/#example-custom-callback","title":"Example: Custom Callback","text":"<pre><code>from ununennium.training import Callback\n\nclass LoggingCallback(Callback):\n    def on_epoch_end(self, trainer, epoch, logs):\n        print(f\"Epoch {epoch}: loss={logs['train_loss']:.4f}\")\n</code></pre>"},{"location":"api/training/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Common scheduler configurations:</p> Scheduler Description <code>CosineAnnealingLR</code> Cosine decay <code>OneCycleLR</code> Super-convergence <code>ReduceLROnPlateau</code> Reduce on plateau"},{"location":"api/training/#example_1","title":"Example","text":"<pre><code>from ununennium.training import LRSchedulerCallback\nimport torch.optim.lr_scheduler as sched\n\nscheduler = sched.CosineAnnealingLR(optimizer, T_max=100)\ncallback = LRSchedulerCallback(scheduler)\n</code></pre>"},{"location":"api/training/#distributed-training","title":"Distributed Training","text":""},{"location":"api/training/#multi-gpu-setup","title":"Multi-GPU Setup","text":"<pre><code># Launch: torchrun --nproc_per_node=4 train.py\n\ntrainer = Trainer(\n    model=model,\n    distributed=True,\n    # ...\n)\n</code></pre>"},{"location":"api/training/#distributed-configuration","title":"Distributed Configuration","text":"Parameter Default Description <code>backend</code> \"nccl\" Communication backend <code>find_unused_params</code> False DDP parameter"},{"location":"api/training/#metrics-integration","title":"Metrics Integration","text":"<pre><code>from ununennium.metrics import IoU, Dice\n\ntrainer = Trainer(\n    # ...\n    metrics=[\n        IoU(num_classes=10),\n        Dice(num_classes=10),\n    ],\n)\n</code></pre>"},{"location":"architecture/data-model/","title":"Data Model","text":"<p>This document details the core data structures in Ununennium, focusing on the <code>GeoTensor</code> and <code>GeoBatch</code> abstractions that preserve geospatial metadata through the processing pipeline.</p>"},{"location":"architecture/data-model/#table-of-contents","title":"Table of Contents","text":"<ol> <li>GeoTensor</li> <li>GeoBatch</li> <li>Coordinate Reference Systems</li> <li>Affine Transforms</li> <li>Operations</li> </ol>"},{"location":"architecture/data-model/#geotensor","title":"GeoTensor","text":"<p><code>GeoTensor</code> is the fundamental data structure in Ununennium. It wraps a PyTorch tensor with geospatial metadata.</p>"},{"location":"architecture/data-model/#structure","title":"Structure","text":"<pre><code>classDiagram\n    class GeoTensor {\n        +data: torch.Tensor\n        +crs: CRS\n        +transform: Affine\n        +bounds: BoundingBox\n        +resolution: tuple~float, float~\n        +nodata: float | None\n        +shape: tuple~int, ...~\n        +dtype: torch.dtype\n        +device: torch.device\n        +to(device) GeoTensor\n        +numpy() np.ndarray\n        +reproject(target_crs) GeoTensor\n        +crop(bounds) GeoTensor\n        +resample(resolution) GeoTensor\n    }\n\n    class BoundingBox {\n        +left: float\n        +bottom: float\n        +right: float\n        +top: float\n        +width: float\n        +height: float\n        +intersects(other) bool\n        +intersection(other) BoundingBox\n    }\n\n    class Affine {\n        +a: float\n        +b: float\n        +c: float\n        +d: float\n        +e: float\n        +f: float\n        +scale: tuple~float, float~\n        +translation: tuple~float, float~\n    }\n\n    GeoTensor --&gt; BoundingBox\n    GeoTensor --&gt; Affine\n</code></pre>"},{"location":"architecture/data-model/#mathematical-definition","title":"Mathematical Definition","text":"<p>A GeoTensor represents a discretized field over a geographic region. The mapping from pixel coordinates \\((i, j)\\) to world coordinates \\((x, y)\\) is defined by the affine transform:</p> \\[ \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} j \\\\ i \\\\ 1 \\end{bmatrix} \\] <p>Where: - \\(a\\): pixel width (x resolution) - \\(e\\): pixel height (y resolution, typically negative) - \\(c\\): x-coordinate of upper-left corner - \\(f\\): y-coordinate of upper-left corner - \\(b, d\\): rotation terms (typically zero for north-up imagery)</p>"},{"location":"architecture/data-model/#creating-geotensors","title":"Creating GeoTensors","text":"<pre><code>import torch\nfrom ununennium.core import GeoTensor\nfrom affine import Affine\n\n# From raw data\ndata = torch.randn(12, 512, 512)\ntransform = Affine.translation(500000, 4500000) * Affine.scale(10, -10)\n\ntensor = GeoTensor(\n    data=data,\n    crs=\"EPSG:32632\",\n    transform=transform,\n    nodata=-9999,\n)\n\nprint(f\"Shape: {tensor.shape}\")        # (12, 512, 512)\nprint(f\"Bounds: {tensor.bounds}\")      # BoundingBox(500000, 4494880, 505120, 4500000)\nprint(f\"Resolution: {tensor.resolution}\")  # (10.0, 10.0)\n</code></pre>"},{"location":"architecture/data-model/#from-file","title":"From File","text":"<pre><code>from ununennium.io import read_geotiff\n\ntensor = read_geotiff(\"sentinel2_l2a.tif\")\n</code></pre>"},{"location":"architecture/data-model/#geobatch","title":"GeoBatch","text":"<p><code>GeoBatch</code> represents a batch of samples for training, maintaining consistent metadata.</p>"},{"location":"architecture/data-model/#structure_1","title":"Structure","text":"Attribute Type Description <code>images</code> <code>torch.Tensor</code> Shape: (B, C, H, W) <code>masks</code> <code>torch.Tensor</code> Shape: (B, H, W) or (B, C, H, W) <code>crs</code> <code>CRS</code> Common coordinate reference system <code>transforms</code> <code>list[Affine]</code> Per-sample affine transforms"},{"location":"architecture/data-model/#creating-batches","title":"Creating Batches","text":"<pre><code>from ununennium.core import GeoBatch\n\nbatch = GeoBatch(\n    images=torch.randn(8, 12, 256, 256),\n    masks=torch.randint(0, 10, (8, 256, 256)),\n    crs=\"EPSG:32632\",\n)\n</code></pre>"},{"location":"architecture/data-model/#coordinate-reference-systems","title":"Coordinate Reference Systems","text":""},{"location":"architecture/data-model/#supported-crs-formats","title":"Supported CRS Formats","text":"Format Example Description EPSG Code <code>\"EPSG:32632\"</code> Standard EPSG identifier WKT <code>WKT string</code> Well-Known Text representation PROJ4 <code>\"+proj=utm +zone=32 +datum=WGS84\"</code> PROJ.4 definition string"},{"location":"architecture/data-model/#crs-operations","title":"CRS Operations","text":"<pre><code># Check CRS\nprint(tensor.crs)  # CRS(\"EPSG:32632\")\n\n# Reproject\nreprojected = tensor.reproject(\"EPSG:4326\")\n\n# Get CRS properties\nprint(tensor.crs.is_geographic)  # False (it's projected)\nprint(tensor.crs.units)          # metre\n</code></pre>"},{"location":"architecture/data-model/#common-crs-for-earth-observation","title":"Common CRS for Earth Observation","text":"CRS Use Case Units EPSG:4326 Global, WGS84 Geographic Degrees EPSG:32601-32660 UTM Zones 1-60 North Meters EPSG:32701-32760 UTM Zones 1-60 South Meters EPSG:3857 Web Mercator Meters"},{"location":"architecture/data-model/#affine-transforms","title":"Affine Transforms","text":""},{"location":"architecture/data-model/#transform-composition","title":"Transform Composition","text":"<p>Transforms can be composed for complex operations:</p> <pre><code>from affine import Affine\n\n# Create base transform\nbase = Affine.translation(500000, 4500000) * Affine.scale(10, -10)\n\n# Apply crop offset\ncrop_offset = Affine.translation(-100 * 10, 100 * 10)  # 100 pixels left/down\ncropped = base * crop_offset\n\n# Apply resampling\nresample = Affine.scale(0.5, 0.5)  # 2x downsample\nresampled = base * resample\n</code></pre>"},{"location":"architecture/data-model/#pixel-world-coordinate-conversion","title":"Pixel-World Coordinate Conversion","text":"<pre><code># Pixel to world\nrow, col = 100, 200\nx, y = tensor.transform * (col, row)\n\n# World to pixel\ncol, row = ~tensor.transform * (x, y)\n</code></pre>"},{"location":"architecture/data-model/#operations","title":"Operations","text":""},{"location":"architecture/data-model/#cropping","title":"Cropping","text":"<p>Cropping preserves CRS and updates the affine transform:</p> <pre><code>from ununennium.core import BoundingBox\n\n# Crop by bounding box\nbounds = BoundingBox(502000, 4496000, 504000, 4498000)\ncropped = tensor.crop(bounds)\n\n# Crop by pixel indices\ncropped = tensor[..., 100:300, 200:400]  # Slicing preserves metadata\n</code></pre>"},{"location":"architecture/data-model/#resampling","title":"Resampling","text":"<p>Resampling uses configurable interpolation kernels:</p> \\[ I'(x', y') = \\sum_{(x,y) \\in \\mathcal{N}} I(x, y) \\cdot K\\left(\\frac{x' - x}{\\sigma_x}, \\frac{y' - y}{\\sigma_y}\\right) \\] <pre><code># Resample to new resolution\nresampled = tensor.resample(\n    resolution=(20.0, 20.0),  # New pixel size\n    method=\"bilinear\",        # Interpolation kernel\n)\n</code></pre> Method Description Use Case <code>nearest</code> Nearest neighbor Categorical data, masks <code>bilinear</code> Bilinear interpolation General imagery <code>cubic</code> Bicubic interpolation High-quality resampling <code>lanczos</code> Lanczos windowed sinc Best quality, slowest"},{"location":"architecture/data-model/#reprojection","title":"Reprojection","text":"<pre><code># Reproject to different CRS\nwgs84 = tensor.reproject(\n    target_crs=\"EPSG:4326\",\n    resolution=(0.0001, 0.0001),  # Degrees\n    method=\"bilinear\",\n)\n</code></pre>"},{"location":"architecture/data-model/#memory-management","title":"Memory Management","text":""},{"location":"architecture/data-model/#device-placement","title":"Device Placement","text":"<pre><code># Move to GPU\ngpu_tensor = tensor.to(\"cuda:0\")\n\n# Move back to CPU\ncpu_tensor = gpu_tensor.to(\"cpu\")\n</code></pre>"},{"location":"architecture/data-model/#memory-efficient-operations","title":"Memory-Efficient Operations","text":"<p>For large rasters, use streaming operations:</p> <pre><code>from ununennium.io import stream_geotiff\n\n# Process in chunks\nfor chunk in stream_geotiff(\"large_raster.tif\", chunk_size=1024):\n    # Process chunk\n    result = process(chunk)\n</code></pre>"},{"location":"architecture/data-model/#type-safety","title":"Type Safety","text":"<p>GeoTensor enforces type constraints at runtime:</p> Constraint Validation CRS validity Checked on creation Transform consistency Verified against bounds NoData type Must match tensor dtype Resolution positivity Must be positive values <pre><code># This raises ValueError\ntry:\n    GeoTensor(data, crs=\"INVALID_CRS\")\nexcept ValueError as e:\n    print(f\"Invalid CRS: {e}\")\n</code></pre>"},{"location":"architecture/data-model/#next-steps","title":"Next Steps","text":"<ul> <li>Pipelines - Data flow architecture</li> <li>API Reference: Core - Complete API documentation</li> </ul>"},{"location":"architecture/overview/","title":"System Architecture Overview","text":"<p>This document provides a comprehensive overview of Ununennium's architecture, design principles, and component interactions.</p>"},{"location":"architecture/overview/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Design Principles</li> <li>System Overview</li> <li>Component Architecture</li> <li>Data Flow</li> <li>Extension Points</li> </ol>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":"<p>Ununennium is built on the following architectural principles:</p> Principle Description CRS Preservation Coordinate Reference Systems are tracked through the entire pipeline Lazy Evaluation Data is loaded on-demand to handle datasets larger than memory Modularity Components are loosely coupled and independently testable Type Safety Strong typing with runtime validation for geospatial constraints Reproducibility Deterministic operations with explicit random state management"},{"location":"architecture/overview/#system-overview","title":"System Overview","text":"<pre><code>graph TB\n    subgraph \"Data Layer\"\n        COG[COG Reader]\n        STAC[STAC Client]\n        ZARR[Zarr Store]\n    end\n\n    subgraph \"Core Layer\"\n        GT[GeoTensor]\n        GB[GeoBatch]\n        TILE[Tiler]\n        SAMP[Sampler]\n    end\n\n    subgraph \"Processing Layer\"\n        PREP[Preprocessing]\n        AUG[Augmentation]\n        IDX[Spectral Indices]\n    end\n\n    subgraph \"Model Layer\"\n        REG[Model Registry]\n        BACK[Backbones]\n        HEAD[Task Heads]\n        ARCH[Architectures]\n    end\n\n    subgraph \"Training Layer\"\n        TRAIN[Trainer]\n        CB[Callbacks]\n        OPT[Optimizers]\n        LOSS[Loss Functions]\n    end\n\n    subgraph \"Evaluation Layer\"\n        MET[Metrics]\n        CAL[Calibration]\n        VIS[Visualization]\n    end\n\n    subgraph \"Export Layer\"\n        ONNX[ONNX Export]\n        TS[TorchScript]\n        QUANT[Quantization]\n    end\n\n    COG --&gt; GT\n    STAC --&gt; GT\n    ZARR --&gt; GT\n    GT --&gt; TILE\n    TILE --&gt; SAMP\n    SAMP --&gt; GB\n    GB --&gt; PREP\n    PREP --&gt; AUG\n    AUG --&gt; IDX\n    IDX --&gt; ARCH\n    REG --&gt; ARCH\n    BACK --&gt; ARCH\n    HEAD --&gt; ARCH\n    ARCH --&gt; TRAIN\n    CB --&gt; TRAIN\n    OPT --&gt; TRAIN\n    LOSS --&gt; TRAIN\n    TRAIN --&gt; MET\n    MET --&gt; CAL\n    TRAIN --&gt; ONNX\n    TRAIN --&gt; TS\n</code></pre>"},{"location":"architecture/overview/#component-architecture","title":"Component Architecture","text":""},{"location":"architecture/overview/#data-layer","title":"Data Layer","text":"<p>The data layer provides cloud-native access to geospatial imagery.</p> <pre><code>classDiagram\n    class GeoReader {\n        &lt;&lt;interface&gt;&gt;\n        +read(path: str) GeoTensor\n        +read_window(path: str, bounds: BoundingBox) GeoTensor\n    }\n\n    class COGReader {\n        +read(path: str) GeoTensor\n        +read_window(path: str, bounds: BoundingBox) GeoTensor\n        +get_overview_level(scale: float) int\n    }\n\n    class STACClient {\n        +search(bbox: tuple, datetime: str) Iterator\n        +read_item(item: Item) GeoTensor\n    }\n\n    class ZarrReader {\n        +read(path: str) GeoTensor\n        +read_chunk(path: str, chunk_id: tuple) GeoTensor\n    }\n\n    GeoReader &lt;|-- COGReader\n    GeoReader &lt;|-- STACClient\n    GeoReader &lt;|-- ZarrReader\n</code></pre>"},{"location":"architecture/overview/#core-layer","title":"Core Layer","text":"<p>The core layer provides CRS-aware tensor abstractions.</p> Component Responsibility <code>GeoTensor</code> Single CRS-aware tensor with metadata <code>GeoBatch</code> Batch of GeoTensors with consistent CRS <code>Tiler</code> Patch extraction with overlap handling <code>Sampler</code> Spatial sampling strategies"},{"location":"architecture/overview/#geotensor-structure","title":"GeoTensor Structure","text":"<pre><code>GeoTensor\n\u251c\u2500\u2500 data: torch.Tensor          # Shape: (C, H, W)\n\u251c\u2500\u2500 crs: CRS                    # Coordinate Reference System\n\u251c\u2500\u2500 transform: Affine           # Pixel-to-world transform\n\u251c\u2500\u2500 bounds: BoundingBox         # Geographic extent\n\u251c\u2500\u2500 resolution: tuple[float, float]  # Pixel size in CRS units\n\u2514\u2500\u2500 nodata: float | None        # NoData value\n</code></pre>"},{"location":"architecture/overview/#model-layer","title":"Model Layer","text":"<p>The model layer follows a registry pattern for architecture composition.</p> <pre><code>graph LR\n    subgraph \"Registry Pattern\"\n        REG[ModelRegistry]\n\n        subgraph \"Backbones\"\n            RN[ResNet]\n            EN[EfficientNet]\n            VIT[ViT]\n            SWIN[Swin]\n        end\n\n        subgraph \"Heads\"\n            SEG[SegmentationHead]\n            CLS[ClassificationHead]\n            DET[DetectionHead]\n        end\n\n        subgraph \"Architectures\"\n            UNET[U-Net]\n            DL[DeepLabV3+]\n            FPN[FPN]\n        end\n    end\n\n    REG --&gt; RN\n    REG --&gt; EN\n    REG --&gt; VIT\n    REG --&gt; SWIN\n    REG --&gt; SEG\n    REG --&gt; CLS\n    REG --&gt; DET\n    REG --&gt; UNET\n    REG --&gt; DL\n    REG --&gt; FPN\n</code></pre>"},{"location":"architecture/overview/#training-layer","title":"Training Layer","text":"<p>The training layer provides a flexible trainer with callback hooks.</p> <pre><code>sequenceDiagram\n    participant User\n    participant Trainer\n    participant Callbacks\n    participant Model\n    participant DataLoader\n\n    User-&gt;&gt;Trainer: fit(epochs)\n    loop Each Epoch\n        Trainer-&gt;&gt;Callbacks: on_epoch_start()\n        loop Each Batch\n            DataLoader-&gt;&gt;Trainer: batch\n            Trainer-&gt;&gt;Callbacks: on_batch_start()\n            Trainer-&gt;&gt;Model: forward(batch)\n            Model--&gt;&gt;Trainer: predictions\n            Trainer-&gt;&gt;Trainer: compute_loss()\n            Trainer-&gt;&gt;Model: backward()\n            Trainer-&gt;&gt;Callbacks: on_batch_end()\n        end\n        Trainer-&gt;&gt;Callbacks: on_epoch_end()\n    end\n    Trainer--&gt;&gt;User: history\n</code></pre>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#training-pipeline","title":"Training Pipeline","text":"<pre><code>graph LR\n    subgraph \"Input\"\n        S3[(Cloud Storage)]\n        FS[(Local Files)]\n    end\n\n    subgraph \"Loading\"\n        STREAM[Streaming Reader]\n        CACHE[(Tile Cache)]\n    end\n\n    subgraph \"Preprocessing\"\n        NORM[Normalize]\n        MASK[Cloud Mask]\n        IDX[Indices]\n    end\n\n    subgraph \"Batching\"\n        SAMP[Sampler]\n        LOAD[DataLoader]\n    end\n\n    subgraph \"Training\"\n        GPU[GPU]\n        MODEL[Model]\n    end\n\n    S3 --&gt; STREAM\n    FS --&gt; STREAM\n    STREAM --&gt; CACHE\n    CACHE --&gt; NORM\n    NORM --&gt; MASK\n    MASK --&gt; IDX\n    IDX --&gt; SAMP\n    SAMP --&gt; LOAD\n    LOAD --&gt; GPU\n    GPU --&gt; MODEL\n</code></pre>"},{"location":"architecture/overview/#inference-pipeline","title":"Inference Pipeline","text":"<pre><code>graph LR\n    subgraph \"Input\"\n        IMG[Large Raster]\n    end\n\n    subgraph \"Tiling\"\n        TILE[Tiler]\n        WIN[Sliding Window]\n    end\n\n    subgraph \"Inference\"\n        BATCH[Batch]\n        MODEL[Model]\n        PRED[Predictions]\n    end\n\n    subgraph \"Assembly\"\n        STITCH[Stitcher]\n        BLEND[Overlap Blend]\n    end\n\n    subgraph \"Output\"\n        OUT[Output Raster]\n    end\n\n    IMG --&gt; TILE\n    TILE --&gt; WIN\n    WIN --&gt; BATCH\n    BATCH --&gt; MODEL\n    MODEL --&gt; PRED\n    PRED --&gt; STITCH\n    STITCH --&gt; BLEND\n    BLEND --&gt; OUT\n</code></pre>"},{"location":"architecture/overview/#extension-points","title":"Extension Points","text":"<p>Ununennium provides several extension points for customization:</p> Extension Point Interface Purpose Custom Readers <code>GeoReader</code> New data formats Custom Backbones <code>BackboneRegistry</code> New architectures Custom Losses <code>LossFunction</code> Domain-specific objectives Custom Callbacks <code>Callback</code> Training customization Custom Metrics <code>Metric</code> Evaluation criteria"},{"location":"architecture/overview/#registering-custom-components","title":"Registering Custom Components","text":"<pre><code>from ununennium.models import register_backbone\n\n@register_backbone(\"my_backbone\")\nclass MyBackbone(nn.Module):\n    \"\"\"Custom backbone implementation.\"\"\"\n\n    def __init__(self, in_channels: int = 3):\n        super().__init__()\n        # Implementation\n\n    def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n        # Return multi-scale features\n        pass\n</code></pre>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Data Model - Deep dive into GeoTensor</li> <li>Pipelines - Detailed pipeline architecture</li> <li>Performance - Optimization guide</li> </ul>"},{"location":"architecture/performance/","title":"Performance","text":"<p>This document covers performance characteristics, benchmarks, and optimization strategies for Ununennium.</p>"},{"location":"architecture/performance/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Benchmark Results</li> <li>Memory Optimization</li> <li>Training Optimization</li> <li>Inference Optimization</li> <li>Profiling</li> </ol>"},{"location":"architecture/performance/#benchmark-results","title":"Benchmark Results","text":""},{"location":"architecture/performance/#hardware-configuration","title":"Hardware Configuration","text":"Component Specification GPU NVIDIA A100 80GB CPU AMD EPYC 7763 (64 cores) RAM 512 GB DDR4 Storage NVMe SSD (7 GB/s read) PyTorch 2.1.0 CUDA 12.1"},{"location":"architecture/performance/#model-throughput","title":"Model Throughput","text":"Model Input Size Batch Throughput Memory U-Net ResNet-50 512x512x12 16 142 img/s 12.4 GB U-Net EfficientNet-B4 512x512x12 16 98 img/s 14.2 GB DeepLabV3+ ResNet-101 512x512x12 12 76 img/s 16.8 GB ViT-L/16 224x224x12 32 256 img/s 18.1 GB Pix2Pix 256x256x2 8 67 img/s 8.6 GB ESRGAN (4x) 64x64x12 16 124 img/s 6.2 GB"},{"location":"architecture/performance/#io-throughput","title":"I/O Throughput","text":"Source Operation Throughput Local SSD COG read (512x512) 2,400 tiles/s S3 (same region) COG read (512x512) 180 tiles/s Zarr (local) Chunk read 3,100 chunks/s"},{"location":"architecture/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"architecture/performance/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>Trade compute for memory by recomputing activations during backward pass:</p> \\[ \\text{Memory} = O(\\sqrt{L}) \\quad \\text{vs} \\quad O(L) \\] <p>where \\(L\\) is the number of layers.</p> <pre><code>from ununennium.models import create_model\n\nmodel = create_model(\n    \"unet_resnet50\",\n    in_channels=12,\n    num_classes=10,\n    checkpoint_gradients=True,  # Enable checkpointing\n)\n</code></pre>"},{"location":"architecture/performance/#mixed-precision","title":"Mixed Precision","text":"<p>FP16 reduces memory by 50% with minimal accuracy impact:</p> <pre><code>from ununennium.training import Trainer\n\ntrainer = Trainer(\n    model=model,\n    mixed_precision=True,  # Enable AMP\n    # ...\n)\n</code></pre>"},{"location":"architecture/performance/#memory-estimation","title":"Memory Estimation","text":"<p>Estimate memory requirements:</p> Component Formula Model parameters \\(4 \\cdot P\\) bytes (FP32) Optimizer state \\(8 \\cdot P\\) bytes (Adam) Activations \\(B \\cdot C \\cdot H \\cdot W \\cdot 4\\) bytes per layer Gradients \\(4 \\cdot P\\) bytes"},{"location":"architecture/performance/#training-optimization","title":"Training Optimization","text":""},{"location":"architecture/performance/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Simulate larger batch sizes without more memory:</p> \\[ \\text{Effective Batch} = \\text{Micro Batch} \\times \\text{Accumulation Steps} \\] <pre><code>trainer = Trainer(\n    model=model,\n    gradient_accumulation_steps=4,  # 4x effective batch\n    # ...\n)\n</code></pre>"},{"location":"architecture/performance/#distributed-training","title":"Distributed Training","text":"<p>Data parallel training with DDP:</p> <pre><code>graph TB\n    subgraph \"Multi-GPU Training\"\n        GPU0[GPU 0&lt;br/&gt;Batch 0]\n        GPU1[GPU 1&lt;br/&gt;Batch 1]\n        GPU2[GPU 2&lt;br/&gt;Batch 2]\n        GPU3[GPU 3&lt;br/&gt;Batch 3]\n    end\n\n    subgraph \"Synchronization\"\n        SYNC[AllReduce Gradients]\n    end\n\n    GPU0 --&gt; SYNC\n    GPU1 --&gt; SYNC\n    GPU2 --&gt; SYNC\n    GPU3 --&gt; SYNC\n    SYNC --&gt; GPU0\n    SYNC --&gt; GPU1\n    SYNC --&gt; GPU2\n    SYNC --&gt; GPU3\n</code></pre> <pre><code># Launch with torchrun\n# torchrun --nproc_per_node=4 train.py\n\nfrom ununennium.training import Trainer\n\ntrainer = Trainer(\n    model=model,\n    distributed=True,\n    # ...\n)\n</code></pre>"},{"location":"architecture/performance/#learning-rate-scaling","title":"Learning Rate Scaling","text":"<p>When increasing batch size, scale learning rate:</p> \\[ \\eta_{\\text{new}} = \\eta_{\\text{base}} \\times \\frac{B_{\\text{new}}}{B_{\\text{base}}} \\]"},{"location":"architecture/performance/#inference-optimization","title":"Inference Optimization","text":""},{"location":"architecture/performance/#torchscript-compilation","title":"TorchScript Compilation","text":"<pre><code>from ununennium.export import to_torchscript\n\nscripted = to_torchscript(model, example_input=torch.randn(1, 12, 512, 512))\nscripted.save(\"model.pt\")\n</code></pre>"},{"location":"architecture/performance/#onnx-export","title":"ONNX Export","text":"<pre><code>from ununennium.export import to_onnx\n\nto_onnx(\n    model,\n    example_input=torch.randn(1, 12, 512, 512),\n    output_path=\"model.onnx\",\n    opset_version=17,\n    dynamic_axes={\"input\": {0: \"batch\"}},\n)\n</code></pre>"},{"location":"architecture/performance/#batch-inference","title":"Batch Inference","text":"<p>Optimal batch size depends on GPU memory and model size:</p> GPU Memory Recommended Batch (512x512x12) 8 GB 2-4 16 GB 8-12 24 GB 16-24 40 GB 32-48 80 GB 64-96"},{"location":"architecture/performance/#profiling","title":"Profiling","text":""},{"location":"architecture/performance/#pytorch-profiler","title":"PyTorch Profiler","text":"<pre><code>from torch.profiler import profile, ProfilerActivity\n\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    record_shapes=True,\n    profile_memory=True,\n) as prof:\n    output = model(input_batch)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n</code></pre>"},{"location":"architecture/performance/#memory-profiling","title":"Memory Profiling","text":"<pre><code>import torch\n\n# Peak memory\ntorch.cuda.reset_peak_memory_stats()\noutput = model(input_batch)\npeak = torch.cuda.max_memory_allocated() / 1e9\nprint(f\"Peak memory: {peak:.2f} GB\")\n</code></pre>"},{"location":"architecture/performance/#common-bottlenecks","title":"Common Bottlenecks","text":"Symptom Likely Cause Solution GPU utilization low Data loading Increase num_workers, enable prefetch Training speed varies GC pauses Tune garbage collection OOM on large images Tile size too large Reduce tile size, enable checkpointing Slow I/O Remote data source Use caching, co-located storage"},{"location":"architecture/performance/#optimization-checklist","title":"Optimization Checklist","text":"Category Optimization Impact Memory Mixed precision -50% memory Memory Gradient checkpointing -40% memory Speed Compile model (torch.compile) +20-50% speed Speed Increase batch size Variable Speed Pin memory +5-10% speed I/O Prefetch factor +10-30% throughput I/O Use COG format +100-200% vs GeoTIFF"},{"location":"architecture/performance/#next-steps","title":"Next Steps","text":"<ul> <li>Security and Privacy - Data handling</li> <li>Benchmarking Guide - Custom benchmarks</li> </ul>"},{"location":"architecture/pipelines/","title":"Pipelines","text":"<p>This document describes the data flow pipelines in Ununennium, from raw data ingestion through model training and inference.</p>"},{"location":"architecture/pipelines/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Pipeline Overview</li> <li>Training Pipeline</li> <li>Inference Pipeline</li> <li>Streaming Architecture</li> </ol>"},{"location":"architecture/pipelines/#pipeline-overview","title":"Pipeline Overview","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        S3[Cloud Storage]\n        LOCAL[Local Files]\n        STAC[STAC Catalog]\n    end\n\n    subgraph \"Ingestion\"\n        READ[Reader Pool]\n        CACHE[Tile Cache]\n    end\n\n    subgraph \"Processing\"\n        TILE[Tiling]\n        PREP[Preprocessing]\n        AUG[Augmentation]\n    end\n\n    subgraph \"Execution\"\n        TRAIN[Training]\n        INFER[Inference]\n    end\n\n    S3 --&gt; READ\n    LOCAL --&gt; READ\n    STAC --&gt; READ\n    READ --&gt; CACHE\n    CACHE --&gt; TILE\n    TILE --&gt; PREP\n    PREP --&gt; AUG\n    AUG --&gt; TRAIN\n    AUG --&gt; INFER\n</code></pre>"},{"location":"architecture/pipelines/#training-pipeline","title":"Training Pipeline","text":""},{"location":"architecture/pipelines/#data-loading-flow","title":"Data Loading Flow","text":"<pre><code>sequenceDiagram\n    participant DS as Dataset\n    participant TL as Tiler\n    participant PP as Preprocessor\n    participant AU as Augmenter\n    participant DL as DataLoader\n    participant GPU as GPU\n\n    loop Each Epoch\n        DL-&gt;&gt;DS: get_sample(idx)\n        DS-&gt;&gt;TL: extract_tile(bounds)\n        TL--&gt;&gt;DS: tile_data\n        DS-&gt;&gt;PP: preprocess(tile)\n        PP--&gt;&gt;DS: normalized_tile\n        DS-&gt;&gt;AU: augment(tile)\n        AU--&gt;&gt;DS: augmented_tile\n        DS--&gt;&gt;DL: sample\n        DL-&gt;&gt;GPU: batch.to(device)\n    end\n</code></pre>"},{"location":"architecture/pipelines/#tiling-strategy","title":"Tiling Strategy","text":"<p>Large rasters are divided into overlapping patches for training:</p> <pre><code>graph TB\n    subgraph \"Source Raster\"\n        SR[10980 x 10980 pixels]\n    end\n\n    subgraph \"Tiling Parameters\"\n        SIZE[Tile Size: 512 x 512]\n        STRIDE[Stride: 256 x 256]\n        OVERLAP[Overlap: 50%]\n    end\n\n    subgraph \"Output Tiles\"\n        T1[Tile 1]\n        T2[Tile 2]\n        TN[Tile N...]\n    end\n\n    SR --&gt; SIZE\n    SIZE --&gt; STRIDE\n    STRIDE --&gt; OVERLAP\n    OVERLAP --&gt; T1\n    OVERLAP --&gt; T2\n    OVERLAP --&gt; TN\n</code></pre> <p>The overlap ratio \\(r\\) determines the stride:</p> \\[ s = \\lfloor w \\cdot (1 - r) \\rfloor \\] <p>where \\(w\\) is the tile width and \\(s\\) is the stride.</p>"},{"location":"architecture/pipelines/#spatial-sampling","title":"Spatial Sampling","text":"<p>To avoid spatial autocorrelation in train/validation splits:</p> <pre><code>graph LR\n    subgraph \"Block-based CV\"\n        B1[Block 1&lt;br/&gt;Train]\n        B2[Block 2&lt;br/&gt;Val]\n        B3[Block 3&lt;br/&gt;Train]\n        B4[Block 4&lt;br/&gt;Test]\n    end\n</code></pre> <p>Block size should exceed the spatial autocorrelation range:</p> \\[ \\text{block\\_size} \\geq 2 \\cdot \\text{correlation\\_range} \\]"},{"location":"architecture/pipelines/#inference-pipeline","title":"Inference Pipeline","text":""},{"location":"architecture/pipelines/#sliding-window-inference","title":"Sliding Window Inference","text":"<pre><code>graph TB\n    subgraph \"Input\"\n        IMG[Large Raster]\n    end\n\n    subgraph \"Windowing\"\n        W1[Window 1]\n        W2[Window 2]\n        WN[Window N]\n    end\n\n    subgraph \"Inference\"\n        MODEL[Model]\n        P1[Pred 1]\n        P2[Pred 2]\n        PN[Pred N]\n    end\n\n    subgraph \"Assembly\"\n        STITCH[Stitcher]\n        OUT[Output Raster]\n    end\n\n    IMG --&gt; W1\n    IMG --&gt; W2\n    IMG --&gt; WN\n    W1 --&gt; MODEL\n    W2 --&gt; MODEL\n    WN --&gt; MODEL\n    MODEL --&gt; P1\n    MODEL --&gt; P2\n    MODEL --&gt; PN\n    P1 --&gt; STITCH\n    P2 --&gt; STITCH\n    PN --&gt; STITCH\n    STITCH --&gt; OUT\n</code></pre>"},{"location":"architecture/pipelines/#overlap-blending","title":"Overlap Blending","text":"<p>To avoid seam artifacts, overlapping predictions are blended using cosine weights:</p> \\[ w(i, j) = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{\\pi \\cdot \\min(i, W-i)}{m}\\right)\\right) \\cdot \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{\\pi \\cdot \\min(j, H-j)}{m}\\right)\\right) \\] <p>where \\(m\\) is the margin width.</p> <pre><code>from ununennium.tiling import SlidingWindowInference\n\ninferencer = SlidingWindowInference(\n    model=model,\n    tile_size=(512, 512),\n    overlap=0.5,\n    blend_mode=\"cosine\",  # or \"linear\", \"none\"\n)\n\noutput = inferencer.predict(\"input_raster.tif\")\n</code></pre>"},{"location":"architecture/pipelines/#streaming-architecture","title":"Streaming Architecture","text":""},{"location":"architecture/pipelines/#cog-streaming","title":"COG Streaming","text":"<p>Cloud Optimized GeoTIFFs enable efficient partial reads:</p> <pre><code>graph LR\n    subgraph \"COG Structure\"\n        META[Metadata]\n        OV1[Overview 1]\n        OV2[Overview 2]\n        FULL[Full Resolution]\n    end\n\n    subgraph \"HTTP Range Requests\"\n        REQ1[Fetch Header]\n        REQ2[Fetch Tile]\n    end\n\n    META --&gt; REQ1\n    FULL --&gt; REQ2\n</code></pre>"},{"location":"architecture/pipelines/#memory-budget","title":"Memory Budget","text":"<p>For streaming large datasets:</p> Parameter Formula Tiles in memory \\(\\frac{\\text{memory\\_budget}}{\\text{tile\\_size}^2 \\cdot \\text{channels} \\cdot \\text{dtype\\_size}}\\) Prefetch queue \\(\\text{batch\\_size} \\cdot \\text{prefetch\\_factor}\\) Cache size \\(\\frac{\\text{cache\\_budget}}{\\text{avg\\_tile\\_bytes}}\\) <pre><code>from ununennium.io import StreamingDataset\n\ndataset = StreamingDataset(\n    urls=[\"s3://bucket/image1.tif\", ...],\n    tile_size=512,\n    cache_size=\"2GB\",\n    prefetch=4,\n)\n</code></pre>"},{"location":"architecture/pipelines/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"architecture/pipelines/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>pipeline:\n  data:\n    sources:\n      - type: stac\n        url: \"https://earth-search.aws.element84.com/v1\"\n        collection: sentinel-2-l2a\n\n  tiling:\n    size: [512, 512]\n    overlap: 0.25\n\n  preprocessing:\n    - type: normalize\n      method: percentile\n      p: [2, 98]\n    - type: indices\n      compute: [ndvi, ndwi]\n\n  augmentation:\n    - type: rotate\n      degrees: [-15, 15]\n    - type: flip\n      horizontal: true\n      vertical: true\n</code></pre>"},{"location":"architecture/pipelines/#next-steps","title":"Next Steps","text":"<ul> <li>Performance - Optimization strategies</li> <li>API Reference: Data I/O - I/O API documentation</li> </ul>"},{"location":"architecture/security-privacy/","title":"Security and Privacy","text":"<p>This document covers security considerations and privacy best practices when using Ununennium for Earth observation applications.</p>"},{"location":"architecture/security-privacy/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Data Security</li> <li>Privacy Considerations</li> <li>Model Security</li> <li>Deployment Security</li> </ol>"},{"location":"architecture/security-privacy/#data-security","title":"Data Security","text":""},{"location":"architecture/security-privacy/#data-handling-principles","title":"Data Handling Principles","text":"Principle Implementation Least privilege Request minimal data access Encryption at rest Use encrypted storage Encryption in transit HTTPS for remote data Access logging Audit data access patterns"},{"location":"architecture/security-privacy/#credential-management","title":"Credential Management","text":"<p>Never hardcode credentials:</p> <pre><code># Correct: Use environment variables\nimport os\n\naws_key = os.environ.get(\"AWS_ACCESS_KEY_ID\")\naws_secret = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n</code></pre> <pre><code># Incorrect: Hardcoded credentials\naws_key = \"AKIAIOSFODNN7EXAMPLE\"  # Never do this\n</code></pre>"},{"location":"architecture/security-privacy/#secure-data-sources","title":"Secure Data Sources","text":"Source Security Feature S3 IAM roles, bucket policies Azure Blob Managed identities, SAS tokens GCS Service accounts, signed URLs"},{"location":"architecture/security-privacy/#privacy-considerations","title":"Privacy Considerations","text":""},{"location":"architecture/security-privacy/#geospatial-privacy","title":"Geospatial Privacy","text":"<p>Earth observation data may reveal sensitive information:</p> Data Type Privacy Risk Mitigation High-resolution imagery Individual identification Resolution limits Building footprints Property ownership Aggregation Vehicle detection Movement tracking Temporal aggregation Crop type Agricultural economics Statistical disclosure control"},{"location":"architecture/security-privacy/#resolution-thresholds","title":"Resolution Thresholds","text":"<p>General guidelines for privacy-preserving resolution:</p> Resolution Identifiable Features &lt; 0.3 m Individuals, license plates 0.3 - 1 m Vehicles, small structures 1 - 5 m Buildings, roads 5 - 30 m Land use patterns &gt; 30 m Regional features"},{"location":"architecture/security-privacy/#data-anonymization","title":"Data Anonymization","text":"<p>For sensitive applications:</p> <pre><code>from ununennium.preprocessing import anonymize\n\n# Blur high-resolution imagery\nblurred = anonymize(image, method=\"gaussian\", sigma=5.0)\n\n# Downsample to safe resolution\nsafe = anonymize(image, method=\"downsample\", target_resolution=5.0)\n</code></pre>"},{"location":"architecture/security-privacy/#model-security","title":"Model Security","text":""},{"location":"architecture/security-privacy/#model-provenance","title":"Model Provenance","text":"<p>Verify model sources before use:</p> Check Purpose Source verification Confirm official repository Checksum validation Detect tampering Signature verification Cryptographic authenticity <pre><code>from ununennium.models import download_pretrained\n\n# Verifies checksum automatically\nmodel = download_pretrained(\n    \"unet_resnet50\",\n    verify_checksum=True,\n)\n</code></pre>"},{"location":"architecture/security-privacy/#adversarial-robustness","title":"Adversarial Robustness","text":"<p>Earth observation models may be vulnerable to adversarial attacks:</p> Attack Type Description Defense Evasion Perturbed inputs Input validation Poisoning Corrupted training data Data validation Model extraction Query-based stealing Rate limiting"},{"location":"architecture/security-privacy/#deployment-security","title":"Deployment Security","text":""},{"location":"architecture/security-privacy/#api-security","title":"API Security","text":"<p>For deployed models:</p> Measure Implementation Authentication OAuth 2.0, API keys Rate limiting Prevent abuse Input validation Reject malformed inputs Output sanitization Prevent information leakage"},{"location":"architecture/security-privacy/#container-security","title":"Container Security","text":"<pre><code># Use minimal base image\nFROM python:3.11-slim\n\n# Run as non-root\nRUN useradd -m appuser\nUSER appuser\n\n# Minimize attack surface\nRUN pip install ununennium --no-cache-dir\n</code></pre>"},{"location":"architecture/security-privacy/#logging-and-monitoring","title":"Logging and Monitoring","text":"Event Log Level Retention Authentication failures WARNING 90 days Unusual query patterns INFO 30 days Model predictions DEBUG 7 days"},{"location":"architecture/security-privacy/#compliance-considerations","title":"Compliance Considerations","text":""},{"location":"architecture/security-privacy/#regulatory-frameworks","title":"Regulatory Frameworks","text":"Region Framework Relevance EU GDPR Personal data in imagery US CCPA California consumer data Global ISO 27001 Information security"},{"location":"architecture/security-privacy/#data-retention","title":"Data Retention","text":"<p>Implement data lifecycle policies:</p> <pre><code>from ununennium.utils import DataRetentionPolicy\n\npolicy = DataRetentionPolicy(\n    raw_data=\"30d\",      # Delete after 30 days\n    processed_data=\"90d\",\n    model_outputs=\"1y\",\n)\n</code></pre>"},{"location":"architecture/security-privacy/#security-checklist","title":"Security Checklist","text":"Category Item Status Credentials No hardcoded secrets Required Data Encrypted storage Recommended Data Access logging Recommended Model Checksum verification Recommended Deploy Non-root containers Required Deploy Rate limiting Recommended Privacy Resolution assessment Project-specific"},{"location":"architecture/security-privacy/#reporting-issues","title":"Reporting Issues","text":"<p>For security vulnerabilities, see SECURITY.md.</p>"},{"location":"guides/benchmarking/","title":"Benchmarking","text":"<p>This guide covers performance benchmarking for Ununennium models.</p>"},{"location":"guides/benchmarking/#throughput-benchmarking","title":"Throughput Benchmarking","text":""},{"location":"guides/benchmarking/#measure-training-throughput","title":"Measure Training Throughput","text":"<pre><code>import time\nimport torch\n\ndef benchmark_training(model, loader, n_batches=100):\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # Warmup\n    for batch in loader:\n        break\n    _ = model(batch[\"image\"].cuda())\n    torch.cuda.synchronize()\n\n    # Benchmark\n    start = time.perf_counter()\n    for i, batch in enumerate(loader):\n        if i &gt;= n_batches:\n            break\n\n        optimizer.zero_grad()\n        output = model(batch[\"image\"].cuda())\n        loss = output.mean()\n        loss.backward()\n        optimizer.step()\n\n    torch.cuda.synchronize()\n    elapsed = time.perf_counter() - start\n\n    throughput = n_batches * loader.batch_size / elapsed\n    print(f\"Throughput: {throughput:.1f} img/s\")\n    return throughput\n</code></pre>"},{"location":"guides/benchmarking/#measure-inference-throughput","title":"Measure Inference Throughput","text":"<pre><code>def benchmark_inference(model, input_shape, n_iterations=100):\n    model.eval()\n    x = torch.randn(input_shape).cuda()\n\n    # Warmup\n    with torch.no_grad():\n        for _ in range(10):\n            _ = model(x)\n    torch.cuda.synchronize()\n\n    # Benchmark\n    start = time.perf_counter()\n    with torch.no_grad():\n        for _ in range(n_iterations):\n            _ = model(x)\n    torch.cuda.synchronize()\n    elapsed = time.perf_counter() - start\n\n    throughput = n_iterations * input_shape[0] / elapsed\n    latency = elapsed / n_iterations * 1000\n    print(f\"Throughput: {throughput:.1f} img/s, Latency: {latency:.2f} ms\")\n</code></pre>"},{"location":"guides/benchmarking/#memory-profiling","title":"Memory Profiling","text":"<pre><code>def measure_memory(model, input_shape):\n    torch.cuda.reset_peak_memory_stats()\n\n    x = torch.randn(input_shape).cuda()\n\n    # Forward\n    output = model(x)\n    fwd_mem = torch.cuda.max_memory_allocated() / 1e9\n\n    # Backward\n    output.mean().backward()\n    total_mem = torch.cuda.max_memory_allocated() / 1e9\n\n    print(f\"Forward: {fwd_mem:.2f} GB, Total: {total_mem:.2f} GB\")\n</code></pre>"},{"location":"guides/benchmarking/#benchmark-suite","title":"Benchmark Suite","text":"<pre><code>from ununennium.benchmarks import ModelBenchmark\n\nbenchmark = ModelBenchmark(\n    model=model,\n    input_shapes=[(1, 12, 256, 256), (1, 12, 512, 512)],\n    batch_sizes=[1, 4, 8, 16],\n)\n\nresults = benchmark.run()\nresults.to_csv(\"benchmark_results.csv\")\n</code></pre>"},{"location":"guides/benchmarking/#see-also","title":"See Also","text":"<ul> <li>Performance Guide</li> </ul>"},{"location":"guides/configuration/","title":"Configuration","text":"<p>This guide covers the configuration system in Ununennium.</p>"},{"location":"guides/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"guides/configuration/#yaml-format","title":"YAML Format","text":"<pre><code># config.yaml\nmodel:\n  name: unet_resnet50\n  in_channels: 12\n  num_classes: 10\n  pretrained: true\n\ntraining:\n  epochs: 100\n  batch_size: 16\n  learning_rate: 1e-4\n  weight_decay: 1e-5\n  mixed_precision: true\n  gradient_accumulation_steps: 4\n\ndata:\n  train_paths: [\"data/train/*.tif\"]\n  val_paths: [\"data/val/*.tif\"]\n  tile_size: 256\n  overlap: 0.25\n  num_workers: 4\n\ncallbacks:\n  - type: checkpoint\n    path: checkpoints/\n    monitor: val_iou\n  - type: early_stopping\n    patience: 10\n</code></pre>"},{"location":"guides/configuration/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from ununennium.utils import load_config\n\nconfig = load_config(\"config.yaml\")\nmodel = create_model(**config[\"model\"])\n</code></pre>"},{"location":"guides/configuration/#environment-variables","title":"Environment Variables","text":"<pre><code># Override config values\nexport UU_BATCH_SIZE=32\nexport UU_DEVICE=cuda:1\n</code></pre> <pre><code>import os\nbatch_size = int(os.environ.get(\"UU_BATCH_SIZE\", config[\"training\"][\"batch_size\"]))\n</code></pre>"},{"location":"guides/configuration/#experiment-organization","title":"Experiment Organization","text":"<pre><code>experiments/\n\u251c\u2500\u2500 exp_001/\n\u2502   \u251c\u2500\u2500 config.yaml\n\u2502   \u251c\u2500\u2500 checkpoints/\n\u2502   \u251c\u2500\u2500 logs/\n\u2502   \u2514\u2500\u2500 results/\n\u2514\u2500\u2500 exp_002/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guides/configuration/#see-also","title":"See Also","text":"<ul> <li>Reproducibility</li> </ul>"},{"location":"guides/datasets-and-splits/","title":"Datasets and Splits","text":"<p>This guide covers best practices for dataset management and train/val/test splitting in geospatial ML.</p>"},{"location":"guides/datasets-and-splits/#spatial-autocorrelation","title":"Spatial Autocorrelation","text":"<p>Nearby pixels are correlated. Random splits cause data leakage.</p>"},{"location":"guides/datasets-and-splits/#correlation-range","title":"Correlation Range","text":"Land Cover Typical Range Urban 100-500 m Agriculture 500m-2 km Forest 1-10 km"},{"location":"guides/datasets-and-splits/#block-size-rule","title":"Block Size Rule","text":"\\[ \\text{block\\_size} \\geq 2 \\times \\text{correlation\\_range} \\]"},{"location":"guides/datasets-and-splits/#splitting-strategies","title":"Splitting Strategies","text":""},{"location":"guides/datasets-and-splits/#block-based-splitting","title":"Block-Based Splitting","text":"<pre><code>from ununennium.datasets import BlockSplitter\n\nsplitter = BlockSplitter(block_size=(5000, 5000))\ntrain, val, test = splitter.split(bounds, ratios=[0.7, 0.15, 0.15])\n</code></pre>"},{"location":"guides/datasets-and-splits/#spatial-k-fold","title":"Spatial K-Fold","text":"<pre><code>from ununennium.datasets import SpatialKFold\n\ncv = SpatialKFold(n_folds=5, buffer=1000)\nfor train_idx, val_idx in cv.split(data):\n    # Train and evaluate\n    pass\n</code></pre>"},{"location":"guides/datasets-and-splits/#class-imbalance","title":"Class Imbalance","text":"<p>Use stratified spatial sampling:</p> <pre><code>from ununennium.datasets import StratifiedBlockSampler\n\nsampler = StratifiedBlockSampler(\n    class_weights={0: 1.0, 1: 5.0, 2: 10.0},\n    block_size=(1000, 1000),\n)\n</code></pre>"},{"location":"guides/datasets-and-splits/#temporal-considerations","title":"Temporal Considerations","text":"<p>For change detection, ensure temporal separation:</p> <pre><code># Wrong: Same scene in train and val (different tiles)\n# Right: Different acquisition dates in train and val\n</code></pre>"},{"location":"guides/datasets-and-splits/#see-also","title":"See Also","text":"<ul> <li>Tutorial 02: Train/Val/Test</li> <li>Spatial Autocorrelation Theory</li> </ul>"},{"location":"guides/metrics/","title":"Metrics","text":"<p>This guide covers metric selection and interpretation for Earth observation tasks.</p>"},{"location":"guides/metrics/#segmentation-metrics","title":"Segmentation Metrics","text":""},{"location":"guides/metrics/#iou-intersection-over-union","title":"IoU (Intersection over Union)","text":"\\[ \\text{IoU} = \\frac{TP}{TP + FP + FN} \\] <ul> <li>Range: [0, 1], higher is better</li> <li>Penalizes both false positives and false negatives</li> </ul>"},{"location":"guides/metrics/#dice-coefficient","title":"Dice Coefficient","text":"\\[ \\text{Dice} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN} \\] <ul> <li>Equivalent to F1 score</li> <li>More tolerant of small objects</li> </ul>"},{"location":"guides/metrics/#when-to-use","title":"When to Use","text":"Metric Best For IoU General evaluation Dice Class imbalance Pixel Accuracy Gross overview"},{"location":"guides/metrics/#change-detection-metrics","title":"Change Detection Metrics","text":""},{"location":"guides/metrics/#overall-accuracy","title":"Overall Accuracy","text":"\\[ OA = \\frac{TP + TN}{TP + TN + FP + FN} \\]"},{"location":"guides/metrics/#kappa-coefficient","title":"Kappa Coefficient","text":"\\[ \\kappa = \\frac{p_o - p_e}{1 - p_e} \\] <p>Accounts for chance agreement.</p> Kappa Interpretation &lt; 0 Worse than chance 0-0.2 Slight 0.2-0.6 Fair to moderate 0.6-0.8 Substantial &gt; 0.8 Almost perfect"},{"location":"guides/metrics/#super-resolution-metrics","title":"Super-Resolution Metrics","text":""},{"location":"guides/metrics/#psnr","title":"PSNR","text":"\\[ \\text{PSNR} = 10 \\cdot \\log_{10}\\left(\\frac{MAX^2}{MSE}\\right) \\] <p>Higher is better. Typical range: 20-40 dB.</p>"},{"location":"guides/metrics/#ssim","title":"SSIM","text":"<p>Measures structural similarity. Range [0, 1].</p>"},{"location":"guides/metrics/#calibration-metrics","title":"Calibration Metrics","text":""},{"location":"guides/metrics/#ece-expected-calibration-error","title":"ECE (Expected Calibration Error)","text":"<p>Measures alignment between confidence and accuracy.</p> \\[ \\text{ECE} = \\sum_m \\frac{|B_m|}{n} \\cdot |\\text{acc}(B_m) - \\text{conf}(B_m)| \\] <p>Lower is better.</p>"},{"location":"guides/metrics/#see-also","title":"See Also","text":"<ul> <li>Evaluation API</li> <li>Uncertainty and Calibration</li> </ul>"},{"location":"guides/reproducibility/","title":"Reproducibility","text":"<p>This guide covers practices for reproducible geospatial ML experiments.</p>"},{"location":"guides/reproducibility/#random-seeds","title":"Random Seeds","text":""},{"location":"guides/reproducibility/#setting-seeds","title":"Setting Seeds","text":"<pre><code>import torch\nimport numpy as np\nimport random\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # Deterministic operations\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"guides/reproducibility/#per-experiment-seeds","title":"Per-Experiment Seeds","text":"<pre><code>experiment:\n  seed: 42\n  data_seed: 123\n  model_seed: 456\n</code></pre>"},{"location":"guides/reproducibility/#configuration-management","title":"Configuration Management","text":""},{"location":"guides/reproducibility/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>model:\n  name: unet_resnet50\n  in_channels: 12\n  num_classes: 10\n\ntraining:\n  epochs: 100\n  batch_size: 16\n  learning_rate: 1e-4\n\ndata:\n  tile_size: 256\n  overlap: 0.25\n</code></pre>"},{"location":"guides/reproducibility/#logging-configuration","title":"Logging Configuration","text":"<pre><code>from ununennium.utils import save_config\n\nsave_config(config, \"experiment_001/config.yaml\")\n</code></pre>"},{"location":"guides/reproducibility/#checkpointing","title":"Checkpointing","text":"<pre><code>from ununennium.training import CheckpointCallback\n\ncallback = CheckpointCallback(\n    path=\"checkpoints/\",\n    save_optimizer=True,\n    save_config=True,\n)\n</code></pre>"},{"location":"guides/reproducibility/#version-tracking","title":"Version Tracking","text":"<pre><code>import ununennium\n\nprint(f\"ununennium: {ununennium.__version__}\")\nprint(f\"torch: {torch.__version__}\")\nprint(f\"numpy: {np.__version__}\")\n</code></pre>"},{"location":"guides/reproducibility/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide</li> <li>CONTRIBUTING.md</li> </ul>"},{"location":"guides/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions.</p>"},{"location":"guides/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"guides/troubleshooting/#gdal-not-found","title":"GDAL Not Found","text":"<pre><code># Ubuntu/Debian\nsudo apt-get install libgdal-dev\n\n# macOS\nbrew install gdal\n\n# Then install\npip install \"ununennium[geo]\"\n</code></pre>"},{"location":"guides/troubleshooting/#cuda-version-mismatch","title":"CUDA Version Mismatch","text":"<pre><code># Check CUDA version\nnvidia-smi\n\n# Install matching PyTorch\npip install torch --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"guides/troubleshooting/#data-loading-issues","title":"Data Loading Issues","text":""},{"location":"guides/troubleshooting/#crs-error","title":"CRS Error","text":"<pre><code>CRSError: Invalid CRS specification\n</code></pre> <p>Solution: Ensure your GeoTIFF has a valid CRS:</p> <pre><code>import rasterio\nwith rasterio.open(\"image.tif\") as src:\n    print(src.crs)  # Should not be None\n</code></pre>"},{"location":"guides/troubleshooting/#memory-error","title":"Memory Error","text":"<pre><code>RuntimeError: CUDA out of memory\n</code></pre> <p>Solutions: 1. Reduce batch size 2. Use smaller tiles 3. Enable gradient checkpointing 4. Use mixed precision</p>"},{"location":"guides/troubleshooting/#training-issues","title":"Training Issues","text":""},{"location":"guides/troubleshooting/#nan-loss","title":"NaN Loss","text":"<p>Causes: - Learning rate too high - Division by zero in indices</p> <p>Solutions: <pre><code># Lower learning rate\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Use epsilon in computations\nndvi = (nir - red) / (nir + red + 1e-8)\n</code></pre></p>"},{"location":"guides/troubleshooting/#validation-better-than-test","title":"Validation Better Than Test","text":"<p>Cause: Spatial leakage from random splitting.</p> <p>Solution: Use block-based splitting.</p>"},{"location":"guides/troubleshooting/#inference-issues","title":"Inference Issues","text":""},{"location":"guides/troubleshooting/#seam-artifacts","title":"Seam Artifacts","text":"<p>Solution: Increase overlap and use blending:</p> <pre><code>inferencer = SlidingWindowInference(\n    model=model,\n    overlap=0.5,\n    blend_mode=\"cosine\",\n)\n</code></pre>"},{"location":"guides/troubleshooting/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues</li> <li>SUPPORT.md</li> </ul>"},{"location":"guides/uncertainty-and-calibration/","title":"Uncertainty and Calibration","text":"<p>This guide covers uncertainty quantification and calibration for geospatial models.</p>"},{"location":"guides/uncertainty-and-calibration/#types-of-uncertainty","title":"Types of Uncertainty","text":""},{"location":"guides/uncertainty-and-calibration/#aleatoric-uncertainty","title":"Aleatoric Uncertainty","text":"<p>Irreducible uncertainty from data noise.</p>"},{"location":"guides/uncertainty-and-calibration/#epistemic-uncertainty","title":"Epistemic Uncertainty","text":"<p>Model uncertainty, reducible with more data.</p>"},{"location":"guides/uncertainty-and-calibration/#calibration","title":"Calibration","text":"<p>A calibrated model satisfies:</p> \\[ P(Y = y | \\hat{p} = p) = p \\]"},{"location":"guides/uncertainty-and-calibration/#reliability-diagram","title":"Reliability Diagram","text":"<pre><code>from ununennium.visualization import reliability_diagram\n\nreliability_diagram(probabilities, targets, n_bins=10)\nplt.savefig(\"reliability.png\")\n</code></pre>"},{"location":"guides/uncertainty-and-calibration/#temperature-scaling","title":"Temperature Scaling","text":"<pre><code>from ununennium.calibration import TemperatureScaling\n\ncalibrator = TemperatureScaling()\ncalibrator.fit(val_logits, val_targets)\n\ncalibrated_probs = calibrator(test_logits)\n</code></pre>"},{"location":"guides/uncertainty-and-calibration/#metrics","title":"Metrics","text":""},{"location":"guides/uncertainty-and-calibration/#ece","title":"ECE","text":"\\[ \\text{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{n} \\left| \\text{acc}(B_m) - \\text{conf}(B_m) \\right| \\]"},{"location":"guides/uncertainty-and-calibration/#brier-score","title":"Brier Score","text":"\\[ \\text{Brier} = \\frac{1}{n} \\sum_{i=1}^{n} (p_i - y_i)^2 \\]"},{"location":"guides/uncertainty-and-calibration/#mc-dropout","title":"MC Dropout","text":"<pre><code>model.train()  # Enable dropout\n\npredictions = []\nfor _ in range(100):\n    with torch.no_grad():\n        pred = model(x)\n        predictions.append(pred)\n\nmean = torch.stack(predictions).mean(0)\nuncertainty = torch.stack(predictions).std(0)\n</code></pre>"},{"location":"guides/uncertainty-and-calibration/#see-also","title":"See Also","text":"<ul> <li>Evaluation API</li> <li>Metrics Guide</li> </ul>"},{"location":"research/bibliography/","title":"Bibliography","text":"<p>Curated references for Earth observation machine learning. All entries include DOIs or permanent links.</p>"},{"location":"research/bibliography/#foundational-deep-learning","title":"Foundational Deep Learning","text":""},{"location":"research/bibliography/#neural-networks","title":"Neural Networks","text":"<ol> <li> <p>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.    DOI: 10.1038/nature14539</p> </li> <li> <p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. CVPR.    DOI: 10.1109/CVPR.2016.90</p> </li> <li> <p>Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS, 30.    arXiv:1706.03762</p> </li> </ol>"},{"location":"research/bibliography/#optimization","title":"Optimization","text":"<ol> <li> <p>Kingma, D.P., &amp; Ba, J. (2015). Adam: A Method for Stochastic Optimization. ICLR.    arXiv:1412.6980</p> </li> <li> <p>Loshchilov, I., &amp; Hutter, F. (2019). Decoupled Weight Decay Regularization. ICLR.    arXiv:1711.05101</p> </li> </ol>"},{"location":"research/bibliography/#semantic-segmentation","title":"Semantic Segmentation","text":"<ol> <li> <p>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI.    DOI: 10.1007/978-3-319-24574-4_28</p> </li> <li> <p>Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., &amp; Adam, H. (2018). Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. ECCV.    DOI: 10.1007/978-3-030-01234-2_49</p> </li> <li> <p>Kirillov, A., et al. (2023). Segment Anything. ICCV.    arXiv:2304.02643</p> </li> </ol>"},{"location":"research/bibliography/#object-detection","title":"Object Detection","text":"<ol> <li> <p>Lin, T.Y., et al. (2017). Focal Loss for Dense Object Detection. ICCV.    DOI: 10.1109/ICCV.2017.324</p> </li> <li> <p>He, K., et al. (2017). Mask R-CNN. ICCV.     DOI: 10.1109/ICCV.2017.322</p> </li> </ol>"},{"location":"research/bibliography/#change-detection","title":"Change Detection","text":"<ol> <li> <p>Daudt, R.C., Le Saux, B., &amp; Boulch, A. (2018). Fully Convolutional Siamese Networks for Change Detection. ICIP.     DOI: 10.1109/ICIP.2018.8451652</p> </li> <li> <p>Chen, H., &amp; Shi, Z. (2020). A Spatial-Temporal Attention-Based Method and a New Dataset for Remote Sensing Image Change Detection. Remote Sensing, 12(10), 1662.     DOI: 10.3390/rs12101662</p> </li> </ol>"},{"location":"research/bibliography/#generative-adversarial-networks","title":"Generative Adversarial Networks","text":"<ol> <li> <p>Goodfellow, I., et al. (2014). Generative Adversarial Nets. NeurIPS, 27.     arXiv:1406.2661</p> </li> <li> <p>Isola, P., Zhu, J.Y., Zhou, T., &amp; Efros, A.A. (2017). Image-to-Image Translation with Conditional Adversarial Networks. CVPR.     DOI: 10.1109/CVPR.2017.632</p> </li> <li> <p>Zhu, J.Y., Park, T., Isola, P., &amp; Efros, A.A. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. ICCV.     DOI: 10.1109/ICCV.2017.244</p> </li> <li> <p>Wang, X., et al. (2018). ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks. ECCV Workshops.     arXiv:1809.00219</p> </li> <li> <p>Ledig, C., et al. (2017). Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. CVPR.     DOI: 10.1109/CVPR.2017.19</p> </li> </ol>"},{"location":"research/bibliography/#physics-informed-neural-networks","title":"Physics-Informed Neural Networks","text":"<ol> <li> <p>Raissi, M., Perdikaris, P., &amp; Karniadakis, G.E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378, 686-707.     DOI: 10.1016/j.jcp.2018.10.045</p> </li> <li> <p>Karniadakis, G.E., et al. (2021). Physics-informed machine learning. Nature Reviews Physics, 3(6), 422-440.     DOI: 10.1038/s42254-021-00314-5</p> </li> <li> <p>Lu, L., et al. (2021). DeepXDE: A deep learning library for solving differential equations. SIAM Review, 63(1), 208-228.     DOI: 10.1137/19M1274067</p> </li> </ol>"},{"location":"research/bibliography/#remote-sensing-and-deep-learning","title":"Remote Sensing and Deep Learning","text":"<ol> <li> <p>Zhu, X.X., et al. (2017). Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources. IEEE Geoscience and Remote Sensing Magazine, 5(4), 8-36.     DOI: 10.1109/MGRS.2017.2762307</p> </li> <li> <p>Ma, L., et al. (2019). Deep learning in remote sensing applications: A meta-analysis and review. ISPRS Journal of Photogrammetry and Remote Sensing, 152, 166-177.     DOI: 10.1016/j.isprsjprs.2019.04.015</p> </li> </ol>"},{"location":"research/bibliography/#spectral-indices","title":"Spectral Indices","text":"<ol> <li> <p>Tucker, C.J. (1979). Red and photographic infrared linear combinations for monitoring vegetation. Remote Sensing of Environment, 8(2), 127-150.     DOI: 10.1016/0034-4257(79)90013-0</p> </li> <li> <p>Huete, A., et al. (2002). Overview of the radiometric and biophysical performance of the MODIS vegetation indices. Remote Sensing of Environment, 83(1-2), 195-213.     DOI: 10.1016/S0034-4257(02)00096-2</p> </li> <li> <p>McFeeters, S.K. (1996). The use of the Normalized Difference Water Index (NDWI) in the delineation of open water features. International Journal of Remote Sensing, 17(7), 1425-1432.     DOI: 10.1080/01431169608948714</p> </li> </ol>"},{"location":"research/bibliography/#spatial-statistics","title":"Spatial Statistics","text":"<ol> <li> <p>Moran, P.A.P. (1950). Notes on Continuous Stochastic Phenomena. Biometrika, 37(1/2), 17-23.     DOI: 10.2307/2332142</p> </li> <li> <p>Matheron, G. (1963). Principles of geostatistics. Economic Geology, 58(8), 1246-1266.     DOI: 10.2113/gsecongeo.58.8.1246</p> </li> <li> <p>Roberts, D.R., et al. (2017). Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure. Ecography, 40(8), 913-929.     DOI: 10.1111/ecog.02881</p> </li> </ol>"},{"location":"research/bibliography/#calibration-and-uncertainty","title":"Calibration and Uncertainty","text":"<ol> <li> <p>Guo, C., et al. (2017). On Calibration of Modern Neural Networks. ICML.     arXiv:1706.04599</p> </li> <li> <p>Kendall, A., &amp; Gal, Y. (2017). What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? NeurIPS.     arXiv:1703.04977</p> </li> </ol>"},{"location":"research/bibliography/#benchmark-datasets","title":"Benchmark Datasets","text":"<ol> <li> <p>Helber, P., et al. (2019). EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7), 2217-2226.     DOI: 10.1109/JSTARS.2019.2918242</p> </li> <li> <p>Yang, Y., &amp; Newsam, S. (2010). Bag-of-Visual-Words and Spatial Extensions for Land-Use Classification. ACM SIGSPATIAL.     DOI: 10.1145/1869790.1869829</p> </li> <li> <p>Wang, J., et al. (2021). LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. NeurIPS Datasets and Benchmarks.     arXiv:2110.08733</p> </li> </ol>"},{"location":"research/bibliography/#textbooks","title":"Textbooks","text":"<ol> <li> <p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.     Link</p> </li> <li> <p>Jensen, J.R. (2015). Introductory Digital Image Processing: A Remote Sensing Perspective (4th ed.). Pearson.     ISBN: 978-0134058160</p> </li> <li> <p>Richards, J.A. (2022). Remote Sensing Digital Image Analysis (6th ed.). Springer.     DOI: 10.1007/978-3-030-82327-6</p> </li> </ol>"},{"location":"research/bibliography/#data-sources","title":"Data Sources","text":""},{"location":"research/bibliography/#satellite-data-portals","title":"Satellite Data Portals","text":"Source URL Data Copernicus Open Access Hub https://scihub.copernicus.eu/ Sentinel-1/2/3 USGS Earth Explorer https://earthexplorer.usgs.gov/ Landsat, MODIS AWS Earth Search https://earth-search.aws.element84.com/v1 STAC catalog"},{"location":"research/bibliography/#standards","title":"Standards","text":"Standard URL Cloud Optimized GeoTIFF https://www.cogeo.org/ STAC Specification https://stacspec.org/ Zarr https://zarr.dev/ <p>Last updated: December 16, 2025</p>"},{"location":"research/gan-theory/","title":"GAN Theory","text":"<p>Theoretical foundations of Generative Adversarial Networks for Earth observation image translation and enhancement.</p>"},{"location":"research/gan-theory/#adversarial-learning-framework","title":"Adversarial Learning Framework","text":""},{"location":"research/gan-theory/#minimax-game","title":"Minimax Game","text":"<p>GANs consist of two networks in a zero-sum game:</p> \\[ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))] \\] <p>where: - \\(G\\): Generator network - \\(D\\): Discriminator network - \\(p_{data}\\): Real data distribution - \\(p_z\\): Prior noise distribution</p> <p>Reference: Goodfellow, I., et al. (2014). Generative Adversarial Nets. Advances in Neural Information Processing Systems, 27. arXiv:1406.2661</p>"},{"location":"research/gan-theory/#nash-equilibrium","title":"Nash Equilibrium","text":"<p>At optimal convergence: - \\(p_G = p_{data}\\) (generator perfectly matches data) - \\(D(x) = 0.5\\) for all \\(x\\) (discriminator cannot distinguish)</p>"},{"location":"research/gan-theory/#loss-functions","title":"Loss Functions","text":""},{"location":"research/gan-theory/#vanilla-gan-loss","title":"Vanilla GAN Loss","text":"<p>Discriminator: $$ \\mathcal{L}D = -\\mathbb{E}{x \\sim p_{data}}[\\log D(x)] - \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))] $$</p> <p>Generator: $$ \\mathcal{L}G = -\\mathbb{E}{z \\sim p_z}[\\log D(G(z))] $$</p>"},{"location":"research/gan-theory/#least-squares-gan-lsgan","title":"Least Squares GAN (LSGAN)","text":"<p>More stable training using squared error:</p> \\[ \\mathcal{L}_D = \\frac{1}{2}\\mathbb{E}_{x}[(D(x) - 1)^2] + \\frac{1}{2}\\mathbb{E}_{z}[D(G(z))^2] \\] \\[ \\mathcal{L}_G = \\frac{1}{2}\\mathbb{E}_{z}[(D(G(z)) - 1)^2] \\] <p>Reference: Mao, X., et al. (2017). Least Squares Generative Adversarial Networks. ICCV. DOI: 10.1109/ICCV.2017.304</p>"},{"location":"research/gan-theory/#wasserstein-gan-wgan","title":"Wasserstein GAN (WGAN)","text":"<p>Uses Earth Mover distance for improved gradients:</p> \\[ \\mathcal{L}_D = \\mathbb{E}_{z}[D(G(z))] - \\mathbb{E}_{x}[D(x)] \\] <p>with Lipschitz constraint on \\(D\\).</p> <p>Reference: Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). Wasserstein GAN. ICML. arXiv:1701.07875</p>"},{"location":"research/gan-theory/#hinge-loss","title":"Hinge Loss","text":"\\[ \\mathcal{L}_D = \\mathbb{E}_{x}[\\max(0, 1 - D(x))] + \\mathbb{E}_{z}[\\max(0, 1 + D(G(z)))] \\] \\[ \\mathcal{L}_G = -\\mathbb{E}_{z}[D(G(z))] \\] <p>Reference: Lim, J.H., &amp; Ye, J.C. (2017). Geometric GAN. arXiv:1705.02894</p>"},{"location":"research/gan-theory/#image-translation-architectures","title":"Image Translation Architectures","text":""},{"location":"research/gan-theory/#pix2pix-paired-translation","title":"Pix2Pix (Paired Translation)","text":"<p>Conditional GAN with L1 reconstruction loss:</p> \\[ \\mathcal{L} = \\mathcal{L}_{cGAN}(G, D) + \\lambda \\mathcal{L}_{L1}(G) \\] <p>where: $$ \\mathcal{L}{L1}(G) = \\mathbb{E}{x,y}[|y - G(x)|_1] $$</p> <p>Architecture: - Generator: U-Net with skip connections - Discriminator: PatchGAN (classifies 70x70 patches)</p> <p>Reference: Isola, P., et al. (2017). Image-to-Image Translation with Conditional Adversarial Networks. CVPR. DOI: 10.1109/CVPR.2017.632</p>"},{"location":"research/gan-theory/#cyclegan-unpaired-translation","title":"CycleGAN (Unpaired Translation)","text":"<p>For unpaired training using cycle consistency:</p> \\[ \\mathcal{L}_{cyc}(G, F) = \\mathbb{E}_{x}[\\|F(G(x)) - x\\|_1] + \\mathbb{E}_{y}[\\|G(F(y)) - y\\|_1] \\] <p>Total Loss: $$ \\mathcal{L} = \\mathcal{L}{GAN}(G, D_Y) + \\mathcal{L}{GAN}(F, D_X) + \\lambda_{cyc}\\mathcal{L}{cyc} + \\lambda{id}\\mathcal{L}_{id} $$</p> <p>Reference: Zhu, J.Y., et al. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. ICCV. DOI: 10.1109/ICCV.2017.244</p>"},{"location":"research/gan-theory/#super-resolution-gans","title":"Super-Resolution GANs","text":""},{"location":"research/gan-theory/#srgan","title":"SRGAN","text":"<p>Perceptual loss for natural textures:</p> \\[ \\mathcal{L}_{percep} = \\sum_{l} \\frac{1}{C_l H_l W_l} \\|\\phi_l(I_{HR}) - \\phi_l(G(I_{LR}))\\|_2^2 \\] <p>where \\(\\phi_l\\) are VGG19 feature maps.</p> <p>Reference: Ledig, C., et al. (2017). Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. CVPR. DOI: 10.1109/CVPR.2017.19</p>"},{"location":"research/gan-theory/#esrgan","title":"ESRGAN","text":"<p>Enhanced version with: - RRDB (Residual-in-Residual Dense Block) - Relativistic discriminator - Perceptual loss before activation</p> <p>Reference: Wang, X., et al. (2018). ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks. ECCV Workshops. arXiv:1809.00219</p>"},{"location":"research/gan-theory/#training-stability","title":"Training Stability","text":""},{"location":"research/gan-theory/#spectral-normalization","title":"Spectral Normalization","text":"<p>Normalizes weight matrices by spectral norm:</p> \\[ \\bar{W} = \\frac{W}{\\sigma(W)} \\] <p>where \\(\\sigma(W)\\) is the largest singular value.</p> <p>Reference: Miyato, T., et al. (2018). Spectral Normalization for Generative Adversarial Networks. ICLR. arXiv:1802.05957</p>"},{"location":"research/gan-theory/#gradient-penalty-wgan-gp","title":"Gradient Penalty (WGAN-GP)","text":"\\[ \\mathcal{L}_{GP} = \\lambda \\mathbb{E}_{\\hat{x}}[(\\|\\nabla_{\\hat{x}} D(\\hat{x})\\|_2 - 1)^2] \\] <p>where \\(\\hat{x}\\) is interpolated between real and fake.</p> <p>Reference: Gulrajani, I., et al. (2017). Improved Training of Wasserstein GANs. NeurIPS. arXiv:1704.00028</p>"},{"location":"research/gan-theory/#earth-observation-applications","title":"Earth Observation Applications","text":"Application Architecture Input Output SAR-to-Optical Pix2Pix SAR RGB Cloud Removal CycleGAN Cloudy Clear Pan-sharpening SRGAN MS + PAN HR MS Resolution Enhancement ESRGAN 10m 2.5m"},{"location":"research/gan-theory/#see-also","title":"See Also","text":"<ul> <li>Math Foundations</li> <li>PINN Theory</li> <li>GAN API Reference</li> </ul>"},{"location":"research/math-foundations/","title":"Mathematical Foundations","text":"<p>This document establishes the mathematical foundations for Earth observation machine learning, covering coordinate systems, spectral indices, spatial statistics, and deep learning fundamentals.</p>"},{"location":"research/math-foundations/#coordinate-reference-systems","title":"Coordinate Reference Systems","text":""},{"location":"research/math-foundations/#geodetic-datum","title":"Geodetic Datum","text":"<p>The World Geodetic System 1984 (WGS84) defines the reference ellipsoid:</p> \\[ \\frac{x^2 + y^2}{a^2} + \\frac{z^2}{b^2} = 1 \\] <p>where \\(a = 6378137\\) m (semi-major axis) and \\(b = 6356752.314\\) m (semi-minor axis).</p> <p>Reference: National Imagery and Mapping Agency (2000). Department of Defense World Geodetic System 1984. NIMA TR8350.2. Link</p>"},{"location":"research/math-foundations/#map-projections","title":"Map Projections","text":""},{"location":"research/math-foundations/#universal-transverse-mercator-utm","title":"Universal Transverse Mercator (UTM)","text":"<p>UTM divides Earth into 60 zones, each 6 degrees wide:</p> \\[ \\text{zone} = \\left\\lfloor \\frac{\\lambda + 180}{6} \\right\\rfloor + 1 \\] <p>The forward transformation from geodetic coordinates \\((\\phi, \\lambda)\\) to UTM \\((E, N)\\):</p> \\[ E = E_0 + k_0 \\nu \\left( A + \\frac{(1-T+C)A^3}{6} + \\frac{(5-18T+T^2+72C-58e'^2)A^5}{120} \\right) \\] <p>where \\(\\nu = a/\\sqrt{1 - e^2\\sin^2\\phi}\\), \\(T = \\tan^2\\phi\\), and \\(k_0 = 0.9996\\).</p> <p>Reference: Snyder, J.P. (1987). Map Projections: A Working Manual. USGS Professional Paper 1395. DOI: 10.3133/pp1395</p>"},{"location":"research/math-foundations/#resampling-theory","title":"Resampling Theory","text":""},{"location":"research/math-foundations/#interpolation-kernels","title":"Interpolation Kernels","text":"<p>Image resampling requires interpolation from discrete samples:</p> \\[ f(x) = \\sum_{k} f[k] \\cdot h(x - k) \\]"},{"location":"research/math-foundations/#bicubic-interpolation","title":"Bicubic Interpolation","text":"<p>The Keys cubic convolution kernel:</p> \\[ h(x) = \\begin{cases} (a+2)|x|^3 - (a+3)|x|^2 + 1 &amp; |x| \\leq 1 \\\\ a|x|^3 - 5a|x|^2 + 8a|x| - 4a &amp; 1 &lt; |x| &lt; 2 \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <p>with \\(a = -0.5\\) for optimal approximation.</p> <p>Reference: Keys, R. (1981). Cubic convolution interpolation for digital image processing. IEEE Transactions on Acoustics, Speech, and Signal Processing, 29(6), 1153-1160. DOI: 10.1109/TASSP.1981.1163711</p>"},{"location":"research/math-foundations/#spectral-indices","title":"Spectral Indices","text":""},{"location":"research/math-foundations/#normalized-difference-formulation","title":"Normalized Difference Formulation","text":"<p>The general normalized difference index:</p> \\[ \\text{NDI}_{i,j} = \\frac{\\rho_i - \\rho_j}{\\rho_i + \\rho_j} \\]"},{"location":"research/math-foundations/#vegetation-indices","title":"Vegetation Indices","text":""},{"location":"research/math-foundations/#ndvi-normalized-difference-vegetation-index","title":"NDVI (Normalized Difference Vegetation Index)","text":"\\[ \\text{NDVI} = \\frac{\\rho_{NIR} - \\rho_{Red}}{\\rho_{NIR} + \\rho_{Red}} \\] <p>NDVI ranges from -1 to +1, with healthy vegetation typically 0.2-0.9.</p> <p>Reference: Tucker, C.J. (1979). Red and photographic infrared linear combinations for monitoring vegetation. Remote Sensing of Environment, 8(2), 127-150. DOI: 10.1016/0034-4257(79)90013-0</p>"},{"location":"research/math-foundations/#evi-enhanced-vegetation-index","title":"EVI (Enhanced Vegetation Index)","text":"\\[ \\text{EVI} = G \\cdot \\frac{\\rho_{NIR} - \\rho_{Red}}{\\rho_{NIR} + C_1 \\cdot \\rho_{Red} - C_2 \\cdot \\rho_{Blue} + L} \\] <p>with \\(G = 2.5\\), \\(C_1 = 6\\), \\(C_2 = 7.5\\), and \\(L = 1\\).</p> <p>Reference: Huete, A., et al. (2002). Overview of the radiometric and biophysical performance of the MODIS vegetation indices. Remote Sensing of Environment, 83(1-2), 195-213. DOI: 10.1016/S0034-4257(02)00096-2</p>"},{"location":"research/math-foundations/#water-indices","title":"Water Indices","text":""},{"location":"research/math-foundations/#ndwi-normalized-difference-water-index","title":"NDWI (Normalized Difference Water Index)","text":"\\[ \\text{NDWI} = \\frac{\\rho_{Green} - \\rho_{NIR}}{\\rho_{Green} + \\rho_{NIR}} \\] <p>Reference: McFeeters, S.K. (1996). The use of the Normalized Difference Water Index (NDWI) in the delineation of open water features. International Journal of Remote Sensing, 17(7), 1425-1432. DOI: 10.1080/01431169608948714</p>"},{"location":"research/math-foundations/#spatial-statistics","title":"Spatial Statistics","text":""},{"location":"research/math-foundations/#spatial-autocorrelation","title":"Spatial Autocorrelation","text":""},{"location":"research/math-foundations/#morans-i","title":"Moran's I","text":"<p>Global measure of spatial autocorrelation:</p> \\[ I = \\frac{n}{\\sum_i \\sum_j w_{ij}} \\cdot \\frac{\\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i (x_i - \\bar{x})^2} \\] Value Interpretation \\(I &gt; 0\\) Positive autocorrelation (clustering) \\(I \\approx 0\\) Random pattern \\(I &lt; 0\\) Negative autocorrelation (dispersion) <p>Reference: Moran, P.A.P. (1950). Notes on Continuous Stochastic Phenomena. Biometrika, 37(1/2), 17-23. DOI: 10.2307/2332142</p>"},{"location":"research/math-foundations/#semivariogram","title":"Semivariogram","text":"<p>The semivariance function:</p> \\[ \\gamma(h) = \\frac{1}{2|N(h)|} \\sum_{(i,j) \\in N(h)} (z_i - z_j)^2 \\] <p>Components: - Nugget (\\(c_0\\)): Variance at zero distance - Sill (\\(c_0 + c\\)): Total variance - Range (\\(a\\)): Distance where autocorrelation ceases</p> <p>Reference: Matheron, G. (1963). Principles of geostatistics. Economic Geology, 58(8), 1246-1266. DOI: 10.2113/gsecongeo.58.8.1246</p>"},{"location":"research/math-foundations/#deep-learning-foundations","title":"Deep Learning Foundations","text":""},{"location":"research/math-foundations/#convolutional-neural-networks","title":"Convolutional Neural Networks","text":"<p>The discrete 2D convolution:</p> \\[ (I * K)[i,j] = \\sum_m \\sum_n I[i+m, j+n] \\cdot K[m, n] \\] <p>Reference: LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444. DOI: 10.1038/nature14539</p>"},{"location":"research/math-foundations/#attention-mechanism","title":"Attention Mechanism","text":"<p>Scaled dot-product attention:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>Reference: Vaswani, A., et al. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 30. arXiv:1706.03762</p>"},{"location":"research/math-foundations/#loss-functions","title":"Loss Functions","text":""},{"location":"research/math-foundations/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<p>For classification:</p> \\[ \\mathcal{L}_{CE} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c) \\]"},{"location":"research/math-foundations/#dice-loss","title":"Dice Loss","text":"<p>For segmentation with class imbalance:</p> \\[ \\mathcal{L}_{Dice} = 1 - \\frac{2 \\sum_i p_i g_i}{\\sum_i p_i + \\sum_i g_i} \\] <p>Reference: Milletari, F., Navab, N., &amp; Ahmadi, S.A. (2016). V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation. 3DV 2016. DOI: 10.1109/3DV.2016.79</p>"},{"location":"research/math-foundations/#see-also","title":"See Also","text":"<ul> <li>GAN Theory</li> <li>PINN Theory</li> <li>Bibliography</li> </ul>"},{"location":"research/pinn-theory/","title":"PINN Theory","text":"<p>Theoretical foundations of Physics-Informed Neural Networks for scientific computing in Earth observation.</p>"},{"location":"research/pinn-theory/#overview","title":"Overview","text":"<p>Physics-Informed Neural Networks (PINNs) incorporate physical laws directly into the neural network training process by encoding differential equations as soft constraints.</p> <pre><code>graph LR\n    subgraph \"PINN Framework\"\n        X[Coordinates x, t] --&gt; NN[Neural Network]\n        NN --&gt; U[Solution u]\n\n        U --&gt; L_DATA[Data Loss]\n        U --&gt; L_PDE[PDE Residual]\n        U --&gt; L_BC[Boundary Loss]\n        U --&gt; L_IC[Initial Loss]\n    end\n\n    L_DATA --&gt; TOTAL[Total Loss]\n    L_PDE --&gt; TOTAL\n    L_BC --&gt; TOTAL\n    L_IC --&gt; TOTAL\n</code></pre> <p>Reference: Raissi, M., Perdikaris, P., &amp; Karniadakis, G.E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378, 686-707. DOI: 10.1016/j.jcp.2018.10.045</p>"},{"location":"research/pinn-theory/#mathematical-formulation","title":"Mathematical Formulation","text":""},{"location":"research/pinn-theory/#problem-setup","title":"Problem Setup","text":"<p>Consider a general PDE:</p> \\[ \\mathcal{N}[u](x, t) = 0, \\quad x \\in \\Omega, \\quad t \\in [0, T] \\] <p>with boundary conditions: $$ \\mathcal{B}u = 0, \\quad x \\in \\partial\\Omega $$</p> <p>and initial condition: $$ u(x, 0) = u_0(x) $$</p>"},{"location":"research/pinn-theory/#neural-network-approximation","title":"Neural Network Approximation","text":"<p>Approximate the solution with a neural network:</p> \\[ u(x, t) \\approx u_\\theta(x, t) \\] <p>where \\(\\theta\\) are the learnable parameters.</p>"},{"location":"research/pinn-theory/#composite-loss-function","title":"Composite Loss Function","text":"\\[ \\mathcal{L}(\\theta) = \\lambda_d \\mathcal{L}_{data} + \\lambda_p \\mathcal{L}_{pde} + \\lambda_b \\mathcal{L}_{bc} + \\lambda_i \\mathcal{L}_{ic} \\] <p>where:</p> \\[ \\mathcal{L}_{data} = \\frac{1}{N_d}\\sum_{i=1}^{N_d}\\|u_\\theta(x_i, t_i) - u_i^{obs}\\|^2 \\] \\[ \\mathcal{L}_{pde} = \\frac{1}{N_c}\\sum_{j=1}^{N_c}\\|\\mathcal{N}[u_\\theta](x_j, t_j)\\|^2 \\] \\[ \\mathcal{L}_{bc} = \\frac{1}{N_b}\\sum_{k=1}^{N_b}\\|\\mathcal{B}[u_\\theta](x_k, t_k)\\|^2 \\] \\[ \\mathcal{L}_{ic} = \\frac{1}{N_i}\\sum_{l=1}^{N_i}\\|u_\\theta(x_l, 0) - u_0(x_l)\\|^2 \\]"},{"location":"research/pinn-theory/#automatic-differentiation","title":"Automatic Differentiation","text":"<p>PDE residuals are computed via automatic differentiation:</p> <pre><code>def compute_pde_residual(network, x, t):\n    coords = torch.cat([x, t], dim=-1)\n    coords.requires_grad_(True)\n\n    u = network(coords)\n\n    # First-order derivatives\n    grad_u = torch.autograd.grad(\n        u, coords,\n        grad_outputs=torch.ones_like(u),\n        create_graph=True\n    )[0]\n    u_x, u_t = grad_u[:, 0:1], grad_u[:, 1:2]\n\n    # Second-order derivatives\n    u_xx = torch.autograd.grad(\n        u_x, coords,\n        grad_outputs=torch.ones_like(u_x),\n        create_graph=True\n    )[0][:, 0:1]\n\n    return u_t, u_x, u_xx\n</code></pre>"},{"location":"research/pinn-theory/#common-pdes-in-earth-observation","title":"Common PDEs in Earth Observation","text":""},{"location":"research/pinn-theory/#heatdiffusion-equation","title":"Heat/Diffusion Equation","text":"\\[ \\frac{\\partial u}{\\partial t} = D \\nabla^2 u \\] <p>Applications: Land surface temperature diffusion, thermal anomaly propagation</p> <p>Reference: Fourier, J. (1822). Th\u00e9orie analytique de la chaleur. Firmin Didot.</p>"},{"location":"research/pinn-theory/#advection-equation","title":"Advection Equation","text":"\\[ \\frac{\\partial u}{\\partial t} + \\mathbf{v} \\cdot \\nabla u = 0 \\] <p>Applications: Pollution transport, sediment transport</p>"},{"location":"research/pinn-theory/#advection-diffusion-equation","title":"Advection-Diffusion Equation","text":"\\[ \\frac{\\partial u}{\\partial t} + \\mathbf{v} \\cdot \\nabla u = D \\nabla^2 u + S \\] <p>Applications: Sea surface temperature, air quality modeling</p> <p>Reference: Okubo, A. (1971). Oceanic diffusion diagrams. Deep Sea Research, 18(8), 789-802. DOI: 10.1016/0011-7471(71)90046-5</p>"},{"location":"research/pinn-theory/#navier-stokes-equations","title":"Navier-Stokes Equations","text":"\\[ \\frac{\\partial \\mathbf{u}}{\\partial t} + (\\mathbf{u} \\cdot \\nabla)\\mathbf{u} = -\\frac{1}{\\rho}\\nabla p + \\nu \\nabla^2 \\mathbf{u} \\] \\[ \\nabla \\cdot \\mathbf{u} = 0 \\] <p>Applications: Ocean current modeling, atmospheric dynamics</p>"},{"location":"research/pinn-theory/#network-architectures","title":"Network Architectures","text":""},{"location":"research/pinn-theory/#multi-layer-perceptron-mlp","title":"Multi-Layer Perceptron (MLP)","text":"<p>Standard architecture: $$ u_\\theta(x) = W_L \\sigma(W_{L-1} \\ldots \\sigma(W_1 x + b_1) \\ldots + b_{L-1}) + b_L $$</p>"},{"location":"research/pinn-theory/#fourier-feature-networks","title":"Fourier Feature Networks","text":"<p>Address spectral bias by encoding inputs:</p> \\[ \\gamma(x) = [\\cos(2\\pi B x), \\sin(2\\pi B x)]^T \\] <p>where \\(B \\sim \\mathcal{N}(0, \\sigma^2)\\).</p> <p>Reference: Tancik, M., et al. (2020). Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. NeurIPS. arXiv:2006.10739</p>"},{"location":"research/pinn-theory/#activation-functions","title":"Activation Functions","text":"Function Formula Properties Tanh \\(\\tanh(x)\\) Smooth, bounded Sin \\(\\sin(x)\\) Periodic, bias reduction Swish \\(x \\cdot \\sigma(x)\\) Smooth, non-saturating"},{"location":"research/pinn-theory/#collocation-strategies","title":"Collocation Strategies","text":""},{"location":"research/pinn-theory/#uniform-random-sampling","title":"Uniform Random Sampling","text":"<p>Simple but may miss important regions: $$ x_j \\sim \\mathcal{U}(\\Omega) $$</p>"},{"location":"research/pinn-theory/#latin-hypercube-sampling-lhs","title":"Latin Hypercube Sampling (LHS)","text":"<p>Better space coverage:</p> <p>Reference: McKay, M.D., Beckman, R.J., &amp; Conover, W.J. (1979). A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code. Technometrics, 21(2), 239-245. DOI: 10.2307/1268522</p>"},{"location":"research/pinn-theory/#adaptive-residual-based-sampling","title":"Adaptive Residual-Based Sampling","text":"<p>Sample more where residual is high: $$ p(x) \\propto |\\mathcal{N}u_\\theta| $$</p> <p>Reference: Lu, L., et al. (2021). DeepXDE: A deep learning library for solving differential equations. SIAM Review, 63(1), 208-228. DOI: 10.1137/19M1274067</p>"},{"location":"research/pinn-theory/#training-considerations","title":"Training Considerations","text":""},{"location":"research/pinn-theory/#loss-weighting","title":"Loss Weighting","text":"Phase \\(\\lambda_d\\) \\(\\lambda_p\\) \\(\\lambda_b\\) Early 1.0 0.1 1.0 Middle 1.0 1.0 1.0 Late 1.0 10.0 1.0"},{"location":"research/pinn-theory/#learning-rate-schedule","title":"Learning Rate Schedule","text":"<p>Cosine annealing with warm restarts often works well:</p> <p>Reference: Loshchilov, I., &amp; Hutter, F. (2017). SGDR: Stochastic Gradient Descent with Warm Restarts. ICLR. arXiv:1608.03983</p>"},{"location":"research/pinn-theory/#earth-observation-applications","title":"Earth Observation Applications","text":"Application PDE Input Dim Observable SST Interpolation Advection-Diffusion 3 (x, y, t) Temperature Pollution Dispersion Transport 4 (x, y, z, t) Concentration Soil Moisture Richards 3 (x, z, t) Moisture Groundwater Flow Darcy 3 (x, y, t) Head"},{"location":"research/pinn-theory/#see-also","title":"See Also","text":"<ul> <li>Math Foundations</li> <li>GAN Theory</li> <li>PINN API Reference</li> </ul>"},{"location":"research/remote-sensing-task-taxonomy/","title":"Remote Sensing Task Taxonomy","text":"<p>A comprehensive classification of machine learning tasks for Earth observation applications.</p>"},{"location":"research/remote-sensing-task-taxonomy/#task-hierarchy","title":"Task Hierarchy","text":"<pre><code>graph TB\n    EO[Earth Observation ML Tasks]\n\n    EO --&gt; PIX[Pixel-Level Tasks]\n    EO --&gt; SCENE[Scene-Level Tasks]\n    EO --&gt; TEMP[Temporal Tasks]\n    EO --&gt; GEN[Generative Tasks]\n\n    PIX --&gt; SEG[Semantic Segmentation]\n    PIX --&gt; INST[Instance Segmentation]\n    PIX --&gt; REG[Regression]\n\n    SCENE --&gt; CLS[Classification]\n    SCENE --&gt; DET[Object Detection]\n\n    TEMP --&gt; CD[Change Detection]\n    TEMP --&gt; TS[Time Series]\n\n    GEN --&gt; SR[Super-Resolution]\n    GEN --&gt; TRANS[Domain Translation]\n    GEN --&gt; INP[Inpainting]\n</code></pre>"},{"location":"research/remote-sensing-task-taxonomy/#pixel-level-tasks","title":"Pixel-Level Tasks","text":""},{"location":"research/remote-sensing-task-taxonomy/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Assign a class label to every pixel in the image.</p> <p>Formulation: $$ f: \\mathbb{R}^{H \\times W \\times C} \\rightarrow {1, \\ldots, K}^{H \\times W} $$</p> <p>Applications: | Application | Classes | Resolution | |-------------|---------|------------| | Land Cover Mapping | 5-20 | 10-30 m | | Crop Type Classification | 10-50 | 10-30 m | | Urban Mapping | 5-15 | 0.5-5 m |</p> <p>Benchmark Datasets: - DeepGlobe: 803 images, 7 classes, 50cm resolution   - Demir, I., et al. (2018). DeepGlobe 2018: A Challenge to Parse the Earth Through Satellite Images. CVPR Workshops. Link</p> <ul> <li>LoveDA: 5,987 images, 7 classes, 30cm resolution</li> <li>Wang, J., et al. (2021). LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. NeurIPS Datasets Track. arXiv:2110.08733</li> </ul> <p>Reference Architecture: U-Net - Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI. DOI: 10.1007/978-3-319-24574-4_28</p>"},{"location":"research/remote-sensing-task-taxonomy/#instance-segmentation","title":"Instance Segmentation","text":"<p>Detect and segment individual objects.</p> <p>Applications: - Building footprint extraction - Individual tree detection - Vehicle counting</p> <p>Reference Architecture: Mask R-CNN - He, K., et al. (2017). Mask R-CNN. ICCV. DOI: 10.1109/ICCV.2017.322</p>"},{"location":"research/remote-sensing-task-taxonomy/#scene-level-tasks","title":"Scene-Level Tasks","text":""},{"location":"research/remote-sensing-task-taxonomy/#scene-classification","title":"Scene Classification","text":"<p>Assign a single label to entire image patches.</p> <p>Benchmark Datasets: - EuroSAT: 27,000 patches, 10 classes, Sentinel-2   - Helber, P., et al. (2019). EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. IEEE JSTARS, 12(7). DOI: 10.1109/JSTARS.2019.2918242</p> <ul> <li>UC Merced Land Use: 2,100 patches, 21 classes, 30cm</li> <li>Yang, Y., &amp; Newsam, S. (2010). Bag-of-Visual-Words and Spatial Extensions for Land-Use Classification. ACM SIGSPATIAL. DOI: 10.1145/1869790.1869829</li> </ul>"},{"location":"research/remote-sensing-task-taxonomy/#object-detection","title":"Object Detection","text":"<p>Localize objects with bounding boxes.</p> <p>Applications: | Application | Object Size | Resolution Needed | |-------------|-------------|-------------------| | Ship Detection | 20-300 m | 3-10 m | | Aircraft Detection | 15-80 m | 0.3-1 m | | Vehicle Detection | 3-15 m | 0.3-0.5 m |</p> <p>Reference Architecture: RetinaNet - Lin, T.Y., et al. (2017). Focal Loss for Dense Object Detection. ICCV. DOI: 10.1109/ICCV.2017.324</p>"},{"location":"research/remote-sensing-task-taxonomy/#temporal-tasks","title":"Temporal Tasks","text":""},{"location":"research/remote-sensing-task-taxonomy/#change-detection","title":"Change Detection","text":"<p>Identify changes between two or more temporal observations.</p> <p>Formulation: $$ f: \\mathbb{R}^{H \\times W \\times C} \\times \\mathbb{R}^{H \\times W \\times C} \\rightarrow {0, 1}^{H \\times W} $$</p> <p>Types: | Type | Output | Example | |------|--------|---------| | Binary | Change/No-change | Deforestation detection | | Multi-class | Change type | LULC transitions | | Regression | Change magnitude | Biomass change |</p> <p>Benchmark Datasets: - LEVIR-CD: 637 pairs, 0.5m resolution, building changes   - Chen, H., &amp; Shi, Z. (2020). A Spatial-Temporal Attention-Based Method for Remote Sensing Image Change Detection. Remote Sensing, 12(10). DOI: 10.3390/rs12101662</p> <p>Reference Architecture: Siamese Networks - Daudt, R.C., Le Saux, B., &amp; Boulch, A. (2018). Fully Convolutional Siamese Networks for Change Detection. ICIP. DOI: 10.1109/ICIP.2018.8451652</p>"},{"location":"research/remote-sensing-task-taxonomy/#time-series-analysis","title":"Time Series Analysis","text":"<p>Process sequences of observations over time.</p> <p>Applications: - Crop phenology monitoring - Seasonal pattern analysis - Anomaly detection</p> <p>Reference: LSTM/Transformer architectures - Ru\u00dfwurm, M., &amp; K\u00f6rner, M. (2018). Temporal Vegetation Modelling Using Long Short-Term Memory Networks for Crop Identification from Medium-Resolution Multi-Spectral Satellite Images. CVPR Workshops. DOI: 10.1109/CVPRW.2017.193</p>"},{"location":"research/remote-sensing-task-taxonomy/#generative-tasks","title":"Generative Tasks","text":""},{"location":"research/remote-sensing-task-taxonomy/#super-resolution","title":"Super-Resolution","text":"<p>Enhance spatial resolution beyond sensor limits.</p> <p>Formulation: $$ f: \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\mathbb{R}^{sH \\times sW \\times C} $$</p> <p>where \\(s\\) is the scale factor (2x, 4x, 8x).</p> <p>Reference Architectures: - SRCNN: Dong, C., et al. (2014). Learning a Deep Convolutional Network for Image Super-Resolution. ECCV. DOI: 10.1007/978-3-319-10593-2_13</p> <ul> <li>ESRGAN: Wang, X., et al. (2018). ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks. ECCV Workshops. arXiv:1809.00219</li> </ul>"},{"location":"research/remote-sensing-task-taxonomy/#domain-translation","title":"Domain Translation","text":"<p>Transform images between different domains.</p> <p>Applications: | Task | Source | Target | |------|--------|--------| | SAR-to-Optical | SAR | RGB | | Cloud Removal | Cloudy | Cloud-free | | Colorization | Panchromatic | Multispectral |</p> <p>Reference Architectures: - Pix2Pix: Isola, P., et al. (2017). Image-to-Image Translation with Conditional Adversarial Networks. CVPR. DOI: 10.1109/CVPR.2017.632</p> <ul> <li>CycleGAN: Zhu, J.Y., et al. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. ICCV. DOI: 10.1109/ICCV.2017.244</li> </ul>"},{"location":"research/remote-sensing-task-taxonomy/#see-also","title":"See Also","text":"<ul> <li>Math Foundations</li> <li>GAN Theory</li> <li>Bibliography</li> </ul>"},{"location":"tutorials/00_quickstart/","title":"Tutorial 00: Quickstart","text":"<p>Get started with Ununennium in 15 minutes. This tutorial covers installation, loading data, creating models, and making predictions.</p>"},{"location":"tutorials/00_quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>CUDA-capable GPU (recommended)</li> </ul>"},{"location":"tutorials/00_quickstart/#step-1-installation","title":"Step 1: Installation","text":"<pre><code># Full installation with all dependencies\npip install \"ununennium[all]\"\n\n# Verify installation\npython -c \"import ununennium; print(ununennium.__version__)\"\n</code></pre>"},{"location":"tutorials/00_quickstart/#step-2-load-satellite-imagery","title":"Step 2: Load Satellite Imagery","text":"<p>Ununennium's <code>GeoTensor</code> preserves coordinate reference system (CRS) information through all operations.</p> <pre><code>import ununennium as uu\n\n# Load a GeoTIFF file\ntensor = uu.io.read_geotiff(\"path/to/sentinel2.tif\")\n\n# Inspect metadata\nprint(f\"Shape: {tensor.shape}\")         # (12, 10980, 10980)\nprint(f\"CRS: {tensor.crs}\")             # EPSG:32632\nprint(f\"Resolution: {tensor.resolution}\")  # (10.0, 10.0) meters\nprint(f\"Bounds: {tensor.bounds}\")\n</code></pre>"},{"location":"tutorials/00_quickstart/#working-with-partial-reads","title":"Working with Partial Reads","text":"<p>For large files, read only the region you need:</p> <pre><code>from ununennium.core import BoundingBox\n\n# Define area of interest\naoi = BoundingBox(\n    left=500000,\n    bottom=4490000,\n    right=510000,\n    top=4500000,\n)\n\n# Read only the AOI\ntile = uu.io.read_geotiff(\"large_file.tif\", window=aoi)\n</code></pre>"},{"location":"tutorials/00_quickstart/#step-3-create-a-model","title":"Step 3: Create a Model","text":"<p>Use the model registry to create architectures:</p> <pre><code>from ununennium.models import create_model, list_models\n\n# List available models\nprint(list_models(task=\"segmentation\"))\n# ['unet_resnet50', 'unet_efficientnet_b4', 'deeplabv3_resnet101', ...]\n\n# Create U-Net with ResNet-50 backbone\nmodel = create_model(\n    \"unet_resnet50\",\n    in_channels=12,      # Sentinel-2 bands\n    num_classes=10,      # Land cover classes\n)\n\n# Move to GPU\nmodel = model.cuda()\n</code></pre>"},{"location":"tutorials/00_quickstart/#step-4-make-predictions","title":"Step 4: Make Predictions","text":"<p>Run inference on your data:</p> <pre><code>import torch\n\n# Prepare input\nx = tensor.data.unsqueeze(0).cuda()  # Add batch dimension, move to GPU\n\n# Inference\nmodel.eval()\nwith torch.no_grad():\n    logits = model(x)\n    prediction = logits.argmax(dim=1)\n\nprint(f\"Prediction shape: {prediction.shape}\")  # (1, 10980, 10980)\n</code></pre>"},{"location":"tutorials/00_quickstart/#step-5-compute-spectral-indices","title":"Step 5: Compute Spectral Indices","text":"<p>Calculate common vegetation indices:</p> <pre><code>from ununennium.preprocessing import compute_index\n\n# Sentinel-2 band mapping (0-indexed)\nbands = {\n    \"blue\": 1,\n    \"green\": 2,\n    \"red\": 3,\n    \"nir\": 7,\n    \"swir\": 11,\n}\n\n# Compute NDVI\nndvi = compute_index(tensor, \"ndvi\", bands)\nprint(f\"NDVI range: [{ndvi.min():.2f}, {ndvi.max():.2f}]\")\n</code></pre>"},{"location":"tutorials/00_quickstart/#step-6-train-a-model","title":"Step 6: Train a Model","text":"<p>Complete training loop with callbacks:</p> <pre><code>from ununennium.training import Trainer, CheckpointCallback\nfrom ununennium.losses import DiceLoss\nfrom ununennium.metrics import IoU\nimport torch\n\n# Setup optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    loss_fn=DiceLoss(),\n    train_loader=train_loader,\n    val_loader=val_loader,\n    callbacks=[\n        CheckpointCallback(\"checkpoints/\", monitor=\"val_iou\"),\n    ],\n    metrics=[IoU(num_classes=10)],\n    mixed_precision=True,\n)\n\n# Train\nhistory = trainer.fit(epochs=50)\n\n# Access training history\nprint(f\"Final val IoU: {history['val_iou'][-1]:.4f}\")\n</code></pre>"},{"location":"tutorials/00_quickstart/#step-7-export-for-production","title":"Step 7: Export for Production","text":"<p>Export to ONNX for deployment:</p> <pre><code>from ununennium.export import to_onnx\n\nto_onnx(\n    model,\n    example_input=torch.randn(1, 12, 512, 512).cuda(),\n    output_path=\"model.onnx\",\n    opset_version=17,\n)\n</code></pre>"},{"location":"tutorials/00_quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial 01: Ingest and Tiling - Data preparation</li> <li>Tutorial 02: Train/Val/Test - Proper experiment design</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"tutorials/01_ingest_and_tiling/","title":"Tutorial 01: Ingest and Tiling","text":"<p>This tutorial covers data ingestion from various sources and tiling strategies for large rasters.</p>"},{"location":"tutorials/01_ingest_and_tiling/#data-sources","title":"Data Sources","text":""},{"location":"tutorials/01_ingest_and_tiling/#local-geotiffs","title":"Local GeoTIFFs","text":"<pre><code>from ununennium.io import read_geotiff\n\n# Single file\ntensor = read_geotiff(\"sentinel2_l2a.tif\")\n\n# Specific bands (1-indexed)\nrgb = read_geotiff(\"sentinel2_l2a.tif\", bands=[4, 3, 2])\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#cloud-optimized-geotiffs-cogs","title":"Cloud-Optimized GeoTIFFs (COGs)","text":"<p>COGs support efficient partial reads:</p> <pre><code>from ununennium.io import read_geotiff\nfrom ununennium.core import BoundingBox\n\n# Read from HTTP\ntensor = read_geotiff(\"https://example.com/image.tif\")\n\n# Read specific window\naoi = BoundingBox(500000, 4490000, 510000, 4500000)\ntile = read_geotiff(\"https://example.com/image.tif\", window=aoi)\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#stac-catalogs","title":"STAC Catalogs","text":"<p>Query spatiotemporal asset catalogs:</p> <pre><code>from ununennium.io import STACClient\n\nclient = STACClient(\"https://earth-search.aws.element84.com/v1\")\n\n# Search for Sentinel-2 scenes\nitems = client.search(\n    bbox=(9.0, 45.0, 10.0, 46.0),\n    datetime=\"2023-06-01/2023-06-30\",\n    collections=[\"sentinel-2-l2a\"],\n    query={\"eo:cloud_cover\": {\"lt\": 20}},\n)\n\n# Load first item\nfor item in items:\n    tensor = client.read_item(item, assets=[\"B04\", \"B03\", \"B02\", \"B08\"])\n    print(f\"Loaded: {tensor.shape}, CRS: {tensor.crs}\")\n    break\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#tiling-strategies","title":"Tiling Strategies","text":""},{"location":"tutorials/01_ingest_and_tiling/#why-tile","title":"Why Tile?","text":"<p>Large satellite images (e.g., 10980 x 10980 Sentinel-2 scenes) exceed GPU memory. Tiling creates manageable patches.</p> <pre><code>graph TB\n    subgraph \"Source Image\"\n        SRC[10980 x 10980 pixels]\n    end\n\n    subgraph \"Tiles\"\n        T1[512 x 512]\n        T2[512 x 512]\n        T3[512 x 512]\n        TN[...]\n    end\n\n    SRC --&gt; T1\n    SRC --&gt; T2\n    SRC --&gt; T3\n    SRC --&gt; TN\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#basic-tiling","title":"Basic Tiling","text":"<pre><code>from ununennium.tiling import Tiler\n\ntiler = Tiler(\n    tile_size=(512, 512),\n    overlap=0.25,  # 25% overlap\n)\n\n# Generate tiles\ntiles = list(tiler.tile(tensor))\nprint(f\"Generated {len(tiles)} tiles\")\n\nfor tile in tiles[:3]:\n    print(f\"  Shape: {tile.shape}, Bounds: {tile.bounds}\")\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#tile-parameters","title":"Tile Parameters","text":"Parameter Description Typical Values <code>tile_size</code> Output tile dimensions 256, 512, 1024 <code>overlap</code> Fraction of overlap 0.0 - 0.5 <code>drop_nodata</code> Skip tiles with only nodata True/False <code>min_coverage</code> Minimum valid pixel fraction 0.5 - 1.0"},{"location":"tutorials/01_ingest_and_tiling/#overlap-rationale","title":"Overlap Rationale","text":"<p>Overlap prevents edge artifacts during inference:</p> \\[ \\text{stride} = \\text{tile\\_size} \\times (1 - \\text{overlap}) \\] <p>For 512px tiles with 25% overlap: stride = 384px</p>"},{"location":"tutorials/01_ingest_and_tiling/#sampling-strategies","title":"Sampling Strategies","text":""},{"location":"tutorials/01_ingest_and_tiling/#random-sampling","title":"Random Sampling","text":"<p>For training, often sample randomly rather than exhaustively:</p> <pre><code>from ununennium.tiling import RandomSampler\n\nsampler = RandomSampler(\n    tile_size=(256, 256),\n    n_samples=1000,\n    seed=42,\n)\n\nsamples = list(sampler.sample(tensor))\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#importance-sampling","title":"Importance Sampling","text":"<p>Sample more from regions of interest:</p> <pre><code>from ununennium.tiling import ImportanceSampler\n\n# Weight map (e.g., class imbalance correction)\nweights = compute_weights(tensor)\n\nsampler = ImportanceSampler(\n    tile_size=(256, 256),\n    n_samples=1000,\n    weights=weights,\n)\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#spatial-cross-validation-sampling","title":"Spatial Cross-Validation Sampling","text":"<p>Avoid spatial autocorrelation leakage:</p> <pre><code>from ununennium.tiling import BlockSampler\n\nsampler = BlockSampler(\n    tile_size=(256, 256),\n    block_size=(2560, 2560),  # 10x10 tiles per block\n    n_folds=5,\n)\n\n# Get train/val split for fold 0\ntrain_tiles, val_tiles = sampler.get_fold(tensor, fold=0)\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#building-a-dataset","title":"Building a Dataset","text":""},{"location":"tutorials/01_ingest_and_tiling/#geodataset","title":"GeoDataset","text":"<pre><code>from ununennium.datasets import GeoDataset\nfrom ununennium.preprocessing import normalize, compute_index\n\nclass MyDataset(GeoDataset):\n    def __init__(self, image_paths, mask_paths, tile_size=256):\n        super().__init__()\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.tile_size = tile_size\n\n    def __len__(self):\n        return len(self.image_paths) * 100  # tiles per image\n\n    def __getitem__(self, idx):\n        # Load random tile\n        image = self.load_tile(idx)\n        mask = self.load_mask(idx)\n\n        # Preprocess\n        image = normalize(image, method=\"percentile\")\n\n        return {\"image\": image, \"mask\": mask}\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#dataloader","title":"DataLoader","text":"<pre><code>from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n)\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#performance-tips","title":"Performance Tips","text":"Tip Impact Use COG format 10-100x faster partial reads Pre-tile to disk Fastest training I/O Pin memory Faster GPU transfer Increase num_workers Better CPU/GPU overlap"},{"location":"tutorials/01_ingest_and_tiling/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial 02: Train/Val/Test - Spatial cross-validation</li> <li>Datasets Guide - Advanced dataset patterns</li> </ul>"},{"location":"tutorials/02_train_val_test/","title":"Tutorial 02: Train / Val / Test Splitting","text":"<p>This tutorial covers proper experimental design for geospatial machine learning, with emphasis on avoiding spatial autocorrelation leakage.</p>"},{"location":"tutorials/02_train_val_test/#the-spatial-autocorrelation-problem","title":"The Spatial Autocorrelation Problem","text":""},{"location":"tutorials/02_train_val_test/#what-is-spatial-autocorrelation","title":"What Is Spatial Autocorrelation?","text":"<p>Nearby pixels are not independent. A tree in pixel (100, 100) predicts trees in pixel (101, 101).</p> \\[ \\text{Moran's } I = \\frac{n}{\\sum_i \\sum_j w_{ij}} \\cdot \\frac{\\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i (x_i - \\bar{x})^2} \\]"},{"location":"tutorials/02_train_val_test/#the-leakage-problem","title":"The Leakage Problem","text":"<p>Random pixel-wise splits cause leakage:</p> <pre><code>graph LR\n    subgraph \"Random Split (Wrong)\"\n        P1[Train Pixel] --- P2[Val Pixel] --- P3[Train Pixel]\n    end\n\n    subgraph \"Block Split (Correct)\"\n        B1[Train Block] --&gt; GAP[Buffer Zone] --&gt; B2[Val Block]\n    end\n</code></pre> <p>Symptoms of leakage: - Validation metrics much better than test metrics - Model fails on new geographic regions - Overly optimistic performance estimates</p>"},{"location":"tutorials/02_train_val_test/#block-based-splitting","title":"Block-Based Splitting","text":""},{"location":"tutorials/02_train_val_test/#principle","title":"Principle","text":"<p>Split by contiguous spatial blocks, not individual pixels:</p> <pre><code>from ununennium.datasets import BlockSplitter\n\nsplitter = BlockSplitter(\n    block_size=(10, 10),  # 10x10 km blocks\n    buffer_size=(3, 3),   # 3 km buffer between train/val\n)\n\ntrain_blocks, val_blocks, test_blocks = splitter.split(\n    bounds=image.bounds,\n    train_ratio=0.7,\n    val_ratio=0.15,\n    test_ratio=0.15,\n    seed=42,\n)\n</code></pre>"},{"location":"tutorials/02_train_val_test/#block-size-selection","title":"Block Size Selection","text":"<p>Block size should exceed the spatial autocorrelation range:</p> Data Type Typical Correlation Range Suggested Block Size Urban 100-500 m 1 km Agriculture 500m - 2 km 5 km Forest 1-10 km 15 km Climate 10-100 km 150 km"},{"location":"tutorials/02_train_val_test/#spatial-cross-validation","title":"Spatial Cross-Validation","text":""},{"location":"tutorials/02_train_val_test/#k-fold-spatial-cv","title":"K-Fold Spatial CV","text":"<pre><code>from ununennium.datasets import SpatialKFold\n\ncv = SpatialKFold(\n    n_folds=5,\n    strategy=\"block\",  # or \"geographic\"\n    buffer=1000,       # 1 km buffer\n)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(bounds)):\n    print(f\"Fold {fold}: {len(train_idx)} train, {len(val_idx)} val\")\n\n    # Train and evaluate for this fold\n    train_loader = create_loader(data, train_idx)\n    val_loader = create_loader(data, val_idx)\n\n    model = train(train_loader)\n    metrics = evaluate(model, val_loader)\n\n    fold_results.append(metrics)\n\n# Aggregate results\nmean_iou = np.mean([r[\"iou\"] for r in fold_results])\nstd_iou = np.std([r[\"iou\"] for r in fold_results])\nprint(f\"mIoU: {mean_iou:.4f} +/- {std_iou:.4f}\")\n</code></pre>"},{"location":"tutorials/02_train_val_test/#leave-one-region-out","title":"Leave-One-Region-Out","text":"<p>For geographic generalization:</p> <pre><code>from ununennium.datasets import LeaveOneRegionOut\n\n# Define geographic regions\nregions = {\n    \"north\": BoundingBox(...),\n    \"south\": BoundingBox(...),\n    \"east\": BoundingBox(...),\n    \"west\": BoundingBox(...),\n}\n\ncv = LeaveOneRegionOut(regions)\n\nfor test_region, (train_data, test_data) in cv.split(data):\n    print(f\"Testing on {test_region}\")\n    # Train on all other regions, test on this one\n</code></pre>"},{"location":"tutorials/02_train_val_test/#trainvaltest-workflow","title":"Train/Val/Test Workflow","text":""},{"location":"tutorials/02_train_val_test/#complete-pipeline","title":"Complete Pipeline","text":"<pre><code>from ununennium.datasets import BlockSplitter, GeoDataset\nfrom ununennium.training import Trainer\nimport torch\n\n# 1. Create spatial splits\nsplitter = BlockSplitter(block_size=(5000, 5000))\ntrain_bounds, val_bounds, test_bounds = splitter.split(\n    bounds=full_bounds,\n    train_ratio=0.7,\n    val_ratio=0.15,\n    test_ratio=0.15,\n)\n\n# 2. Create datasets\ntrain_dataset = GeoDataset(image_paths, mask_paths, bounds=train_bounds)\nval_dataset = GeoDataset(image_paths, mask_paths, bounds=val_bounds)\ntest_dataset = GeoDataset(image_paths, mask_paths, bounds=test_bounds)\n\n# 3. Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# 4. Train with validation\ntrainer = Trainer(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    # ...\n)\nhistory = trainer.fit(epochs=100)\n\n# 5. Final evaluation on held-out test set\ntest_metrics = trainer.evaluate(test_loader)\nprint(f\"Test mIoU: {test_metrics['iou']:.4f}\")\n</code></pre>"},{"location":"tutorials/02_train_val_test/#visualizing-splits","title":"Visualizing Splits","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n# Plot blocks colored by split\nfor block in train_blocks:\n    ax.add_patch(plt.Rectangle(\n        (block.left, block.bottom),\n        block.width, block.height,\n        facecolor=\"blue\", alpha=0.3, label=\"Train\"\n    ))\n\nfor block in val_blocks:\n    ax.add_patch(plt.Rectangle(\n        (block.left, block.bottom),\n        block.width, block.height,\n        facecolor=\"green\", alpha=0.3, label=\"Val\"\n    ))\n\nfor block in test_blocks:\n    ax.add_patch(plt.Rectangle(\n        (block.left, block.bottom),\n        block.width, block.height,\n        facecolor=\"red\", alpha=0.3, label=\"Test\"\n    ))\n\nax.set_xlabel(\"Easting (m)\")\nax.set_ylabel(\"Northing (m)\")\nax.set_title(\"Spatial Train/Val/Test Split\")\nplt.savefig(\"split_visualization.png\")\n</code></pre>"},{"location":"tutorials/02_train_val_test/#common-pitfalls","title":"Common Pitfalls","text":"Pitfall Problem Solution Random pixel split Data leakage Use block splitting No buffer zone Correlation across boundary Add buffer Same scene in train+val Temporal leakage Split by acquisition date Stratify by class only Geographic bias Stratify spatially"},{"location":"tutorials/02_train_val_test/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial 03: Inference at Scale - Production deployment</li> <li>Datasets and Splits Guide - Deep dive</li> </ul>"},{"location":"tutorials/03_inference_at_scale/","title":"Tutorial 03: Inference at Scale","text":"<p>This tutorial covers production deployment patterns for large-scale geospatial inference.</p>"},{"location":"tutorials/03_inference_at_scale/#sliding-window-inference","title":"Sliding Window Inference","text":""},{"location":"tutorials/03_inference_at_scale/#basic-pattern","title":"Basic Pattern","text":"<p>For images larger than GPU memory:</p> <pre><code>from ununennium.tiling import SlidingWindowInference\n\ninferencer = SlidingWindowInference(\n    model=model,\n    tile_size=(512, 512),\n    overlap=0.5,           # 50% overlap\n    blend_mode=\"cosine\",   # Smooth blending\n    batch_size=8,\n)\n\n# Run inference on full raster\noutput = inferencer.predict(\"input_raster.tif\")\n\n# Save with preserved geospatial metadata\noutput.save(\"predictions.tif\")\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#overlap-blending","title":"Overlap Blending","text":"<p>Blending eliminates seam artifacts:</p> <pre><code>graph TB\n    subgraph \"Overlap Region\"\n        T1[Tile 1 Prediction]\n        T2[Tile 2 Prediction]\n        W1[Weight 1]\n        W2[Weight 2]\n        BLEND[Blended Output]\n    end\n\n    T1 --&gt; W1\n    T2 --&gt; W2\n    W1 --&gt; BLEND\n    W2 --&gt; BLEND\n</code></pre> <p>Cosine weight function:</p> \\[ w(d) = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{\\pi \\cdot d}{m}\\right)\\right) \\] <p>where \\(d\\) is distance from edge and \\(m\\) is margin width.</p>"},{"location":"tutorials/03_inference_at_scale/#batch-processing","title":"Batch Processing","text":""},{"location":"tutorials/03_inference_at_scale/#multiple-files","title":"Multiple Files","text":"<pre><code>from pathlib import Path\nfrom ununennium.tiling import SlidingWindowInference\n\ninput_dir = Path(\"input_scenes/\")\noutput_dir = Path(\"predictions/\")\noutput_dir.mkdir(exist_ok=True)\n\ninferencer = SlidingWindowInference(model=model, tile_size=(512, 512))\n\nfor input_path in input_dir.glob(\"*.tif\"):\n    output_path = output_dir / f\"{input_path.stem}_pred.tif\"\n\n    output = inferencer.predict(str(input_path))\n    output.save(str(output_path))\n\n    print(f\"Processed: {input_path.name}\")\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from concurrent.futures import ProcessPoolExecutor\n\ndef process_file(input_path, model_path, output_dir):\n    # Load model in worker process\n    model = torch.load(model_path)\n    model.eval()\n\n    inferencer = SlidingWindowInference(model=model, tile_size=(512, 512))\n\n    output_path = output_dir / f\"{input_path.stem}_pred.tif\"\n    output = inferencer.predict(str(input_path))\n    output.save(str(output_path))\n\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    executor.map(\n        process_file,\n        input_files,\n        [model_path] * len(input_files),\n        [output_dir] * len(input_files),\n    )\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#onnx-deployment","title":"ONNX Deployment","text":""},{"location":"tutorials/03_inference_at_scale/#export-model","title":"Export Model","text":"<pre><code>from ununennium.export import to_onnx\n\nto_onnx(\n    model,\n    example_input=torch.randn(1, 12, 512, 512).cuda(),\n    output_path=\"model.onnx\",\n    opset_version=17,\n    dynamic_axes={\n        \"input\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n        \"output\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n    },\n)\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#inference-with-onnx-runtime","title":"Inference with ONNX Runtime","text":"<pre><code>import onnxruntime as ort\nimport numpy as np\n\nsession = ort.InferenceSession(\n    \"model.onnx\",\n    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n)\n\ndef predict_onnx(image: np.ndarray) -&gt; np.ndarray:\n    outputs = session.run(\n        None,\n        {\"input\": image.astype(np.float32)},\n    )\n    return outputs[0]\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#torchscript-deployment","title":"TorchScript Deployment","text":"<pre><code>from ununennium.export import to_torchscript\n\nscripted = to_torchscript(\n    model,\n    example_input=torch.randn(1, 12, 512, 512).cuda(),\n    optimize=True,\n)\nscripted.save(\"model.pt\")\n\n# Load and use\nmodel = torch.jit.load(\"model.pt\")\noutput = model(input_tensor)\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#performance-optimization","title":"Performance Optimization","text":""},{"location":"tutorials/03_inference_at_scale/#batch-size-tuning","title":"Batch Size Tuning","text":"GPU Memory Tile Size Suggested Batch 8 GB 512x512 2-4 16 GB 512x512 8-12 24 GB 512x512 16-24 80 GB 512x512 64-96"},{"location":"tutorials/03_inference_at_scale/#half-precision","title":"Half Precision","text":"<pre><code>model = model.half()  # FP16\n\noutput = inferencer.predict(\n    \"input.tif\",\n    dtype=torch.float16,\n)\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#cloud-deployment","title":"Cloud Deployment","text":""},{"location":"tutorials/03_inference_at_scale/#container-image","title":"Container Image","text":"<pre><code>FROM pytorch/pytorch:2.1.0-cuda12.1-runtime\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY model.pt /app/model.pt\nCOPY inference.py /app/\n\nWORKDIR /app\nCMD [\"python\", \"inference.py\"]\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#kubernetes-job","title":"Kubernetes Job","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: inference-job\nspec:\n  parallelism: 4\n  template:\n    spec:\n      containers:\n      - name: inference\n        image: myregistry/inference:latest\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        env:\n        - name: INPUT_PATH\n          value: \"s3://bucket/input/\"\n        - name: OUTPUT_PATH\n          value: \"s3://bucket/output/\"\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial 04: Change Detection</li> <li>Performance Guide</li> </ul>"},{"location":"tutorials/04_change_detection/","title":"Tutorial 04: Change Detection","text":"<p>This tutorial covers building change detection models for multi-temporal Earth observation data.</p>"},{"location":"tutorials/04_change_detection/#change-detection-overview","title":"Change Detection Overview","text":"<p>Change detection identifies differences between images acquired at different times.</p> <pre><code>graph LR\n    T1[Time 1 Image] --&gt; MODEL[Change Detection Model]\n    T2[Time 2 Image] --&gt; MODEL\n    MODEL --&gt; CHANGE[Change Map]\n</code></pre>"},{"location":"tutorials/04_change_detection/#applications","title":"Applications","text":"Application Change Type Typical Resolution Deforestation Forest loss 10-30 m Urban expansion Building construction 0.5-5 m Flood mapping Water extent 10-30 m Crop monitoring Phenology changes 10-30 m"},{"location":"tutorials/04_change_detection/#data-preparation","title":"Data Preparation","text":""},{"location":"tutorials/04_change_detection/#bitemporal-dataset","title":"Bitemporal Dataset","text":"<pre><code>from ununennium.datasets import ChangeDetectionDataset\n\ndataset = ChangeDetectionDataset(\n    pre_paths=[\"2022_01/*.tif\"],      # Before images\n    post_paths=[\"2023_01/*.tif\"],     # After images\n    mask_paths=[\"change_masks/*.tif\"], # Change labels\n    tile_size=256,\n    augment=True,\n)\n\n# Returns dict with 'pre', 'post', 'mask' keys\nsample = dataset[0]\nprint(f\"Pre shape: {sample['pre'].shape}\")\nprint(f\"Post shape: {sample['post'].shape}\")\nprint(f\"Mask shape: {sample['mask'].shape}\")\n</code></pre>"},{"location":"tutorials/04_change_detection/#coregistration","title":"Coregistration","text":"<p>Ensure temporal images are aligned:</p> <pre><code>from ununennium.preprocessing import coregister\n\n# Align post-image to pre-image\naligned_post = coregister(\n    reference=pre_image,\n    target=post_image,\n    method=\"phase_correlation\",\n)\n</code></pre>"},{"location":"tutorials/04_change_detection/#model-architectures","title":"Model Architectures","text":""},{"location":"tutorials/04_change_detection/#siamese-networks","title":"Siamese Networks","text":"<p>Shared-weight encoder for both temporal images:</p> <pre><code>from ununennium.models import create_model\n\nmodel = create_model(\n    \"siamese_unet_resnet50\",\n    in_channels=12,\n    num_classes=2,  # Change / No-change\n    fusion=\"diff\",  # or \"concat\", \"dist\"\n)\n</code></pre> <pre><code>graph TB\n    subgraph \"Siamese Architecture\"\n        PRE[Pre Image] --&gt; ENC1[Encoder]\n        POST[Post Image] --&gt; ENC2[Encoder]\n        ENC1 --&gt; FUSION[Fusion]\n        ENC2 --&gt; FUSION\n        FUSION --&gt; DEC[Decoder]\n        DEC --&gt; OUT[Change Map]\n    end\n\n    ENC1 -.- ENC2\n</code></pre>"},{"location":"tutorials/04_change_detection/#fusion-strategies","title":"Fusion Strategies","text":"Strategy Formula Description Difference \\(f_{post} - f_{pre}\\) Simple subtraction Concatenation \\([f_{pre}, f_{post}]\\) Stack features Distance \\(\\|f_{post} - f_{pre}\\|\\) L2 distance"},{"location":"tutorials/04_change_detection/#early-vs-late-fusion","title":"Early vs Late Fusion","text":"<pre><code># Early fusion: Concatenate inputs\nmodel = create_model(\n    \"unet_resnet50\",\n    in_channels=24,  # 12 + 12 bands\n    num_classes=2,\n)\n\n# Late fusion: Concatenate features\nmodel = create_model(\n    \"siamese_unet_resnet50\",\n    in_channels=12,\n    fusion=\"late\",\n)\n</code></pre>"},{"location":"tutorials/04_change_detection/#training","title":"Training","text":""},{"location":"tutorials/04_change_detection/#loss-functions","title":"Loss Functions","text":"<pre><code>from ununennium.losses import DiceLoss, FocalLoss\n\n# Handle class imbalance (usually more no-change than change)\nloss_fn = FocalLoss(gamma=2.0, alpha=0.75)  # Weight change class\n</code></pre>"},{"location":"tutorials/04_change_detection/#training-loop","title":"Training Loop","text":"<pre><code>from ununennium.training import Trainer\nfrom ununennium.metrics import IoU, F1Score\n\ntrainer = Trainer(\n    model=model,\n    optimizer=torch.optim.AdamW(model.parameters(), lr=1e-4),\n    loss_fn=FocalLoss(gamma=2.0),\n    train_loader=train_loader,\n    val_loader=val_loader,\n    metrics=[\n        IoU(num_classes=2),\n        F1Score(num_classes=2),\n    ],\n)\n\nhistory = trainer.fit(epochs=50)\n</code></pre>"},{"location":"tutorials/04_change_detection/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"tutorials/04_change_detection/#change-detection-metrics","title":"Change Detection Metrics","text":"\\[ \\text{Precision} = \\frac{TP}{TP + FP} \\quad \\text{Recall} = \\frac{TP}{TP + FN} \\] \\[ \\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\] <p>Kappa coefficient for agreement:</p> \\[ \\kappa = \\frac{p_o - p_e}{1 - p_e} \\] <pre><code>from ununennium.metrics import ChangeDetectionMetrics\n\nmetrics = ChangeDetectionMetrics()\nmetrics.update(predictions, targets)\n\nresults = metrics.compute()\nprint(f\"F1: {results['f1']:.4f}\")\nprint(f\"Kappa: {results['kappa']:.4f}\")\nprint(f\"OA: {results['overall_accuracy']:.4f}\")\n</code></pre>"},{"location":"tutorials/04_change_detection/#multi-class-change-detection","title":"Multi-class Change Detection","text":"<p>Detect what changed, not just if it changed:</p> <pre><code># Classes: 0=no-change, 1=forest-loss, 2=urban-growth, 3=water-change\nmodel = create_model(\n    \"siamese_unet_resnet50\",\n    in_channels=12,\n    num_classes=4,  # Multi-class\n)\n</code></pre>"},{"location":"tutorials/04_change_detection/#inference-example","title":"Inference Example","text":"<pre><code># Load bitemporal images\npre_image = uu.io.read_geotiff(\"2022_01_tile.tif\")\npost_image = uu.io.read_geotiff(\"2023_01_tile.tif\")\n\n# Prepare input\nx_pre = pre_image.data.unsqueeze(0).cuda()\nx_post = post_image.data.unsqueeze(0).cuda()\n\n# Predict\nmodel.eval()\nwith torch.no_grad():\n    change_map = model(x_pre, x_post).argmax(dim=1)\n\n# Visualize\nplt.imshow(change_map[0].cpu(), cmap=\"RdYlGn_r\")\n</code></pre>"},{"location":"tutorials/04_change_detection/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial 05: Super-Resolution</li> <li>Change Detection Theory</li> </ul>"},{"location":"tutorials/05_super_resolution/","title":"Tutorial 05: Super-Resolution","text":"<p>This tutorial covers super-resolution techniques to enhance the spatial resolution of satellite imagery.</p>"},{"location":"tutorials/05_super_resolution/#super-resolution-overview","title":"Super-Resolution Overview","text":"<p>Super-resolution reconstructs high-resolution images from low-resolution inputs.</p> \\[ I_{HR} = f_\\theta(I_{LR}) \\]"},{"location":"tutorials/05_super_resolution/#applications","title":"Applications","text":"Application Scale Benefit Sentinel-2 enhancement 2-4x 10m to 2.5-5m Pan-sharpening 4x Fuse multispectral with panchromatic Historical imagery 4-8x Improve archive data"},{"location":"tutorials/05_super_resolution/#single-image-super-resolution","title":"Single-Image Super-Resolution","text":""},{"location":"tutorials/05_super_resolution/#esrgan-model","title":"ESRGAN Model","text":"<pre><code>from ununennium.models.gan import ESRGAN\n\nmodel = ESRGAN(\n    in_channels=12,     # Multispectral bands\n    scale_factor=4,     # 4x upscaling\n    n_rrdb=23,          # RRDB blocks\n)\n</code></pre>"},{"location":"tutorials/05_super_resolution/#training","title":"Training","text":"<pre><code>from ununennium.losses import PerceptualLoss, AdversarialLoss\nfrom ununennium.training import GANTrainer\n\n# Loss components\nperceptual_loss = PerceptualLoss(layers=[\"conv3_4\", \"conv4_4\"])\nadversarial_loss = AdversarialLoss(mode=\"hinge\")\nl1_loss = nn.L1Loss()\n\ndef generator_loss(fake, real, d_fake):\n    return (\n        1.0 * l1_loss(fake, real) +\n        0.1 * perceptual_loss(fake, real) +\n        0.01 * adversarial_loss.generator_loss(d_fake)\n    )\n\ntrainer = GANTrainer(\n    generator=model.generator,\n    discriminator=model.discriminator,\n    g_loss_fn=generator_loss,\n    d_loss_fn=adversarial_loss.discriminator_loss,\n)\n\nhistory = trainer.fit(train_loader, epochs=100)\n</code></pre>"},{"location":"tutorials/05_super_resolution/#data-preparation","title":"Data Preparation","text":""},{"location":"tutorials/05_super_resolution/#creating-lr-hr-pairs","title":"Creating LR-HR Pairs","text":"<pre><code>from ununennium.preprocessing import downsample\n\ndef create_pair(hr_image, scale=4):\n    \"\"\"Create low-resolution input from high-resolution target.\"\"\"\n    lr_image = downsample(\n        hr_image,\n        scale=scale,\n        method=\"bicubic\",\n        antialias=True,\n    )\n    return lr_image, hr_image\n</code></pre>"},{"location":"tutorials/05_super_resolution/#dataset","title":"Dataset","text":"<pre><code>from ununennium.datasets import SuperResolutionDataset\n\ndataset = SuperResolutionDataset(\n    hr_paths=[\"high_res/*.tif\"],\n    scale_factor=4,\n    patch_size=256,  # HR patch size\n    augment=True,\n)\n\nsample = dataset[0]\nprint(f\"LR shape: {sample['lr'].shape}\")   # (12, 64, 64)\nprint(f\"HR shape: {sample['hr'].shape}\")   # (12, 256, 256)\n</code></pre>"},{"location":"tutorials/05_super_resolution/#metrics","title":"Metrics","text":""},{"location":"tutorials/05_super_resolution/#psnr-peak-signal-to-noise-ratio","title":"PSNR (Peak Signal-to-Noise Ratio)","text":"\\[ \\text{PSNR} = 10 \\cdot \\log_{10}\\left(\\frac{MAX^2}{MSE}\\right) \\]"},{"location":"tutorials/05_super_resolution/#ssim-structural-similarity","title":"SSIM (Structural Similarity)","text":"\\[ \\text{SSIM}(x, y) = \\frac{(2\\mu_x\\mu_y + c_1)(2\\sigma_{xy} + c_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)} \\] <pre><code>from ununennium.metrics import PSNR, SSIM\n\npsnr = PSNR()\nssim = SSIM()\n\npsnr.update(sr_images, hr_images)\nssim.update(sr_images, hr_images)\n\nprint(f\"PSNR: {psnr.compute():.2f} dB\")\nprint(f\"SSIM: {ssim.compute():.4f}\")\n</code></pre>"},{"location":"tutorials/05_super_resolution/#real-world-super-resolution","title":"Real-World Super-Resolution","text":""},{"location":"tutorials/05_super_resolution/#degradation-modeling","title":"Degradation Modeling","text":"<p>Real images have complex degradations beyond bicubic downsampling:</p> <pre><code>from ununennium.preprocessing import RealDegradation\n\ndegradation = RealDegradation(\n    blur_kernels=[\"gaussian\", \"motion\", \"defocus\"],\n    noise_level=(0.01, 0.05),\n    jpeg_quality=(30, 95),\n    scale=4,\n)\n\nlr_image = degradation(hr_image)\n</code></pre>"},{"location":"tutorials/05_super_resolution/#real-esrgan-training","title":"Real-ESRGAN Training","text":"<pre><code>from ununennium.models.gan import RealESRGAN\n\nmodel = RealESRGAN(\n    in_channels=3,\n    scale_factor=4,\n    degradation=degradation,\n)\n</code></pre>"},{"location":"tutorials/05_super_resolution/#pan-sharpening","title":"Pan-Sharpening","text":"<p>Fuse low-resolution multispectral with high-resolution panchromatic:</p> <pre><code>from ununennium.models import create_model\n\n# Pan-sharpening model\nmodel = create_model(\n    \"pansharpening_resnet\",\n    ms_channels=4,   # Multispectral bands\n    pan_channels=1,  # Panchromatic band\n    out_channels=4,  # High-res multispectral\n    scale_factor=4,\n)\n\n# Input: (B, 4, H, W) MS + (B, 1, 4H, 4W) PAN\n# Output: (B, 4, 4H, 4W) High-res MS\n</code></pre>"},{"location":"tutorials/05_super_resolution/#inference","title":"Inference","text":""},{"location":"tutorials/05_super_resolution/#single-image","title":"Single Image","text":"<pre><code># Load low-resolution image\nlr_image = uu.io.read_geotiff(\"sentinel2_10m.tif\")\n\n# Super-resolve\nmodel.eval()\nwith torch.no_grad():\n    sr_image = model(lr_image.data.unsqueeze(0).cuda())\n\n# Save with updated resolution\nsr_geotensor = GeoTensor(\n    data=sr_image[0].cpu(),\n    crs=lr_image.crs,\n    transform=lr_image.transform * Affine.scale(0.25),  # 4x finer\n)\nsr_geotensor.save(\"sentinel2_2.5m.tif\")\n</code></pre>"},{"location":"tutorials/05_super_resolution/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial 06: GAN Recipes</li> <li>GAN API Reference</li> </ul>"},{"location":"tutorials/06_gan_recipes/","title":"Tutorial 06: GAN Recipes","text":"<p>This tutorial provides practical recipes for training GANs for Earth observation applications.</p>"},{"location":"tutorials/06_gan_recipes/#recipe-1-sar-to-optical-translation-pix2pix","title":"Recipe 1: SAR-to-Optical Translation (Pix2Pix)","text":"<p>Translate SAR imagery to optical RGB.</p>"},{"location":"tutorials/06_gan_recipes/#setup","title":"Setup","text":"<pre><code>from ununennium.models.gan import Pix2Pix\nfrom ununennium.training import GANTrainer\n\nmodel = Pix2Pix(\n    in_channels=2,       # VV, VH\n    out_channels=3,      # RGB\n    ngf=64,\n    ndf=64,\n    lambda_l1=100.0,\n)\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#dataset","title":"Dataset","text":"<pre><code>from ununennium.datasets import PairedImageDataset\n\ndataset = PairedImageDataset(\n    source_paths=[\"sar/*.tif\"],\n    target_paths=[\"optical/*.tif\"],\n    tile_size=256,\n    augment=True,\n)\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#training","title":"Training","text":"<pre><code>trainer = GANTrainer(\n    generator=model.generator,\n    discriminator=model.discriminator,\n    g_optimizer=torch.optim.Adam(model.generator.parameters(), lr=2e-4, betas=(0.5, 0.999)),\n    d_optimizer=torch.optim.Adam(model.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999)),\n    n_critic=1,\n)\n\nhistory = trainer.fit(train_loader, epochs=200)\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#recipe-2-cloud-removal-cyclegan","title":"Recipe 2: Cloud Removal (CycleGAN)","text":"<p>Remove clouds using unpaired clean/cloudy images.</p>"},{"location":"tutorials/06_gan_recipes/#setup_1","title":"Setup","text":"<pre><code>from ununennium.models.gan import CycleGAN\n\nmodel = CycleGAN(\n    in_channels_a=3,         # Cloudy RGB\n    in_channels_b=3,         # Clean RGB\n    ngf=64,\n    n_residual=9,\n    lambda_cycle=10.0,\n    lambda_identity=0.5,\n)\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#dataset_1","title":"Dataset","text":"<pre><code>from ununennium.datasets import UnpairedImageDataset\n\n# No need for paired data\ndataset_cloudy = UnpairedImageDataset(paths=[\"cloudy/*.tif\"], tile_size=256)\ndataset_clean = UnpairedImageDataset(paths=[\"clean/*.tif\"], tile_size=256)\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#training-loop","title":"Training Loop","text":"<pre><code>for epoch in range(200):\n    for cloudy, clean in zip(loader_cloudy, loader_clean):\n        # Generator update\n        g_loss = model.generator_step(cloudy, clean)\n\n        # Discriminator update\n        d_loss = model.discriminator_step(cloudy, clean)\n\n    print(f\"Epoch {epoch}: G={g_loss:.4f}, D={d_loss:.4f}\")\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#recipe-3-super-resolution-esrgan","title":"Recipe 3: Super-Resolution (ESRGAN)","text":"<p>4x resolution enhancement.</p>"},{"location":"tutorials/06_gan_recipes/#setup_2","title":"Setup","text":"<pre><code>from ununennium.models.gan import ESRGAN\n\nmodel = ESRGAN(\n    in_channels=12,\n    scale_factor=4,\n    n_rrdb=23,\n)\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#loss-weights","title":"Loss Weights","text":"Loss Weight Purpose L1 Pixel 1.0 Reconstruction Perceptual 0.1 Texture quality Adversarial 0.01 Sharpness <pre><code>from ununennium.losses import PerceptualLoss, AdversarialLoss\n\nperceptual = PerceptualLoss(layers=[\"conv4_4\"])\nadversarial = AdversarialLoss(mode=\"hinge\")\n\ndef g_loss(fake, real, d_fake):\n    return (\n        1.0 * F.l1_loss(fake, real) +\n        0.1 * perceptual(fake, real) +\n        0.01 * adversarial.g_loss(d_fake)\n    )\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#recipe-4-semantic-guided-translation","title":"Recipe 4: Semantic-Guided Translation","text":"<p>Use segmentation masks to guide translation.</p> <pre><code>from ununennium.models.gan import SemanticPix2Pix\n\nmodel = SemanticPix2Pix(\n    in_channels=3,\n    out_channels=3,\n    num_classes=10,      # Semantic classes\n    use_instance=True,   # Instance normalization\n)\n\n# Input: image + one-hot semantic map\nfake = model.generator(image, semantic_map)\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#training-tips","title":"Training Tips","text":""},{"location":"tutorials/06_gan_recipes/#learning-rate","title":"Learning Rate","text":"Phase G LR D LR Warmup (epochs 1-10) 1e-4 1e-4 Main (epochs 10-150) 2e-4 2e-4 Decay (epochs 150+) Linear to 0 Linear to 0"},{"location":"tutorials/06_gan_recipes/#discriminator-regularization","title":"Discriminator Regularization","text":"<pre><code># Gradient penalty for WGAN-GP\ndef gradient_penalty(discriminator, real, fake):\n    alpha = torch.rand(real.size(0), 1, 1, 1, device=real.device)\n    interpolated = alpha * real + (1 - alpha) * fake\n    interpolated.requires_grad_(True)\n\n    d_out = discriminator(interpolated)\n\n    gradients = torch.autograd.grad(\n        outputs=d_out,\n        inputs=interpolated,\n        grad_outputs=torch.ones_like(d_out),\n        create_graph=True,\n    )[0]\n\n    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gp\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#mode-collapse-prevention","title":"Mode Collapse Prevention","text":"<ul> <li>Use spectral normalization in discriminator</li> <li>Label smoothing (real=0.9, fake=0.1)</li> <li>Feature matching loss</li> <li>Progressive growing</li> </ul>"},{"location":"tutorials/06_gan_recipes/#evaluation","title":"Evaluation","text":""},{"location":"tutorials/06_gan_recipes/#fid-frechet-inception-distance","title":"FID (Frechet Inception Distance)","text":"<pre><code>from ununennium.metrics import FID\n\nfid = FID()\nfid.update(real_images, fake_images)\nprint(f\"FID: {fid.compute():.2f}\")  # Lower is better\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#visual-quality-assessment","title":"Visual Quality Assessment","text":"<pre><code># Generate comparison grid\nfrom ununennium.visualization import make_comparison_grid\n\ngrid = make_comparison_grid(\n    input_images,\n    generated_images,\n    target_images,\n    labels=[\"Input\", \"Generated\", \"Target\"],\n)\ngrid.save(\"comparison.png\")\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial 07: PINN Recipes</li> <li>GAN Theory</li> <li>GAN API</li> </ul>"},{"location":"tutorials/07_pinn_recipes/","title":"Tutorial 07: PINN Recipes","text":"<p>This tutorial provides practical recipes for Physics-Informed Neural Networks in Earth observation.</p>"},{"location":"tutorials/07_pinn_recipes/#recipe-1-heat-diffusion-basic-pinn","title":"Recipe 1: Heat Diffusion (Basic PINN)","text":"<p>Model thermal diffusion on land surface.</p>"},{"location":"tutorials/07_pinn_recipes/#problem-setup","title":"Problem Setup","text":"<p>Heat equation:</p> \\[ \\frac{\\partial u}{\\partial t} = D \\nabla^2 u \\]"},{"location":"tutorials/07_pinn_recipes/#implementation","title":"Implementation","text":"<pre><code>from ununennium.models.pinn import PINN, DiffusionEquation, MLP, UniformSampler\n\n# Define PDE\nequation = DiffusionEquation(diffusivity=0.1)\n\n# Create network\nnetwork = MLP(\n    layers=[3, 128, 128, 128, 1],  # (x, y, t) -&gt; temperature\n    activation=\"tanh\",\n)\n\n# Create PINN\npinn = PINN(\n    network=network,\n    equation=equation,\n    lambda_pde=10.0,\n)\n\n# Sample collocation points\nsampler = UniformSampler(\n    bounds=[(0, 1), (0, 1), (0, 1)],  # x, y, t\n    n_points=10000,\n)\nx_collocation = sampler.sample()\n\n# Training\noptimizer = torch.optim.Adam(pinn.parameters(), lr=1e-3)\n\nfor epoch in range(10000):\n    optimizer.zero_grad()\n\n    losses = pinn.compute_loss(x_data, u_data, x_collocation)\n    losses[\"total\"].backward()\n\n    optimizer.step()\n\n    if epoch % 1000 == 0:\n        print(f\"Epoch {epoch}: data={losses['data']:.4e}, pde={losses['pde']:.4e}\")\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#recipe-2-advection-diffusion-sst-interpolation","title":"Recipe 2: Advection-Diffusion (SST Interpolation)","text":"<p>Interpolate Sea Surface Temperature using physics constraints.</p>"},{"location":"tutorials/07_pinn_recipes/#problem-setup_1","title":"Problem Setup","text":"\\[ \\frac{\\partial T}{\\partial t} + \\mathbf{v} \\cdot \\nabla T = D \\nabla^2 T \\]"},{"location":"tutorials/07_pinn_recipes/#implementation_1","title":"Implementation","text":"<pre><code>from ununennium.models.pinn import (\n    PINN, AdvectionDiffusionEquation, FourierMLP, LatinHypercubeSampler\n)\n\n# Physical parameters from oceanographic data\nequation = AdvectionDiffusionEquation(\n    diffusivity=100.0,       # m^2/s (horizontal eddy diffusivity)\n    velocity=(0.1, 0.05),    # m/s (mean current)\n)\n\n# Use Fourier features for high-frequency patterns\nnetwork = FourierMLP(\n    in_features=3,           # x, y, t\n    layers=[256, 256, 256, 1],\n    sigma=10.0,\n    n_frequencies=256,\n)\n\npinn = PINN(\n    network=network,\n    equation=equation,\n    lambda_pde=1.0,\n)\n\n# Better space coverage with LHS\nsampler = LatinHypercubeSampler(\n    bounds=[(0, 100000), (0, 100000), (0, 86400)],  # 100km x 100km x 1 day\n    n_points=5000,\n)\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#training-with-sparse-sst-data","title":"Training with Sparse SST Data","text":"<pre><code># Load sparse SST observations\nsst_data = load_sst_observations(\"sst_buoys.csv\")\nx_data = sst_data[[\"x\", \"y\", \"t\"]].values\nu_data = sst_data[\"temperature\"].values\n\n# Collocation points for physics constraint\nx_collocation = sampler.sample()\n\n# Train PINN\nfor epoch in range(20000):\n    losses = pinn.compute_loss(x_data, u_data, x_collocation)\n    # ...\n\n# Predict on dense grid\nx_grid = create_grid(100, 100, 24)  # 100x100 spatial, 24 time steps\nsst_prediction = pinn.network(x_grid)\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#recipe-3-boundary-constrained-pinn","title":"Recipe 3: Boundary-Constrained PINN","text":"<p>Enforce boundary conditions for pollution dispersion.</p>"},{"location":"tutorials/07_pinn_recipes/#setup","title":"Setup","text":"<pre><code>from ununennium.models.pinn import PINN, AdvectionDiffusionEquation, DirichletBC\n\nequation = AdvectionDiffusionEquation(\n    diffusivity=10.0,\n    velocity=(1.0, 0.0),  # Wind direction\n)\n\n# Boundary conditions\nbc_inlet = DirichletBC(value=1.0)   # Pollution source\nbc_walls = DirichletBC(value=0.0)   # No flux\n\npinn = PINN(\n    network=network,\n    equation=equation,\n    lambda_pde=1.0,\n    lambda_bc=10.0,\n)\n\n# Sample boundary points\nx_bc_inlet = sample_boundary(side=\"left\", n=500)\nu_bc_inlet = torch.ones(500, 1)\n\nx_bc_walls = sample_boundary(sides=[\"top\", \"bottom\"], n=1000)\nu_bc_walls = torch.zeros(1000, 1)\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#recipe-4-multi-output-pinn","title":"Recipe 4: Multi-Output PINN","text":"<p>Simultaneously predict temperature and concentration.</p> <pre><code>class CoupledNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.shared = MLP([3, 128, 128], activation=\"tanh\")\n        self.temp_head = nn.Linear(128, 1)\n        self.conc_head = nn.Linear(128, 1)\n\n    def forward(self, x):\n        features = self.shared(x)\n        temperature = self.temp_head(features)\n        concentration = self.conc_head(features)\n        return torch.cat([temperature, concentration], dim=-1)\n\n# Coupled PDEs\nclass CoupledEquation(PDEEquation):\n    def residual(self, x, u):\n        T, C = u[:, 0:1], u[:, 1:2]\n\n        # Temperature equation\n        T_t = grad(T, x, dim=2)\n        T_xx = laplacian(T, x, dims=[0, 1])\n        res_T = T_t - 0.1 * T_xx\n\n        # Concentration equation (depends on temperature)\n        C_t = grad(C, x, dim=2)\n        C_xx = laplacian(C, x, dims=[0, 1])\n        reaction = k(T) * C  # Temperature-dependent reaction\n        res_C = C_t - 0.05 * C_xx + reaction\n\n        return torch.cat([res_T, res_C], dim=-1)\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#training-tips","title":"Training Tips","text":""},{"location":"tutorials/07_pinn_recipes/#learning-rate-schedule","title":"Learning Rate Schedule","text":"<pre><code># Warmup + cosine decay\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=1e-3,\n    total_steps=20000,\n    pct_start=0.1,\n)\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#loss-weighting","title":"Loss Weighting","text":"Phase Data PDE BC Early (0-5k) 1.0 0.1 1.0 Middle (5k-15k) 1.0 1.0 1.0 Late (15k+) 1.0 10.0 1.0"},{"location":"tutorials/07_pinn_recipes/#adaptive-collocation","title":"Adaptive Collocation","text":"<pre><code># Resample where residual is high\nresiduals = pinn.compute_residual(x_collocation)\nhigh_residual_idx = residuals.abs() &gt; threshold\nx_collocation_new = importance_sample(high_residual_idx)\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#visualization","title":"Visualization","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Predict on grid\nx_grid = create_meshgrid(100, 100)\nu_pred = pinn.network(x_grid).reshape(100, 100)\n\n# Plot\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].scatter(x_data[:, 0], x_data[:, 1], c=u_data, s=10)\naxes[0].set_title(\"Observations\")\n\naxes[1].contourf(x_grid[..., 0], x_grid[..., 1], u_pred.detach())\naxes[1].set_title(\"PINN Prediction\")\n\nresidual = pinn.compute_residual(x_grid)\naxes[2].contourf(x_grid[..., 0], x_grid[..., 1], residual.abs())\naxes[2].set_title(\"PDE Residual\")\n\nplt.savefig(\"pinn_results.png\")\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#next-steps","title":"Next Steps","text":"<ul> <li>PINN Theory</li> <li>PINN API</li> </ul>"}]}