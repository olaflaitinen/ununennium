{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Ununennium Documentation","text":"<p>Welcome to the Ununennium documentation. This comprehensive guide covers all aspects of the library, from installation to advanced model deployment.</p>"},{"location":"#documentation-map","title":"Documentation Map","text":"<pre><code>graph TB\n    subgraph \"Getting Started\"\n        QS[Quickstart Tutorial]\n        INST[Installation]\n    end\n\n    subgraph \"Core Concepts\"\n        ARCH[Architecture]\n        DM[Data Model]\n        PIPE[Pipelines]\n    end\n\n    subgraph \"API Reference\"\n        CORE[Core API]\n        IO[Data I/O]\n        MODELS[Models]\n        TRAIN[Training]\n    end\n\n    subgraph \"Tutorials\"\n        T1[Ingest &amp; Tiling]\n        T2[Train/Val/Test]\n        T3[Inference at Scale]\n        T4[Change Detection]\n        T5[Super-Resolution]\n        T6[GAN Recipes]\n        T7[PINN Recipes]\n    end\n\n    subgraph \"Advanced Topics\"\n        GUIDES[Guides]\n        RESEARCH[Research]\n    end\n\n    QS --&gt; ARCH\n    ARCH --&gt; API\n    API --&gt; Tutorials\n    Tutorials --&gt; GUIDES\n</code></pre>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"Section Description Audience Architecture System design and data flow All users API Reference Complete API documentation Developers Tutorials Step-by-step learning paths New users Guides Best practices and deep dives Intermediate Research Mathematical foundations Researchers"},{"location":"#architecture","title":"Architecture","text":"<p>Understand the design and components of Ununennium.</p> <ul> <li>System Overview - High-level architecture, component interactions</li> <li>Data Model - GeoTensor, GeoBatch, CRS handling</li> <li>Pipelines - Data flow from disk to GPU</li> <li>Performance - Benchmarks and optimization</li> <li>Security and Privacy - Data handling best practices</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>Detailed documentation for every public class and function.</p> <ul> <li>Overview - API design principles</li> <li>Core - GeoTensor, GeoBatch, types</li> <li>Data I/O - COG, STAC, Zarr readers</li> <li>Preprocessing - Indices, normalization</li> <li>Training - Trainer, callbacks</li> <li>Models - Model registry, architectures</li> <li>Evaluation - Metrics, validation</li> <li>GAN - Generative adversarial networks</li> <li>PINN - Physics-informed networks</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Step-by-step guides from beginner to advanced.</p> Tutorial Duration Description 00. Quickstart 15 min Zero to trained model 01. Ingest and Tiling 30 min Data preparation 02. Train/Val/Test 45 min Proper experimental design 03. Inference at Scale 30 min Production deployment 04. Change Detection 45 min Multi-temporal analysis 05. Super-Resolution 45 min Resolution enhancement 06. GAN Recipes 60 min Image translation 07. PINN Recipes 60 min Physics-constrained learning"},{"location":"#guides","title":"Guides","text":"<p>Best practices and detailed topic coverage.</p> <ul> <li>Datasets and Splits - Spatial cross-validation</li> <li>Reproducibility - Deterministic experiments</li> <li>Configuration - Config file reference</li> <li>Benchmarking - Performance testing</li> <li>Metrics - Metric selection guide</li> <li>Uncertainty and Calibration - Confidence estimation</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"#research","title":"Research","text":"<p>Mathematical and theoretical foundations.</p> <ul> <li>Math Foundations - Core mathematical concepts</li> <li>Remote Sensing Task Taxonomy - Task classification</li> <li>GAN Theory - Adversarial learning dynamics</li> <li>PINN Theory - Physics-constrained optimization</li> <li>Bibliography - Curated references</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Troubleshooting Guide - Common issues and solutions</li> <li>GitHub Issues - Bug reports</li> <li>GitHub Discussions - Questions</li> </ul>"},{"location":"api/core/","title":"Core API","text":""},{"location":"api/core/#geotensor","title":"GeoTensor","text":"<p><code>ununennium.core.GeoTensor</code></p> <p>The core data structure extending <code>torch.Tensor</code>.</p>"},{"location":"api/core/#constructor","title":"Constructor","text":"<pre><code>GeoTensor(data, affine, crs)\n</code></pre> <p>Parameters: - <code>data</code> (torch.Tensor): The raw tensor data. - <code>affine</code> (rasterio.Affine): Affine transform matrix. - <code>crs</code> (CRS): Coordinate reference system.</p>"},{"location":"api/core/#properties","title":"Properties","text":"<ul> <li><code>width</code>, <code>height</code>: Spatial dimensions.</li> <li><code>bounds</code>: Bounding box <code>(minx, miny, maxx, maxy)</code>.</li> <li><code>res</code>: Resolution <code>(x_res, y_res)</code>.</li> </ul>"},{"location":"api/core/#methods","title":"Methods","text":""},{"location":"api/core/#reproject","title":"<code>reproject</code>","text":"<pre><code>def reproject(dst_crs: CRS, dst_res: float = None) -&gt; GeoTensor\n</code></pre> <p>Reproject tensor to a new CRS.</p>"},{"location":"api/core/#crop","title":"<code>crop</code>","text":"<pre><code>def crop(bbox: BBox) -&gt; GeoTensor\n</code></pre> <p>Crop the tensor to a bounding box.</p>"},{"location":"api/core/#geobatch","title":"GeoBatch","text":"<p>A container for a batch of <code>GeoTensor</code> objects, handling collation and scatter/gather operations.</p>"},{"location":"api/data-io/","title":"Data I/O API","text":""},{"location":"api/data-io/#reading-data","title":"Reading Data","text":""},{"location":"api/data-io/#read_window","title":"<code>read_window</code>","text":"<pre><code>ununennium.io.read_window(path: str, window: Window) -&gt; GeoTensor\n</code></pre> <p>Read a specific window from a raster file.</p>"},{"location":"api/data-io/#stream_from_cog","title":"<code>stream_from_cog</code>","text":"<pre><code>ununennium.io.stream_from_cog(url: str, chunk_size: int = 512) -&gt; Iterator[GeoTensor]\n</code></pre> <p>Stream tiles from a Cloud Optimized GeoTIFF.</p>"},{"location":"api/data-io/#stac-integration","title":"STAC Integration","text":"<p>Utilities for interacting with Spatio-Temporal Asset Catalogs.</p>"},{"location":"api/data-io/#stacreader","title":"<code>STACReader</code>","text":"<pre><code>class STACReader:\n    def search(bbox, datetime, collections) -&gt; ItemCollection\n    def load_item(item, bands) -&gt; GeoTensor\n</code></pre>"},{"location":"api/data-io/#writing-data","title":"Writing Data","text":""},{"location":"api/data-io/#write_geotiff","title":"<code>write_geotiff</code>","text":"<pre><code>ununennium.io.write_geotiff(\n    path: str, \n    tensor: GeoTensor, \n    profile: dict = None\n)\n</code></pre> <p>Write a GeoTensor to disk as a GeoTIFF.</p>"},{"location":"api/evaluation/","title":"Evaluation API","text":""},{"location":"api/evaluation/#metrics","title":"Metrics","text":""},{"location":"api/evaluation/#iou-intersection-over-union","title":"<code>IoU</code> (Intersection over Union)","text":"<pre><code>class IoU(Metric):\n    def update(self, pred, target)\n    def compute() -&gt; float\n</code></pre> <p>Formula:</p> <pre><code>\\text{IoU} = \\frac{\\text{Intersection}}{\\text{Union}}\n</code></pre>"},{"location":"api/evaluation/#f1score-dice-coefficient","title":"<code>F1Score</code> (Dice Coefficient)","text":"<p>Harmonic mean of precision and recall.</p> <pre><code>F1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n</code></pre>"},{"location":"api/evaluation/#confusionmatrix","title":"<code>ConfusionMatrix</code>","text":"<p>Computes the \\(C \\times C\\) confusion matrix for \\(C\\) classes.</p>"},{"location":"api/evaluation/#uncertainty","title":"Uncertainty","text":""},{"location":"api/evaluation/#expectedcalibrationerror","title":"<code>ExpectedCalibrationError</code>","text":"<p>Measures how well predicted probabilities match accuracy.</p> <pre><code>\\text{ECE} = \\sum_{m=1}^M \\frac{|B_m|}{N} |\\text{acc}(B_m) - \\text{conf}(B_m)|\n</code></pre>"},{"location":"api/gan/","title":"GAN API","text":""},{"location":"api/gan/#models","title":"Models","text":""},{"location":"api/gan/#pix2pix","title":"<code>Pix2Pix</code>","text":"<p>Paired image-to-image translation.</p> <pre><code>model = Pix2Pix(in_channels=3, out_channels=3)\n</code></pre>"},{"location":"api/gan/#cyclegan","title":"<code>CycleGAN</code>","text":"<p>Unpaired translation between domains A and B.</p> <pre><code>model = CycleGAN(in_channels_a=3, in_channels_b=3)\n</code></pre>"},{"location":"api/gan/#esrgan","title":"<code>ESRGAN</code>","text":"<p>Super-resolution model.</p> <pre><code>model = ESRGAN(scale_factor=4)\n</code></pre>"},{"location":"api/gan/#losses","title":"Losses","text":""},{"location":"api/gan/#adversarialloss","title":"<code>AdversarialLoss</code>","text":"<p>Generic GAN loss interface (BCE, MSE, Wasserstein).</p>"},{"location":"api/gan/#perceptualloss","title":"<code>PerceptualLoss</code>","text":"<p>VGG-based feature matching loss.</p> <pre><code>\\mathcal{L}_{p} = \\| \\phi(x) - \\phi(y) \\|_2^2\n</code></pre>"},{"location":"api/models/","title":"Models API","text":""},{"location":"api/models/#segmentation-models","title":"Segmentation Models","text":""},{"location":"api/models/#unet","title":"<code>UNet</code>","text":"<pre><code>UNet(in_channels=3, classes=2, encoder_name=\"resnet34\")\n</code></pre> <p>Standard U-Net with ResNet encoder.</p>"},{"location":"api/models/#deeplabv3plus","title":"<code>DeepLabV3Plus</code>","text":"<pre><code>DeepLabV3Plus(in_channels=3, classes=2, encoder_name=\"resnet50\")\n</code></pre> <p>For multi-scale context aggregation.</p>"},{"location":"api/models/#object-detection","title":"Object Detection","text":""},{"location":"api/models/#fasterrcnn","title":"<code>FasterRCNN</code>","text":"<pre><code>FasterRCNN(num_classes=10, backbone=\"resnet50_fpn\")\n</code></pre>"},{"location":"api/models/#registry","title":"Registry","text":"<p>Models can be instantiated via registry:</p> <pre><code>from ununennium.models import create_model\n\nmodel = create_model(\"unet\", encoder_name=\"efficientnet-b0\")\n</code></pre>"},{"location":"api/overview/","title":"API Overview","text":""},{"location":"api/overview/#module-structure","title":"Module Structure","text":"<pre><code>graph TD\n    U[ununennium]\n    U --&gt; Core[core]\n    U --&gt; IO[io]\n    U --&gt; Models[models]\n    U --&gt; Train[training]\n    U --&gt; Metric[metrics]\n\n    Models --&gt; GAN[gan]\n    Models --&gt; PINN[pinn]\n    Models --&gt; Arch[architectures]\n</code></pre>"},{"location":"api/overview/#quick-reference","title":"Quick Reference","text":"Component Import Description GeoTensor <code>from ununennium.core import GeoTensor</code> Geospatial tensor wrapper Trainer <code>from ununennium.training import Trainer</code> Training loop manager U-Net <code>from ununennium.models import UNet</code> Segmentation model IoU <code>from ununennium.metrics import IoU</code> Evaluation metric"},{"location":"api/overview/#type-definitions","title":"Type Definitions","text":"<p>Coordinate systems are defined using EPSG codes (integers) or WKT strings.</p> <pre><code>CRS = int | str\nBBox = tuple[float, float, float, float]\n</code></pre>"},{"location":"api/pinn/","title":"PINN API","text":""},{"location":"api/pinn/#physics-informed-neural-networks","title":"Physics-Informed Neural Networks","text":""},{"location":"api/pinn/#pinnmodule","title":"<code>PINNModule</code>","text":"<p>Base class for physics-informed models. Defines the <code>pde_residual</code> method.</p> <pre><code>class HeatEquationPINN(PINNModule):\n    def pde_residual(self, x, t, u):\n        u_t = grad(u, t)\n        u_xx = grad(grad(u, x), x)\n        return u_t - self.D * u_xx\n</code></pre>"},{"location":"api/pinn/#samplers","title":"Samplers","text":""},{"location":"api/pinn/#collocationsampler","title":"<code>CollocationSampler</code>","text":"<p>Samples points in the domain \\(\\Omega \\times [0, T]\\) for PDE loss computation.</p> <ul> <li><code>UniformSampler</code>: Uniform grid.</li> <li><code>LatinHypercubeSampler</code>: LHS sampling for better coverage.</li> </ul>"},{"location":"api/pinn/#boundary-conditions","title":"Boundary Conditions","text":""},{"location":"api/pinn/#dirichletbc","title":"<code>DirichletBC</code>","text":"<p>Fixed value boundary condition: \\(u(x) = g(x)\\).</p>"},{"location":"api/pinn/#neumannbc","title":"<code>NeumannBC</code>","text":"<p>Fixed derivative boundary condition: \\(\\frac{\\partial u}{\\partial n} = h(x)\\).</p>"},{"location":"api/preprocessing/","title":"Preprocessing API","text":""},{"location":"api/preprocessing/#spectral-indices","title":"Spectral Indices","text":""},{"location":"api/preprocessing/#compute_ndvi","title":"<code>compute_ndvi</code>","text":"<pre><code>def compute_ndvi(nir: torch.Tensor, red: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes Normalized Difference Vegetation Index.</p> <pre><code>\\text{NDVI} = \\frac{\\text{NIR} - \\text{Red}}{\\text{NIR} + \\text{Red}}\n</code></pre>"},{"location":"api/preprocessing/#compute_ndwi","title":"<code>compute_ndwi</code>","text":"<pre><code>def compute_ndwi(green: torch.Tensor, nir: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes Normalized Difference Water Index.</p> <pre><code>\\text{NDWI} = \\frac{\\text{Green} - \\text{NIR}}{\\text{Green} + \\text{NIR}}\n</code></pre>"},{"location":"api/preprocessing/#normalization","title":"Normalization","text":""},{"location":"api/preprocessing/#min_max_scale","title":"<code>min_max_scale</code>","text":"<pre><code>def min_max_scale(\n    tensor: torch.Tensor, \n    min_val: float, \n    max_val: float\n) -&gt; torch.Tensor\n</code></pre> <p>Scale values to [0, 1].</p>"},{"location":"api/preprocessing/#standardize","title":"<code>standardize</code>","text":"<pre><code>def standardize(\n    tensor: torch.Tensor, \n    mean: list[float], \n    std: list[float]\n) -&gt; torch.Tensor\n</code></pre> <p>Z-score normalization.</p> <pre><code>z = \\frac{x - \\mu}{\\sigma}\n</code></pre>"},{"location":"api/training/","title":"Training API","text":""},{"location":"api/training/#trainer","title":"Trainer","text":"<p><code>ununennium.training.Trainer</code></p> <p>Features: - Mixed precision training (AMP) - Distributed Data Parallel (DDP) - Gradle accumulation - Experiment logging</p>"},{"location":"api/training/#usage","title":"Usage","text":"<pre><code>trainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    callbacks=[EarlyStopping(), ModelCheckpoint()]\n)\ntrainer.fit(train_loader, val_loader, epochs=100)\n</code></pre>"},{"location":"api/training/#callbacks","title":"Callbacks","text":""},{"location":"api/training/#earlystopping","title":"<code>EarlyStopping</code>","text":"<p>Stop training when validation metric stops improving.</p>"},{"location":"api/training/#modelcheckpoint","title":"<code>ModelCheckpoint</code>","text":"<p>Save model weights periodically or on best metric.</p>"},{"location":"api/training/#learningratemonitor","title":"<code>LearningRateMonitor</code>","text":"<p>Log learning rate changes.</p>"},{"location":"architecture/data-model/","title":"Data Model","text":""},{"location":"architecture/data-model/#geotensor","title":"GeoTensor","text":"<p>The <code>GeoTensor</code> is the primary citizen of Ununennium. It wraps a PyTorch tensor and ensures that geospatial operations are mathematically consistent.</p>"},{"location":"architecture/data-model/#affine-representations","title":"Affine Representations","text":"<p>The affine transform matrix \\(M\\) defines the relationship between pixel space \\((u, v)\\) and projected map space \\((x, y)\\):</p> <pre><code>\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}\n</code></pre> <p>Where: - \\(a\\): pixel width - \\(e\\): pixel height (usually negative) - \\(c, f\\): top-left origin coordinates - \\(b, d\\): rotation terms (usually 0 for north-up images)</p>"},{"location":"architecture/data-model/#coordinate-propagation","title":"Coordinate Propagation","text":"<p>When performing operations like pooling or strided convolution, the affine transform is updated automatically.</p> <p>For a stride \\(s\\):</p> <pre><code>M_{new} = M_{old} \\times \\begin{bmatrix} s &amp; 0 &amp; 0 \\\\ 0 &amp; s &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\n</code></pre>"},{"location":"architecture/data-model/#geobatch","title":"GeoBatch","text":"<p>A <code>GeoBatch</code> is a collection of <code>GeoTensor</code>s. It supports stacking tensors even if they are from different non-overlapping regions, tracking their individual metadata for reconstruction.</p>"},{"location":"architecture/data-model/#bounding-boxes","title":"Bounding Boxes","text":"<p>Bounding boxes are represented as <code>(minx, miny, maxx, maxy)</code> tuples, automatically handled in CRS operations.</p>"},{"location":"architecture/overview/","title":"System Architecture","text":"<p>Ununennium is designed as a layered framework for high-performance geospatial machine learning.</p> <pre><code>graph TD\n    User[User API] --&gt; High[High-Level API]\n    User --&gt; Core[Core Primitives]\n\n    subgraph \"High-Level API\"\n        Models[Model Registry]\n        Trainer[Trainer / Callbacks]\n        Pipe[Pipelines]\n    end\n\n    subgraph \"Core Primitives\"\n        GT[GeoTensor]\n        GB[GeoBatch]\n        IO[Streaming I/O]\n    end\n\n    subgraph \"Backend\"\n        Torch[PyTorch]\n        GDAL[GDAL/Rasterio]\n        CUDA[CUDA Kernels]\n    end\n\n    High --&gt; Core\n    Core --&gt; Backend\n</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-geotensor","title":"1. GeoTensor","text":"<p>The fundamental data structure extending <code>torch.Tensor</code> with geospatial metadata: - Affine Transform: Maps pixel coordinates to map coordinates. - CRS: Coordinate Reference System (WKT/EPSG). - Bounding Box: Spatial extent management.</p> <pre><code>\\text{Pixel}(col, row) \\xrightarrow{M} \\text{Map}(x, y)\n</code></pre>"},{"location":"architecture/overview/#2-streaming-io","title":"2. Streaming I/O","text":"<p>Lazy loading architecture for files larger than memory (e.g., 100GB+ Sentinel-2 scenes). Uses windowed reading with caching.</p>"},{"location":"architecture/overview/#3-model-registry","title":"3. Model Registry","text":"<p>Standardized interface for 15+ models (CNN, ViT, GAN, PINN), supporting: - Automatic weight downloading - Config-driven instantiation - Standardized forward/predict signatures</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant D as Data Storage (S3/Local)\n    participant I as IO Layer\n    participant T as Tiler\n    participant M as Model\n\n    D-&gt;&gt;I: Stream Window\n    I-&gt;&gt;T: Raw Patch\n    T-&gt;&gt;T: Augment &amp; Normalize\n    T-&gt;&gt;M: Forward Pass\n    M-&gt;&gt;T: Prediction\n    T-&gt;&gt;I: Write Result\n</code></pre>"},{"location":"architecture/performance/","title":"Performance Optimization","text":""},{"location":"architecture/performance/#benchmarks","title":"Benchmarks","text":"<p>System performance on standard hardware (NVIDIA A100).</p> <pre><code>pie title Inference Time Breakdown\n    \"I/O &amp; Tiling\" : 30\n    \"GPU Compute\" : 60\n    \"Post-processing\" : 10\n</code></pre>"},{"location":"architecture/performance/#memory-management","title":"Memory Management","text":""},{"location":"architecture/performance/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>For large backbones (e.g., ViT-Large), we check-point activations to trade compute for memory.</p> <p>Memory reduction factor:</p> <pre><code>\\text{Mem} \\approx \\sqrt{L}\n</code></pre> <p>where \\(L\\) is the number of layers.</p>"},{"location":"architecture/performance/#mixed-precision","title":"Mixed Precision","text":"<p>We use FP16 (AMP) for training. Loss scaling is handled automatically to prevent underflow.</p> <pre><code>W_{new} = W_{old} - \\eta \\cdot \\frac{\\nabla \\mathcal{L}}{\\text{scale}}\n</code></pre>"},{"location":"architecture/performance/#throughput-optimization","title":"Throughput Optimization","text":"<ul> <li>Num Workers: Optimal formula is often \\(N_{cpu} / N_{gpu}\\).</li> <li>Pin Memory: Enabled by default for faster Host-to-Device transfer.</li> <li>Prefetch Factor: Tuned to 2-4 batches.</li> </ul>"},{"location":"architecture/pipelines/","title":"Processing Pipelines","text":""},{"location":"architecture/pipelines/#tiling-and-inference","title":"Tiling and Inference","text":"<p>Processing massive satellite imagery requires tiling strategies that handle edge effects.</p>"},{"location":"architecture/pipelines/#overlap-tile-strategy","title":"Overlap-Tile Strategy","text":"<p>To mitigate boundary artifacts in Convolutional Neural Networks, we use an overlap strategy.</p> <p>Let \\(N\\) be the tile size and \\(O\\) be the overlap. The effective stride is:</p> <pre><code>S = N - 2O\n</code></pre> <p>Only the center \\(S \\times S\\) region of the output prediction is kept.</p> <pre><code>graph LR\n    Input[Input Scene] --&gt; Tiler\n    Tiler --&gt; Batch[Batched Tiles]\n    Batch --&gt; Model\n    Model --&gt; Pred[Predictions]\n    Pred --&gt; Stitcher\n    Stitcher --&gt; Output[Output Map]\n</code></pre>"},{"location":"architecture/pipelines/#importance-sampling","title":"Importance Sampling","text":"<p>During training, random sampling is inefficient for sparse classes. We use importance sampling based on class distribution.</p> <p>Probability of sampling patch \\(i\\) with class label \\(c\\):</p> <pre><code>P(i) \\propto \\frac{1}{\\text{freq}(c)}\n</code></pre>"},{"location":"architecture/pipelines/#streaming-architecture","title":"Streaming Architecture","text":"<p>The pipeline uses Python generators and multi-processing to stream data without loading full files into RAM.</p> <ol> <li>Reader: Opens file handle (GDAL/Rasterio)</li> <li>Window Engine: Yields <code>Window</code> objects</li> <li>Collate: Assembles batches</li> <li>Prefetch: GPU asynchronous transfer</li> </ol>"},{"location":"architecture/security-privacy/","title":"Security and Privacy","text":""},{"location":"architecture/security-privacy/#data-security","title":"Data Security","text":"<ul> <li>Encryption: Support for reading analytical data from encrypted S3 buckets.</li> <li>Sanitization: Metadata scrubbing for exported assets.</li> </ul>"},{"location":"architecture/security-privacy/#geospatial-privacy","title":"Geospatial Privacy","text":"<p>High-resolution imagery can reveal sensitive information.</p>"},{"location":"architecture/security-privacy/#geometric-obfuscation","title":"Geometric Obfuscation","text":"<p>Random jittering of coordinates for anonymized public releases.</p> <pre><code>x' = x + \\epsilon_x, \\quad \\epsilon_x \\sim \\mathcal{N}(0, \\sigma^2)\n</code></pre>"},{"location":"architecture/security-privacy/#resolution-downgrading","title":"Resolution Downgrading","text":"<p>Automatic resampling to lower resolution for privacy-sensitive areas.</p>"},{"location":"architecture/security-privacy/#model-security","title":"Model Security","text":""},{"location":"architecture/security-privacy/#adversarial-defense","title":"Adversarial Defense","text":"<p>Models can be robustified against adversarial attacks using adversarial training.</p> <pre><code>\\min_\\theta \\mathbb{E}_{(x,y)} \\left[ \\max_{\\delta \\in S} \\mathcal{L}(f_\\theta(x+\\delta), y) \\right]\n</code></pre> <p>where \\(S\\) is the allowed perturbation set (e.g., \\(\\ell_\\infty\\) ball).</p>"},{"location":"guides/benchmarking/","title":"Benchmarking","text":"<p>Measuring system performance.</p>"},{"location":"guides/benchmarking/#throughput","title":"Throughput","text":"<p>Measure images per second.</p> <pre><code>\\text{Throughput} = \\frac{\\text{Batch Size} \\times \\text{Batches}}{\\text{Time (s)}}\n</code></pre>"},{"location":"guides/benchmarking/#profiling","title":"Profiling","text":"<p>Use PyTorch Profiler to identify bottlenecks.</p> <pre><code>with torch.profiler.profile(...) as prof:\n    model(input)\nprint(prof.key_averages().table())\n</code></pre> <p>See Performance Architecture for optimization tips.</p>"},{"location":"guides/configuration/","title":"Configuration","text":"<p>Manage complex experiments with YAML config files.</p>"},{"location":"guides/configuration/#structure","title":"Structure","text":"<pre><code>experiment_name: \"unet_sentinel2_v1\"\n\ndata:\n  path: \"data/train.csv\"\n  batch_size: 32\n  num_workers: 4\n\nmodel:\n  name: \"unet\"\n  encoder: \"resnet34\"\n  pretrained: true\n\noptimizer:\n  name: \"adamw\"\n  lr: 1e-4\n  weight_decay: 1e-2\n</code></pre>"},{"location":"guides/configuration/#loading","title":"Loading","text":"<pre><code>from ununennium.config import load_config\n\ncfg = load_config(\"config.yaml\")\n</code></pre>"},{"location":"guides/datasets-and-splits/","title":"Datasets and Splits","text":"<p>Proper dataset management is crucial for geospatial ML.</p>"},{"location":"guides/datasets-and-splits/#spatial-autocorrelation","title":"Spatial Autocorrelation","text":"<p>Nearby pixels are highly correlated. Random splits cause spatial data leakage, leading to over-optimistic validation metrics.</p> <p>Solution: Use block-based splitting.</p> <pre><code>graph TD\n    Raw[Raw Map] --&gt; Grid[Grid Blocks]\n    Grid --&gt; Split{Spatial Split}\n    Split --&gt; Train[Train Blocks]\n    Split --&gt; Val[Val Blocks]\n</code></pre>"},{"location":"guides/datasets-and-splits/#class-imbalance","title":"Class Imbalance","text":"<p>Satellite imagery is often dominated by \"Background\" or \"Water\" classes.</p> <p>Strategies: 1. Oversampling: Sample patches centered on rare classes. 2. Loss Weighting: Inversely proportional to class frequency.</p> <pre><code>w_c = \\frac{N_{total}}{K \\cdot N_c}\n</code></pre>"},{"location":"guides/metrics/","title":"Metrics Guide","text":""},{"location":"guides/metrics/#segmentation","title":"Segmentation","text":""},{"location":"guides/metrics/#iou-jaccard-index","title":"IoU (Jaccard Index)","text":"<pre><code>\\text{IoU} = \\frac{\\text{Intersection}}{\\text{Union}}\n</code></pre>"},{"location":"guides/metrics/#dice-coefficient","title":"Dice Coefficient","text":"<pre><code>\\text{Dice} = \\frac{2 \\cdot \\text{Intersection}}{\\text{Sum of Areas}}\n</code></pre>"},{"location":"guides/metrics/#change-detection","title":"Change Detection","text":""},{"location":"guides/metrics/#precision-recall","title":"Precision &amp; Recall","text":"<pre><code>\\text{Precision} = \\frac{TP}{TP + FP}, \\quad \\text{Recall} = \\frac{TP}{TP + FN}\n</code></pre>"},{"location":"guides/metrics/#calibration","title":"Calibration","text":""},{"location":"guides/metrics/#ece-expected-calibration-error","title":"ECE (Expected Calibration Error)","text":"<pre><code>\\text{ECE} = \\sum_{m=1}^M \\frac{|B_m|}{N} |\\text{acc}(B_m) - \\text{conf}(B_m)|\n</code></pre>"},{"location":"guides/reproducibility/","title":"Reproducibility","text":"<p>Ensuring consistent results across runs.</p>"},{"location":"guides/reproducibility/#seeding","title":"Seeding","text":"<p>Set seeds for all libraries.</p> <pre><code>from ununennium.utils import seed_everything\n\nseed_everything(42)\n# Sets Python, NumPy, PyTorch, CUDA seeds\n</code></pre>"},{"location":"guides/reproducibility/#deterministic-algorithms","title":"Deterministic Algorithms","text":"<p>Enable deterministic CUDA operations (slower but reproducible).</p> <pre><code>torch.use_deterministic_algorithms(True)\n</code></pre>"},{"location":"guides/reproducibility/#hardware-consistency","title":"Hardware Consistency","text":"<p>Results may still vary slightly between different GPU architectures (e.g., A100 vs V100) due to floating point accumulation order.</p>"},{"location":"guides/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions.</p>"},{"location":"guides/troubleshooting/#cuda-oom-out-of-memory","title":"CUDA OOM (Out of Memory)","text":"<p>Symptom: <code>RuntimeError: CUDA out of memory</code></p> <p>Fixes: 1. Reduce <code>batch_size</code>. 2. Enable <code>gradient_checkpointing</code> for the model. 3. Use mixed precision (<code>fp16</code>). 4. Clear cache: <code>torch.cuda.empty_cache()</code> (use sparingly).</p>"},{"location":"guides/troubleshooting/#gdalrasterio-installation","title":"GDAL/Rasterio Installation","text":"<p>Symptom: <code>ImportError: /usr/lib/libgdal.so...</code></p> <p>Fixes: - Use <code>conda</code> for easiest geospatial dependency management. - Or use <code>pip install ununennium[geo]</code> which attempts to pull binary wheels.</p>"},{"location":"guides/troubleshooting/#nan-loss","title":"NaN Loss","text":"<p>Symptom: Loss becomes <code>nan</code>.</p> <p>Fixes: - Check learning rate (too high?). - Check for division by zero in custom losses/metrics. - Enable <code>detect_anomaly=True</code> in PyTorch for traceback.</p>"},{"location":"guides/uncertainty-and-calibration/","title":"Uncertainty and Calibration","text":"<p>Models should know when they don't know.</p>"},{"location":"guides/uncertainty-and-calibration/#probability-calibration","title":"Probability Calibration","text":"<p>A calibrated model's confidence matches its accuracy. If a model says \"80% confidence\", it should be correct 80% of the time.</p>"},{"location":"guides/uncertainty-and-calibration/#reliability-diagrams","title":"Reliability Diagrams","text":"<p>We visualize calibration using reliability diagrams (Confidence vs. Accuracy).</p> <p></p>"},{"location":"guides/uncertainty-and-calibration/#temperature-scaling","title":"Temperature Scaling","text":"<p>A post-processing technique to calibrate predictions.</p> <pre><code>\\hat{p} = \\text{softmax}(\\frac{\\text{logits}}{T})\n</code></pre> <p>where \\(T\\) is a learned scalar parameter.</p>"},{"location":"research/bibliography/","title":"Bibliography","text":"<p>Curated references for Earth observation machine learning. All entries include DOIs or permanent links.</p>"},{"location":"research/bibliography/#foundational-deep-learning","title":"Foundational Deep Learning","text":"<ol> <li> <p>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.    DOI: 10.1038/nature14539</p> </li> <li> <p>He, K., et al. (2016). Deep Residual Learning for Image Recognition. CVPR.    DOI: 10.1109/CVPR.2016.90</p> </li> <li> <p>Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS.    arXiv:1706.03762</p> </li> </ol>"},{"location":"research/bibliography/#segmentation","title":"Segmentation","text":"<ol> <li> <p>Ronneberger, O., et al. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI.    DOI: 10.1007/978-3-319-24574-4_28</p> </li> <li> <p>Chen, L.C., et al. (2018). Encoder-Decoder with Atrous Separable Convolution. ECCV.    DOI: 10.1007/978-3-030-01234-2_49</p> </li> </ol>"},{"location":"research/bibliography/#change-detection","title":"Change Detection","text":"<ol> <li>Daudt, R.C., et al. (2018). Fully Convolutional Siamese Networks for Change Detection. ICIP.    DOI: 10.1109/ICIP.2018.8451652</li> </ol>"},{"location":"research/bibliography/#gans","title":"GANs","text":"<ol> <li> <p>Goodfellow, I., et al. (2014). Generative Adversarial Nets. NeurIPS.    arXiv:1406.2661</p> </li> <li> <p>Isola, P., et al. (2017). Image-to-Image Translation with Conditional Adversarial Networks. CVPR.    DOI: 10.1109/CVPR.2017.632</p> </li> <li> <p>Zhu, J.Y., et al. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent GANs. ICCV.    DOI: 10.1109/ICCV.2017.244</p> </li> <li> <p>Wang, X., et al. (2018). ESRGAN: Enhanced Super-Resolution GANs. ECCV Workshops.     arXiv:1809.00219</p> </li> </ol>"},{"location":"research/bibliography/#physics-informed-neural-networks","title":"Physics-Informed Neural Networks","text":"<ol> <li> <p>Raissi, M., et al. (2019). Physics-informed neural networks. Journal of Computational Physics, 378, 686-707.     DOI: 10.1016/j.jcp.2018.10.045</p> </li> <li> <p>Lu, L., et al. (2021). DeepXDE: A deep learning library for solving differential equations. SIAM Review, 63(1), 208-228.     DOI: 10.1137/19M1274067</p> </li> </ol>"},{"location":"research/bibliography/#remote-sensing-ml","title":"Remote Sensing + ML","text":"<ol> <li>Zhu, X.X., et al. (2017). Deep Learning in Remote Sensing. IEEE GRSM, 5(4), 8-36.     DOI: 10.1109/MGRS.2017.2762307</li> </ol>"},{"location":"research/bibliography/#spectral-indices","title":"Spectral Indices","text":"<ol> <li>Tucker, C.J. (1979). Red and photographic infrared linear combinations. Remote Sensing of Environment, 8(2), 127-150.     DOI: 10.1016/0034-4257(79)90013-0</li> </ol>"},{"location":"research/bibliography/#spatial-statistics","title":"Spatial Statistics","text":"<ol> <li> <p>Moran, P.A.P. (1950). Notes on Continuous Stochastic Phenomena. Biometrika, 37(1/2), 17-23.     DOI: 10.2307/2332142</p> </li> <li> <p>Roberts, D.R., et al. (2017). Cross-validation strategies for data with temporal, spatial structure. Ecography, 40(8), 913-929.     DOI: 10.1111/ecog.02881</p> </li> </ol>"},{"location":"research/bibliography/#datasets","title":"Datasets","text":"<ol> <li>Helber, P., et al. (2019). EuroSAT: A Novel Dataset for Land Use Classification. IEEE JSTARS, 12(7).     DOI: 10.1109/JSTARS.2019.2918242</li> </ol> <p>Last updated: December 16, 2025</p>"},{"location":"research/gan-theory/","title":"GAN Theory","text":"<p>Theoretical foundations of Generative Adversarial Networks for Earth observation.</p>"},{"location":"research/gan-theory/#adversarial-learning","title":"Adversarial Learning","text":""},{"location":"research/gan-theory/#minimax-objective","title":"Minimax Objective","text":"<pre><code>\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]\n</code></pre> <p>Reference: Goodfellow, I., et al. (2014). Generative Adversarial Nets. NeurIPS. arXiv:1406.2661</p>"},{"location":"research/gan-theory/#loss-functions","title":"Loss Functions","text":""},{"location":"research/gan-theory/#lsgan-least-squares","title":"LSGAN (Least Squares)","text":"<pre><code>\\mathcal{L}_D = \\frac{1}{2}\\mathbb{E}_{x}[(D(x) - 1)^2] + \\frac{1}{2}\\mathbb{E}_{z}[D(G(z))^2]\n</code></pre> <p>Reference: Mao, X., et al. (2017). Least Squares Generative Adversarial Networks. ICCV. DOI: 10.1109/ICCV.2017.304</p>"},{"location":"research/gan-theory/#wgan","title":"WGAN","text":"<pre><code>\\mathcal{L}_D = \\mathbb{E}_{z}[D(G(z))] - \\mathbb{E}_{x}[D(x)]\n</code></pre> <p>Reference: Arjovsky, M., et al. (2017). Wasserstein GAN. ICML. arXiv:1701.07875</p>"},{"location":"research/gan-theory/#image-translation","title":"Image Translation","text":""},{"location":"research/gan-theory/#pix2pix","title":"Pix2Pix","text":"<pre><code>\\mathcal{L} = \\mathcal{L}_{cGAN}(G, D) + \\lambda \\mathcal{L}_{L1}(G)\n</code></pre> <p>Reference: Isola, P., et al. (2017). Image-to-Image Translation with Conditional Adversarial Networks. CVPR. DOI: 10.1109/CVPR.2017.632</p>"},{"location":"research/gan-theory/#cyclegan","title":"CycleGAN","text":"<p>Cycle consistency loss:</p> <pre><code>\\mathcal{L}_{cyc}(G, F) = \\mathbb{E}_{x}[\\|F(G(x)) - x\\|_1] + \\mathbb{E}_{y}[\\|G(F(y)) - y\\|_1]\n</code></pre> <p>Reference: Zhu, J.Y., et al. (2017). Unpaired Image-to-Image Translation. ICCV. DOI: 10.1109/ICCV.2017.244</p>"},{"location":"research/gan-theory/#super-resolution","title":"Super-Resolution","text":""},{"location":"research/gan-theory/#perceptual-loss","title":"Perceptual Loss","text":"<pre><code>\\mathcal{L}_{percep} = \\sum_{l} \\frac{1}{C_l H_l W_l} \\|\\phi_l(I_{HR}) - \\phi_l(G(I_{LR}))\\|_2^2\n</code></pre> <p>Reference: Ledig, C., et al. (2017). Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. CVPR. DOI: 10.1109/CVPR.2017.19</p>"},{"location":"research/gan-theory/#training-stability","title":"Training Stability","text":""},{"location":"research/gan-theory/#spectral-normalization","title":"Spectral Normalization","text":"<pre><code>\\bar{W} = \\frac{W}{\\sigma(W)}\n</code></pre> <p>Reference: Miyato, T., et al. (2018). Spectral Normalization for Generative Adversarial Networks. ICLR. arXiv:1802.05957</p>"},{"location":"research/gan-theory/#gradient-penalty","title":"Gradient Penalty","text":"<pre><code>\\mathcal{L}_{GP} = \\lambda \\mathbb{E}_{\\hat{x}}[(\\|\\nabla_{\\hat{x}} D(\\hat{x})\\|_2 - 1)^2]\n</code></pre> <p>Reference: Gulrajani, I., et al. (2017). Improved Training of Wasserstein GANs. NeurIPS. arXiv:1704.00028</p>"},{"location":"research/gan-theory/#see-also","title":"See Also","text":"<ul> <li>Math Foundations</li> <li>PINN Theory</li> </ul>"},{"location":"research/math-foundations/","title":"Mathematical Foundations","text":"<p>This document establishes the mathematical foundations for Earth observation machine learning.</p>"},{"location":"research/math-foundations/#coordinate-reference-systems","title":"Coordinate Reference Systems","text":""},{"location":"research/math-foundations/#geodetic-datum","title":"Geodetic Datum","text":"<p>The World Geodetic System 1984 (WGS84) defines the reference ellipsoid:</p> <pre><code>\\frac{x^2 + y^2}{a^2} + \\frac{z^2}{b^2} = 1\n</code></pre> <p>where a = 6378137 m (semi-major axis) and b = 6356752.314 m (semi-minor axis).</p> <p>Reference: National Imagery and Mapping Agency (2000). Department of Defense World Geodetic System 1984. NIMA TR8350.2.</p>"},{"location":"research/math-foundations/#universal-transverse-mercator-utm","title":"Universal Transverse Mercator (UTM)","text":"<p>UTM divides Earth into 60 zones, each 6 degrees wide:</p> <pre><code>\\text{zone} = \\left\\lfloor \\frac{\\lambda + 180}{6} \\right\\rfloor + 1\n</code></pre> <p>Reference: Snyder, J.P. (1987). Map Projections: A Working Manual. USGS Professional Paper 1395. DOI: 10.3133/pp1395</p>"},{"location":"research/math-foundations/#spectral-indices","title":"Spectral Indices","text":""},{"location":"research/math-foundations/#ndvi-normalized-difference-vegetation-index","title":"NDVI (Normalized Difference Vegetation Index)","text":"<pre><code>\\text{NDVI} = \\frac{\\rho_{NIR} - \\rho_{Red}}{\\rho_{NIR} + \\rho_{Red}}\n</code></pre> <p>NDVI ranges from -1 to +1, with healthy vegetation typically 0.2-0.9.</p> <p>Reference: Tucker, C.J. (1979). Red and photographic infrared linear combinations for monitoring vegetation. Remote Sensing of Environment, 8(2), 127-150. DOI: 10.1016/0034-4257(79)90013-0</p>"},{"location":"research/math-foundations/#evi-enhanced-vegetation-index","title":"EVI (Enhanced Vegetation Index)","text":"<pre><code>\\text{EVI} = G \\cdot \\frac{\\rho_{NIR} - \\rho_{Red}}{\\rho_{NIR} + C_1 \\cdot \\rho_{Red} - C_2 \\cdot \\rho_{Blue} + L}\n</code></pre> <p>with G = 2.5, C\u2081 = 6, C\u2082 = 7.5, and L = 1.</p> <p>Reference: Huete, A., et al. (2002). Overview of the radiometric and biophysical performance of the MODIS vegetation indices. Remote Sensing of Environment, 83(1-2), 195-213. DOI: 10.1016/S0034-4257(02)00096-2</p>"},{"location":"research/math-foundations/#ndwi-normalized-difference-water-index","title":"NDWI (Normalized Difference Water Index)","text":"<pre><code>\\text{NDWI} = \\frac{\\rho_{Green} - \\rho_{NIR}}{\\rho_{Green} + \\rho_{NIR}}\n</code></pre> <p>Reference: McFeeters, S.K. (1996). The use of the Normalized Difference Water Index (NDWI). International Journal of Remote Sensing, 17(7), 1425-1432. DOI: 10.1080/01431169608948714</p>"},{"location":"research/math-foundations/#spatial-statistics","title":"Spatial Statistics","text":""},{"location":"research/math-foundations/#morans-i","title":"Moran's I","text":"<p>Global measure of spatial autocorrelation:</p> <pre><code>I = \\frac{n}{\\sum_i \\sum_j w_{ij}} \\cdot \\frac{\\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i (x_i - \\bar{x})^2}\n</code></pre> Value Interpretation I &gt; 0 Positive autocorrelation (clustering) I \u2248 0 Random pattern I &lt; 0 Negative autocorrelation (dispersion) <p>Reference: Moran, P.A.P. (1950). Notes on Continuous Stochastic Phenomena. Biometrika, 37(1/2), 17-23. DOI: 10.2307/2332142</p>"},{"location":"research/math-foundations/#semivariogram","title":"Semivariogram","text":"<pre><code>\\gamma(h) = \\frac{1}{2|N(h)|} \\sum_{(i,j) \\in N(h)} (z_i - z_j)^2\n</code></pre> <p>Reference: Matheron, G. (1963). Principles of geostatistics. Economic Geology, 58(8), 1246-1266. DOI: 10.2113/gsecongeo.58.8.1246</p>"},{"location":"research/math-foundations/#deep-learning-foundations","title":"Deep Learning Foundations","text":""},{"location":"research/math-foundations/#convolution","title":"Convolution","text":"<pre><code>(I * K)[i,j] = \\sum_m \\sum_n I[i+m, j+n] \\cdot K[m, n]\n</code></pre> <p>Reference: LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444. DOI: 10.1038/nature14539</p>"},{"location":"research/math-foundations/#attention-mechanism","title":"Attention Mechanism","text":"<pre><code>\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n</code></pre> <p>Reference: Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS, 30. arXiv:1706.03762</p>"},{"location":"research/math-foundations/#loss-functions","title":"Loss Functions","text":""},{"location":"research/math-foundations/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<pre><code>\\mathcal{L}_{CE} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)\n</code></pre>"},{"location":"research/math-foundations/#dice-loss","title":"Dice Loss","text":"<pre><code>\\mathcal{L}_{Dice} = 1 - \\frac{2 \\sum_i p_i g_i}{\\sum_i p_i + \\sum_i g_i}\n</code></pre> <p>Reference: Milletari, F., et al. (2016). V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation. 3DV. DOI: 10.1109/3DV.2016.79</p>"},{"location":"research/math-foundations/#see-also","title":"See Also","text":"<ul> <li>GAN Theory</li> <li>PINN Theory</li> <li>Bibliography</li> </ul>"},{"location":"research/pinn-theory/","title":"PINN Theory","text":"<p>Physics-Informed Neural Networks for scientific computing in Earth observation.</p>"},{"location":"research/pinn-theory/#overview","title":"Overview","text":"<p>PINNs incorporate physical laws as soft constraints via the loss function.</p> <p>Reference: Raissi, M., et al. (2019). Physics-informed neural networks. Journal of Computational Physics, 378, 686-707. DOI: 10.1016/j.jcp.2018.10.045</p>"},{"location":"research/pinn-theory/#formulation","title":"Formulation","text":""},{"location":"research/pinn-theory/#general-pde","title":"General PDE","text":"<pre><code>\\mathcal{N}[u](x, t) = 0, \\quad x \\in \\Omega\n</code></pre>"},{"location":"research/pinn-theory/#composite-loss","title":"Composite Loss","text":"<pre><code>\\mathcal{L}(\\theta) = \\lambda_d \\mathcal{L}_{data} + \\lambda_p \\mathcal{L}_{pde} + \\lambda_b \\mathcal{L}_{bc}\n</code></pre> <p>where:</p> <pre><code>\\mathcal{L}_{data} = \\frac{1}{N_d}\\sum_{i=1}^{N_d}\\|u_\\theta(x_i) - u_i^{obs}\\|^2\n</code></pre> <pre><code>\\mathcal{L}_{pde} = \\frac{1}{N_c}\\sum_{j=1}^{N_c}\\|\\mathcal{N}[u_\\theta](x_j)\\|^2\n</code></pre>"},{"location":"research/pinn-theory/#common-pdes","title":"Common PDEs","text":""},{"location":"research/pinn-theory/#heatdiffusion-equation","title":"Heat/Diffusion Equation","text":"<pre><code>\\frac{\\partial u}{\\partial t} = D \\nabla^2 u\n</code></pre>"},{"location":"research/pinn-theory/#advection-diffusion","title":"Advection-Diffusion","text":"<pre><code>\\frac{\\partial u}{\\partial t} + \\mathbf{v} \\cdot \\nabla u = D \\nabla^2 u\n</code></pre> <p>Reference: Okubo, A. (1971). Oceanic diffusion diagrams. Deep Sea Research, 18(8), 789-802. DOI: 10.1016/0011-7471(71)90046-5</p>"},{"location":"research/pinn-theory/#network-architectures","title":"Network Architectures","text":""},{"location":"research/pinn-theory/#fourier-features","title":"Fourier Features","text":"<pre><code>\\gamma(x) = [\\cos(2\\pi B x), \\sin(2\\pi B x)]^T\n</code></pre> <p>Reference: Tancik, M., et al. (2020). Fourier Features Let Networks Learn High Frequency Functions. NeurIPS. arXiv:2006.10739</p>"},{"location":"research/pinn-theory/#collocation-strategies","title":"Collocation Strategies","text":"<p>Reference: Lu, L., et al. (2021). DeepXDE: A deep learning library for solving differential equations. SIAM Review, 63(1), 208-228. DOI: 10.1137/19M1274067</p>"},{"location":"research/pinn-theory/#eo-applications","title":"EO Applications","text":"Application PDE Observable SST Interpolation Advection-Diffusion Temperature Pollution Dispersion Transport Concentration Groundwater Flow Darcy Hydraulic head"},{"location":"research/pinn-theory/#see-also","title":"See Also","text":"<ul> <li>Math Foundations</li> <li>GAN Theory</li> </ul>"},{"location":"research/remote-sensing-task-taxonomy/","title":"Remote Sensing Task Taxonomy","text":"<p>A classification of machine learning tasks for Earth observation.</p>"},{"location":"research/remote-sensing-task-taxonomy/#task-categories","title":"Task Categories","text":"<pre><code>graph TB\n    EO[Earth Observation ML]\n\n    EO --&gt; PIX[Pixel-Level]\n    EO --&gt; SCENE[Scene-Level]\n    EO --&gt; TEMP[Temporal]\n    EO --&gt; GEN[Generative]\n\n    PIX --&gt; SEG[Segmentation]\n    PIX --&gt; REG[Regression]\n\n    SCENE --&gt; CLS[Classification]\n    SCENE --&gt; DET[Detection]\n\n    TEMP --&gt; CD[Change Detection]\n\n    GEN --&gt; SR[Super-Resolution]\n    GEN --&gt; TRANS[Translation]\n</code></pre>"},{"location":"research/remote-sensing-task-taxonomy/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Assign a class label to every pixel.</p> <p>Formulation:</p> <pre><code>f: \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\{1, \\ldots, K\\}^{H \\times W}\n</code></pre> <p>Reference: Ronneberger, O., et al. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI. DOI: 10.1007/978-3-319-24574-4_28</p>"},{"location":"research/remote-sensing-task-taxonomy/#change-detection","title":"Change Detection","text":"<p>Identify changes between temporal observations.</p> <p>Formulation:</p> <pre><code>f: \\mathbb{R}^{H \\times W \\times C} \\times \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\{0, 1\\}^{H \\times W}\n</code></pre> <p>Reference: Daudt, R.C., et al. (2018). Fully Convolutional Siamese Networks for Change Detection. ICIP. DOI: 10.1109/ICIP.2018.8451652</p>"},{"location":"research/remote-sensing-task-taxonomy/#super-resolution","title":"Super-Resolution","text":"<p>Enhance spatial resolution.</p> <pre><code>f: \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\mathbb{R}^{sH \\times sW \\times C}\n</code></pre> <p>Reference: Wang, X., et al. (2018). ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks. ECCV Workshops. arXiv:1809.00219</p>"},{"location":"research/remote-sensing-task-taxonomy/#object-detection","title":"Object Detection","text":"<p>Reference: Lin, T.Y., et al. (2017). Focal Loss for Dense Object Detection. ICCV. DOI: 10.1109/ICCV.2017.324</p>"},{"location":"research/remote-sensing-task-taxonomy/#benchmark-datasets","title":"Benchmark Datasets","text":"Dataset Task Resolution Classes EuroSAT Classification 10m 10 LoveDA Segmentation 30cm 7 LEVIR-CD Change Detection 50cm 2"},{"location":"research/remote-sensing-task-taxonomy/#see-also","title":"See Also","text":"<ul> <li>Math Foundations</li> <li>Bibliography</li> </ul>"},{"location":"tutorials/00_quickstart/","title":"Quickstart Tutorial","text":"<p>This guide covers the end-to-end workflow from data loading to model training and inference.</p>"},{"location":"tutorials/00_quickstart/#1-installation","title":"1. Installation","text":"<pre><code>pip install ununennium[geo]\n</code></pre>"},{"location":"tutorials/00_quickstart/#2-loading-data","title":"2. Loading Data","text":"<p>We'll load a sample multispectral image.</p> <pre><code>from ununennium.core import GeoTensor\nfrom ununennium.io import read_window\n\n# Read a 512x512 window from a Sentinel-2 scene\npath = \"data/sentinel2_l2a.tif\"\ntensor = read_window(path, window=((0, 512), (0, 512)))\n\nprint(f\"Shape: {tensor.shape}\")  # (12, 512, 512)\nprint(f\"CRS: {tensor.crs}\")      # EPSG:32631\n</code></pre>"},{"location":"tutorials/00_quickstart/#3-preprocessing","title":"3. Preprocessing","text":"<p>Compute indices and normalize.</p> <pre><code>from ununennium.preprocessing import compute_ndvi, min_max_scale\n\n# Bands: 0=Blue, 1=Green, 2=Red, 3=NIR\nred = tensor[2]\nnir = tensor[3]\n\nndvi = compute_ndvi(nir, red)\ntensor_norm = min_max_scale(tensor, 0, 10000)\n</code></pre>"},{"location":"tutorials/00_quickstart/#4-model-training","title":"4. Model Training","text":"<p>Train a U-Net on a small dataset.</p> <pre><code>from ununennium.models import UNet\nfrom ununennium.training import Trainer\n\nmodel = UNet(in_channels=12, classes=1)\ntrainer = Trainer(model=model, accelerator=\"gpu\")\n\n# Assume train_loader is defined\ntrainer.fit(train_loader, epochs=10)\n</code></pre>"},{"location":"tutorials/00_quickstart/#5-inference","title":"5. Inference","text":"<p>Predict on new data.</p> <pre><code>pred = model(tensor_norm.unsqueeze(0))\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/","title":"Ingest and Tiling","text":"<p>Processing large satellite imagery requires efficient tiling strategies.</p>"},{"location":"tutorials/01_ingest_and_tiling/#tiling-strategy","title":"Tiling Strategy","text":"<p>We use overlap-tile processing to avoid edge effects.</p> <pre><code>from ununennium.tiling import SlidingWindowTiler\n\ntiler = SlidingWindowTiler(\n    tensor_shape=(10000, 10000),\n    tile_size=512,\n    overlap=64\n)\n\nfor window in tiler:\n    # window is ((row_start, row_end), (col_start, col_end))\n    patch = read_window(path, window)\n</code></pre>"},{"location":"tutorials/01_ingest_and_tiling/#creating-a-dataset","title":"Creating a Dataset","text":"<pre><code>from torch.utils.data import Dataset\n\nclass SentinelDataset(Dataset):\n    def __init__(self, tiler, image_path):\n        self.tiler = tiler\n        self.path = image_path\n\n    def __len__(self):\n        return len(self.tiler)\n\n    def __getitem__(self, idx):\n        window = self.tiler[idx]\n        return read_window(self.path, window)\n</code></pre>"},{"location":"tutorials/02_train_val_test/","title":"Train/Val/Test Splitting","text":"<p>Spatial autocorrelation invalidates random shuffling for splits.</p>"},{"location":"tutorials/02_train_val_test/#spatial-cross-validation","title":"Spatial Cross-Validation","text":"<p>We must split data by contiguous blocks to ensure independence.</p> <pre><code>from ununennium.data import SpatialKFold\n\nsplitter = SpatialKFold(n_splits=5, buffer_radius=100)\n\nfor train_idx, val_idx in splitter.split(coords):\n    # Train coordinates: coords[train_idx]\n    # Val coordinates: coords[val_idx]\n</code></pre>"},{"location":"tutorials/02_train_val_test/#buffer-zones","title":"Buffer Zones","text":"<p>To strictly prevent leakage, we define a buffer zone between train and validation sets.</p> <pre><code>\\text{dist}(S_{train}, S_{val}) &gt; R_{corr}\n</code></pre> <p>where \\(R_{corr}\\) is the spatial autocorrelation range (e.g., from Variogram analysis).</p>"},{"location":"tutorials/03_inference_at_scale/","title":"Inference at Scale","text":"<p>Running inference on terabyte-scale imagery.</p>"},{"location":"tutorials/03_inference_at_scale/#model-export","title":"Model Export","text":"<p>First, export the model to ONNX or TorchScript for speed.</p> <pre><code>from ununennium.export import to_torchscript\n\nscripted_model = to_torchscript(model, example_input)\nscripted_model.save(\"model.pt\")\n</code></pre>"},{"location":"tutorials/03_inference_at_scale/#parallel-inference","title":"Parallel Inference","text":"<p>Use <code>Writer</code> to stitch tiles asynchronously.</p> <pre><code>from ununennium.io import GeoTIFFWriter\n\nwriter = GeoTIFFWriter(\"output.tif\", profile=profile)\n\nwith writer:\n    for batch_windows, batch_preds in inference_loader:\n        writer.write_batch(batch_windows, batch_preds)\n</code></pre>"},{"location":"tutorials/04_change_detection/","title":"Change Detection Tutorial","text":"<p>Detecting changes in bitemporal imagery.</p>"},{"location":"tutorials/04_change_detection/#data-loading","title":"Data Loading","text":"<p>Load pre-event and post-event images.</p> <pre><code>pre_img = read_tensor(\"2024_pre.tif\")\npost_img = read_tensor(\"2025_post.tif\")\n</code></pre>"},{"location":"tutorials/04_change_detection/#siamese-network","title":"Siamese Network","text":"<p>Use a Siamese architecture to extract features from both times.</p> <pre><code>from ununennium.models.architectures import SiameseUNet\n\nmodel = SiameseUNet(in_channels=3, classes=2, fusion=\"diff\")\n# fusion can be \"diff\", \"concat\", or \"dist\"\n</code></pre>"},{"location":"tutorials/04_change_detection/#training","title":"Training","text":"<p>The loss should handle class imbalance (change is rare).</p> <pre><code>from ununennium.losses import DiceLoss\n\ncriterion = DiceLoss()\n</code></pre>"},{"location":"tutorials/05_super_resolution/","title":"Super-Resolution Tutorial","text":"<p>Enhancing visual quality of satellite imagery with ESRGAN.</p>"},{"location":"tutorials/05_super_resolution/#concept","title":"Concept","text":"<p>Upscale low-resolution (LR) inputs to high-resolution (HR) outputs.</p> <pre><code>I_{HR} = G(I_{LR})\n</code></pre>"},{"location":"tutorials/05_super_resolution/#training","title":"Training","text":""},{"location":"tutorials/05_super_resolution/#generator-loss","title":"Generator Loss","text":"<p>Combination of pixel loss, perceptual loss, and adversarial loss.</p> <pre><code>\\mathcal{L}_G = \\lambda_1 \\mathcal{L}_{L1} + \\lambda_2 \\mathcal{L}_{VGG} + \\lambda_3 \\mathcal{L}_{Adv}\n</code></pre>"},{"location":"tutorials/05_super_resolution/#code-example","title":"Code Example","text":"<pre><code>from ununennium.models.gan import ESRGAN\nfrom ununennium.models.gan.losses import PerceptualLoss\n\ngenerator = ESRGAN(scale_factor=4)\nperceptual_criterion = PerceptualLoss()\n\n# In training loop\nfake_hr = generator(lr_img)\nloss_p = perceptual_criterion(fake_hr, real_hr)\n</code></pre>"},{"location":"tutorials/06_gan_recipes/","title":"GAN Recipes","text":"<p>Common workflows for Generative Adversarial Networks.</p>"},{"location":"tutorials/06_gan_recipes/#cloud-removal-cyclegan","title":"Cloud Removal (CycleGAN)","text":"<p>Translate from \"Cloudy\" domain to \"Clear\" domain without paired data.</p> <pre><code>from ununennium.models.gan import CycleGAN\n\nmodel = CycleGAN(in_channels_a=4, in_channels_b=4)\n# Domain A: Cloudy Sentinel-2\n# Domain B: Clear Sentinel-2\n</code></pre>"},{"location":"tutorials/06_gan_recipes/#sar-to-optical-pix2pix","title":"SAR-to-Optical (Pix2Pix)","text":"<p>Translate paired SAR images to Optical.</p> <pre><code>from ununennium.models.gan import Pix2Pix\n\nmodel = Pix2Pix(in_channels=2, out_channels=3)\n# Input: VV, VH (2 channels)\n# Output: RGB (3 channels)\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/","title":"PINN Recipes","text":"<p>Physics-Informed Neural Networks examples.</p>"},{"location":"tutorials/07_pinn_recipes/#advection-diffusion","title":"Advection-Diffusion","text":"<p>Modeling pollutant dispersion.</p> <pre><code>\\frac{\\partial u}{\\partial t} + v \\frac{\\partial u}{\\partial x} = D \\frac{\\partial^2 u}{\\partial x^2}\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#implementation","title":"Implementation","text":"<pre><code>class DispersionPINN(PINNModule):\n    def __init__(self, v=1.0, D=0.1):\n        super().__init__()\n        self.net = MLP(input_dim=2, output_dim=1)\n        self.v = v\n        self.D = D\n\n    def pde_residual(self, x, t, u):\n        u_t = grad(u, t)\n        u_x = grad(u, x)\n        u_xx = grad(u_x, x)\n        return u_t + self.v * u_x - self.D * u_xx\n</code></pre>"},{"location":"tutorials/07_pinn_recipes/#training","title":"Training","text":"<p>Sample points from domain and train.</p> <pre><code>sampler = LatinHypercubeSampler(domain_bounds)\npoints = sampler.sample(1000)\nloss = model.compute_loss(points)\n</code></pre>"}]}