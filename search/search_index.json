{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Ununennium: The Geospatial Deep Learning Standard","text":"<p>Ununennium is a production-grade, theoretically rigorous, and high-performance Python library designed for the era of Petabyte-scale Earth Observation.</p> <p>It bridges the gap between Geospatial Engineering (GDAL, PROJ, OGC standards) and Deep Learning (PyTorch, Autograd, CUDA), providing a unified API for training state-of-the-art models on satellite imagery.</p>"},{"location":"#the-philosophy-of-rigor","title":"The Philosophy of Rigor","text":"<p>Most \"satellite-ai\" libraries treat geospatial rasters as mere pictures (JPEGs). This approach fails in scientific applications because it ignores: 1.  Metric Distortion: The Earth is curved; pixels have variable physical areas. 2.  Spectral Radiometry: \"Color\" is a physical measurement of photons, not a 0-255 integer. 3.  Spatial Autocorrelation: The i.i.d. assumption of standard SGD is violated in geography.</p> <p>Ununennium treats these as first-class citizens. *   GeoTensor: Every tensor knows its CRS (\\(ReferenceSystem\\)) and physical location (\\(Transform\\)). *   Physics-Aware Models: Loss functions respect physical laws (PINNs) and radiometric bounds. *   Sampling Theory: Data loaders implement stratified spatial sampling to respect geostatistics.</p>"},{"location":"#core-capabilities","title":"Core Capabilities","text":""},{"location":"#1-cloud-native-io","title":"1. Cloud-Native I/O","text":"<p>Streaming directly from S3/GCS using COG (Cloud Optimized GeoTIFF) and Zarr logic. No more \"download-then-train\"; we train directly on the data lake.</p> <pre><code># Streaming access to 10TB dataset\nds = ununennium.io.read_stac(\n    \"s3://sentinel-cogs/T32VNM/...\",\n    resolution=10.0,\n    resampling=\"average\" # Radiometrically correct downsampling\n)\n</code></pre>"},{"location":"#2-the-model-zoo","title":"2. The Model Zoo","text":"<p>15+ Architectures optimized for EO. *   Segmentation: U-Net (ResNet/EffNet), DeepLabV3+. *   Generative: Pix2Pix (SAR-to-Optical), CycleGAN (Season Transfer). *   Physics: PDEs solved via Neural Networks (PINNs).</p>"},{"location":"#3-training-at-scale","title":"3. Training at Scale","text":"<p>Mixed Precision (AMP), Distributed Data Parallel (DDP), and Gradient Accumulation tailored for massive 12+ channel satellite inputs.</p>"},{"location":"#navigate-the-documentation","title":"Navigate the Documentation","text":""},{"location":"#theory-guides","title":"Theory Guides","text":"<p>Master the mathematics behind the code. *   Spatial Autocorrelation *   Geodesic Calculations *   Sampling Theory *   Uncertainty Quantification</p>"},{"location":"#feature-catalog","title":"Feature Catalog","text":"<p>Detailed architectural diagrams and formulaic definitions of available models.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Strict typing notation and docstrings for every class and function.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Step-by-step notebooks taking you from raw GeoTIFFs to deployed Onnx models.</p>"},{"location":"#provenance","title":"Provenance","text":"<p>Ununennium is developed by the DeepMind Advanced Agentic Coding Team as an exemplar of \"Software 2.0\" for the physical sciences.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>See CHANGELOG.md in the repository root.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Welcome to Ununennium! This guide will help you get started with satellite imagery machine learning.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<pre><code>pip install ununennium\n</code></pre> <p>For full functionality including geospatial I/O:</p> <pre><code>pip install \"ununennium[all]\"\n</code></pre>"},{"location":"getting_started/#quick-start","title":"Quick Start","text":""},{"location":"getting_started/#loading-satellite-imagery","title":"Loading Satellite Imagery","text":"<pre><code>import ununennium as uu\n\n# Read a GeoTIFF file\ntensor = uu.io.read_geotiff(\"path/to/image.tif\")\nprint(f\"Shape: {tensor.shape}\")\nprint(f\"CRS: {tensor.crs}\")\nprint(f\"Bounds: {tensor.bounds}\")\n\n# Select specific bands\nrgb = tensor.select_bands([2, 1, 0])  # B4, B3, B2 for RGB\n\n# Crop to an area of interest\nfrom ununennium.core import BoundingBox\nbbox = BoundingBox(minx=500000, miny=4500000, maxx=510000, maxy=4510000)\ncropped = tensor.crop(bbox)\n</code></pre>"},{"location":"getting_started/#creating-a-segmentation-model","title":"Creating a Segmentation Model","text":"<pre><code>from ununennium.models import create_model\n\n# Create a U-Net with ResNet-50 backbone for 12-band input\nmodel = create_model(\n    \"unet_resnet50\",\n    in_channels=12,\n    num_classes=10,\n)\n\n# Forward pass\nimport torch\nx = torch.randn(4, 12, 256, 256)\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")  # (4, 10, 256, 256)\n</code></pre>"},{"location":"getting_started/#training-a-model","title":"Training a Model","text":"<pre><code>from ununennium.training import Trainer, CheckpointCallback\nfrom ununennium.datasets import SyntheticDataset\nfrom torch.utils.data import DataLoader\n\n# Create datasets\ntrain_dataset = SyntheticDataset(num_samples=1000, num_channels=12)\nval_dataset = SyntheticDataset(num_samples=200, num_channels=12)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# Setup training\nmodel = create_model(\"unet_resnet50\", in_channels=12, num_classes=10)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nloss_fn = torch.nn.CrossEntropyLoss()\n\ncallbacks = [\n    CheckpointCallback(\"checkpoints/\", monitor=\"val_loss\"),\n]\n\n# Train\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    callbacks=callbacks,\n)\n\nhistory = trainer.fit(epochs=10)\n</code></pre>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference</li> <li>Tutorials</li> <li>Model Zoo</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>PyTorch 2.0+</li> <li>NumPy 1.24+</li> </ul>"},{"location":"installation/#installation-options","title":"Installation Options","text":""},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<pre><code>pip install ununennium\n</code></pre>"},{"location":"installation/#with-geospatial-support","title":"With Geospatial Support","text":"<pre><code>pip install \"ununennium[geo]\"\n</code></pre> <p>Includes: rasterio, pyproj, affine, shapely</p>"},{"location":"installation/#with-stac-support","title":"With STAC Support","text":"<pre><code>pip install \"ununennium[stac]\"\n</code></pre> <p>Includes: pystac, pystac-client, planetary-computer</p>"},{"location":"installation/#with-zarrcloud-storage","title":"With Zarr/Cloud Storage","text":"<pre><code>pip install \"ununennium[zarr]\"\n</code></pre> <p>Includes: zarr, xarray, fsspec, s3fs, gcsfs</p>"},{"location":"installation/#full-installation","title":"Full Installation","text":"<pre><code>pip install \"ununennium[all]\"\n</code></pre> <p>All optional dependencies included.</p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<pre><code>git clone https://github.com/olaflaitinen/ununennium.git\ncd ununennium\npip install -e \".[dev]\"\npre-commit install\n</code></pre>"},{"location":"installation/#gpu-support","title":"GPU Support","text":"<p>PyTorch with CUDA must be installed separately:</p> <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu121\npip install ununennium\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<pre><code>import ununennium as uu\nprint(uu.__version__)  # Should print 1.0.5\n</code></pre>"},{"location":"api/augmentation/","title":"API Reference: Augmentation","text":""},{"location":"api/augmentation/#ununennium.augmentation.geometric","title":"<code>ununennium.augmentation.geometric</code>","text":"<p>Geometric augmentations.</p>"},{"location":"api/augmentation/#ununennium.augmentation.geometric.RandomCrop","title":"<code>RandomCrop</code>","text":"<p>Random crop to specified size.</p> Source code in <code>src/ununennium/augmentation/geometric.py</code> <pre><code>class RandomCrop:\n    \"\"\"Random crop to specified size.\"\"\"\n\n    def __init__(self, size: int | tuple[int, int]):\n        if isinstance(size, int):\n            self.size = (size, size)\n        else:\n            self.size = size\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor | None = None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n        _, h, w = image.shape\n        th, tw = self.size\n\n        if h &lt; th or w &lt; tw:\n            raise ValueError(f\"Image size ({h}, {w}) smaller than crop size {self.size}\")\n\n        y = torch.randint(0, h - th + 1, (1,)).item()\n        x = torch.randint(0, w - tw + 1, (1,)).item()\n\n        image = image[:, y : y + th, x : x + tw]\n        if mask is not None:\n            mask = mask[y : y + th, x : x + tw]\n\n        return image, mask\n</code></pre>"},{"location":"api/augmentation/#ununennium.augmentation.geometric.RandomFlip","title":"<code>RandomFlip</code>","text":"<p>Random horizontal and vertical flip.</p> Source code in <code>src/ununennium/augmentation/geometric.py</code> <pre><code>class RandomFlip:\n    \"\"\"Random horizontal and vertical flip.\"\"\"\n\n    def __init__(self, h_flip_p: float = 0.5, v_flip_p: float = 0.5):\n        self.h_flip_p = h_flip_p\n        self.v_flip_p = v_flip_p\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor | None = None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n        if torch.rand(1).item() &lt; self.h_flip_p:\n            image = torch.flip(image, [-1])\n            if mask is not None:\n                mask = torch.flip(mask, [-1])\n\n        if torch.rand(1).item() &lt; self.v_flip_p:\n            image = torch.flip(image, [-2])\n            if mask is not None:\n                mask = torch.flip(mask, [-2])\n\n        return image, mask\n</code></pre>"},{"location":"api/augmentation/#ununennium.augmentation.geometric.RandomRotate","title":"<code>RandomRotate</code>","text":"<p>Random 90-degree rotations.</p> Source code in <code>src/ununennium/augmentation/geometric.py</code> <pre><code>class RandomRotate:\n    \"\"\"Random 90-degree rotations.\"\"\"\n\n    def __init__(self, p: float = 0.5):\n        self.p = p\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor | None = None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n        if torch.rand(1).item() &lt; self.p:\n            k = int(torch.randint(1, 4, (1,)).item())\n            image = torch.rot90(image, k=k, dims=[-2, -1])\n            if mask is not None:\n                mask = torch.rot90(mask, k=k, dims=[-2, -1])\n\n        return image, mask\n</code></pre>"},{"location":"api/augmentation/#ununennium.augmentation.compose","title":"<code>ununennium.augmentation.compose</code>","text":"<p>Compose multiple augmentations.</p>"},{"location":"api/augmentation/#ununennium.augmentation.compose.Compose","title":"<code>Compose</code>","text":"<p>Compose multiple augmentations.</p> Source code in <code>src/ununennium/augmentation/compose.py</code> <pre><code>class Compose:\n    \"\"\"Compose multiple augmentations.\"\"\"\n\n    def __init__(self, transforms: list[Callable]):\n        self.transforms = transforms\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor | None = None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n        for t in self.transforms:\n            image, mask = t(image, mask)\n        return image, mask\n</code></pre>"},{"location":"api/core/","title":"Core Module API Reference","text":"<p>The <code>nunennium.core</code> module establishes the fundamental data structures for geospatial deep learning. It enforces coordinate reference system (CRS) awareness and bounds tracking throughout the tensor lifecycle.</p>"},{"location":"api/core/#1-geotensor","title":"1. GeoTensor","text":"<p><code>ununennium.core.geotensor.GeoTensor</code></p> <p>A subclass of <code>torch.Tensor</code> that carries geospatial metadata. It behaves exactly like a standard PyTorch tensor but persists metadata through operations.</p>"},{"location":"api/core/#signature","title":"Signature","text":"<pre><code>class GeoTensor(torch.Tensor):\n    def __new__(cls, \n                data: ArrayLike, \n                crs: str | CRS, \n                transform: Affine, \n                bounds: Bounds | None = None) -&gt; GeoTensor\n</code></pre>"},{"location":"api/core/#attributes","title":"Attributes","text":"Attribute Type Description <code>crs</code> <code>pyproj.CRS</code> The Coordinate Reference System of the data. <code>transform</code> <code>affine.Affine</code> The affine transformation matrix mapping pixel coordinates to map coordinates. <code>bounds</code> <code>BoundingBox</code> The spatial extent (left, bottom, right, top). <code>resolution</code> <code>tuple[float, float]</code> Pixel resolution \\((dx, dy)\\) derived from transform."},{"location":"api/core/#methods","title":"Methods","text":""},{"location":"api/core/#reproject","title":"<code>reproject</code>","text":"<p>Reprojects the tensor to a new CRS.</p> <pre><code>def reproject(self, dst_crs: str, resampling: Resampling = Resampling.bilinear) -&gt; GeoTensor\n</code></pre> <ul> <li>dst_crs: Target EPSG code or WKT.</li> <li>resampling: Interpolation kernel (see Resampling Theory).</li> <li>Returns: New <code>GeoTensor</code> in target CRS.</li> <li>Raises: <code>CRSError</code> if transformation is invalid.</li> </ul>"},{"location":"api/core/#crop","title":"<code>crop</code>","text":"<p>Spatial crop based on world coordinates.</p> <pre><code>def crop(self, bbox: BoundingBox) -&gt; GeoTensor\n</code></pre>"},{"location":"api/core/#2-geobatch","title":"2. GeoBatch","text":"<p><code>ununennium.core.geobatch.GeoBatch</code></p> <p>A container for a batch of <code>GeoTensor</code>s, ensuring that a batch collated from different locations maintains spatial context.</p>"},{"location":"api/core/#signature_1","title":"Signature","text":"<pre><code>@dataclass\nclass GeoBatch:\n    images: torch.Tensor\n    masks: torch.Tensor | None\n    metadatas: list[dict]\n</code></pre>"},{"location":"api/core/#usage-pattern","title":"Usage Pattern","text":"<pre><code># In a DataLoader collation function\ndef collate_fn(batch):\n    images = torch.stack([x[\"image\"] for x in batch])\n    metas = [x[\"metadata\"] for x in batch]\n    return GeoBatch(images=images, metadatas=metas)\n</code></pre>"},{"location":"api/core/#3-coordinate-reference-systems-crs","title":"3. Coordinate Reference Systems (CRS)","text":"<p><code>ununennium.core.crs.CRS</code></p> <p>Wrapper around <code>pyproj.CRS</code> providing helper methods for deep learning compatibility.</p> <ul> <li><code>is_geographic</code>: Checks if units are degrees.</li> <li><code>is_projected</code>: Checks if units are meters.</li> <li><code>to_wkt()</code>: Returns Well-Known Text representation.</li> <li><code>to_epsg()</code>: Returns EPSG code integer.</li> </ul>"},{"location":"api/core/#best-practices","title":"Best Practices","text":"<p>[!IMPORTANT] Always use Projected CRS for Training. Convolutional kernels assume Euclidean distance. Training on Lat/Lon (degrees) grids causes spatial distortion variance as latitude changes. Ununennium will emit a warning if you train on a Geographic CRS.</p>"},{"location":"api/datasets/","title":"API Reference: Datasets","text":""},{"location":"api/datasets/#ununennium.datasets.base","title":"<code>ununennium.datasets.base</code>","text":"<p>Base dataset classes.</p>"},{"location":"api/datasets/#ununennium.datasets.base.GeoDataset","title":"<code>GeoDataset</code>","text":"<p>               Bases: <code>ABC</code>, <code>Dataset</code></p> <p>Abstract base class for geospatial datasets.</p> <p>GeoDataset extends PyTorch's Dataset with geospatial awareness, providing CRS information and spatial sampling capabilities.</p> Source code in <code>src/ununennium/datasets/base.py</code> <pre><code>class GeoDataset(ABC, Dataset):\n    \"\"\"Abstract base class for geospatial datasets.\n\n    GeoDataset extends PyTorch's Dataset with geospatial awareness,\n    providing CRS information and spatial sampling capabilities.\n    \"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples in the dataset.\"\"\"\n        ...\n\n    @abstractmethod\n    def __getitem__(self, idx: int) -&gt; tuple[GeoTensor, Any]:\n        \"\"\"Get a sample by index.\n\n        Args:\n            idx: Sample index.\n\n        Returns:\n            Tuple of (image, label) where image is a GeoTensor.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def crs(self) -&gt; Any:\n        \"\"\"Return the coordinate reference system.\"\"\"\n        ...\n\n    @property\n    def num_classes(self) -&gt; int | None:\n        \"\"\"Return number of classes if applicable.\"\"\"\n        return None\n\n    @property\n    def class_names(self) -&gt; list[str] | None:\n        \"\"\"Return class names if applicable.\"\"\"\n        return None\n</code></pre>"},{"location":"api/datasets/#ununennium.datasets.base.GeoDataset.class_names","title":"<code>class_names</code>  <code>property</code>","text":"<p>Return class names if applicable.</p>"},{"location":"api/datasets/#ununennium.datasets.base.GeoDataset.crs","title":"<code>crs</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the coordinate reference system.</p>"},{"location":"api/datasets/#ununennium.datasets.base.GeoDataset.num_classes","title":"<code>num_classes</code>  <code>property</code>","text":"<p>Return number of classes if applicable.</p>"},{"location":"api/datasets/#ununennium.datasets.base.GeoDataset.__getitem__","title":"<code>__getitem__(idx)</code>  <code>abstractmethod</code>","text":"<p>Get a sample by index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Sample index.</p> required <p>Returns:</p> Type Description <code>tuple[GeoTensor, Any]</code> <p>Tuple of (image, label) where image is a GeoTensor.</p> Source code in <code>src/ununennium/datasets/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, idx: int) -&gt; tuple[GeoTensor, Any]:\n    \"\"\"Get a sample by index.\n\n    Args:\n        idx: Sample index.\n\n    Returns:\n        Tuple of (image, label) where image is a GeoTensor.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/datasets/#ununennium.datasets.base.GeoDataset.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return the number of samples in the dataset.</p> Source code in <code>src/ununennium/datasets/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return the number of samples in the dataset.\"\"\"\n    ...\n</code></pre>"},{"location":"api/datasets/#ununennium.datasets.synthetic","title":"<code>ununennium.datasets.synthetic</code>","text":"<p>Synthetic dataset for testing and debugging.</p>"},{"location":"api/datasets/#ununennium.datasets.synthetic.SyntheticDataset","title":"<code>SyntheticDataset</code>","text":"<p>               Bases: <code>GeoDataset</code></p> <p>Synthetic dataset for testing and development.</p> <p>Generates random imagery with configurable properties.</p> Example <p>dataset = SyntheticDataset( ...     num_samples=1000, ...     num_channels=12, ...     image_size=256, ...     num_classes=10, ... ) image, label = dataset[0]</p> Source code in <code>src/ununennium/datasets/synthetic.py</code> <pre><code>class SyntheticDataset(GeoDataset):\n    \"\"\"Synthetic dataset for testing and development.\n\n    Generates random imagery with configurable properties.\n\n    Example:\n        &gt;&gt;&gt; dataset = SyntheticDataset(\n        ...     num_samples=1000,\n        ...     num_channels=12,\n        ...     image_size=256,\n        ...     num_classes=10,\n        ... )\n        &gt;&gt;&gt; image, label = dataset[0]\n    \"\"\"\n\n    def __init__(\n        self,\n        num_samples: int = 1000,\n        num_channels: int = 12,\n        image_size: tuple[int, int] | int = 256,\n        num_classes: int = 10,\n        task: str = \"segmentation\",\n        seed: int | None = None,\n    ):\n        \"\"\"Initialize synthetic dataset.\n\n        Args:\n            num_samples: Number of samples in the dataset.\n            num_channels: Number of spectral bands.\n            image_size: Spatial dimensions (H, W) or single value for square.\n            num_classes: Number of segmentation/classification classes.\n            task: Task type ('segmentation' or 'classification').\n            seed: Random seed for reproducibility.\n        \"\"\"\n        self.num_samples = num_samples\n        self.num_channels = num_channels\n\n        if isinstance(image_size, int):\n            self.image_size = (image_size, image_size)\n        else:\n            self.image_size = image_size\n\n        self._num_classes = num_classes\n        self.task = task\n\n        if seed is not None:\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n\n    def __len__(self) -&gt; int:\n        return self.num_samples\n\n    def __getitem__(self, idx: int) -&gt; tuple[GeoTensor, torch.Tensor]:\n        # Generate random image\n        image = torch.randn(self.num_channels, self.image_size[0], self.image_size[1])\n\n        # Generate label based on task\n        if self.task == \"segmentation\":\n            label = torch.randint(0, self._num_classes, (self.image_size[0], self.image_size[1]))\n        elif self.task == \"classification\":\n            label = torch.randint(0, self._num_classes, (1,)).squeeze()\n        else:\n            raise ValueError(f\"Unknown task: {self.task}\")\n\n        geotensor = GeoTensor(\n            data=image,\n            crs=None,\n            transform=None,\n        )\n\n        return geotensor, label\n\n    @property\n    def crs(self) -&gt; Any:\n        return None\n\n    @property\n    def num_classes(self) -&gt; int:\n        return self._num_classes\n</code></pre>"},{"location":"api/datasets/#ununennium.datasets.synthetic.SyntheticDataset.__init__","title":"<code>__init__(num_samples=1000, num_channels=12, image_size=256, num_classes=10, task='segmentation', seed=None)</code>","text":"<p>Initialize synthetic dataset.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> <code>1000</code> <code>num_channels</code> <code>int</code> <p>Number of spectral bands.</p> <code>12</code> <code>image_size</code> <code>tuple[int, int] | int</code> <p>Spatial dimensions (H, W) or single value for square.</p> <code>256</code> <code>num_classes</code> <code>int</code> <p>Number of segmentation/classification classes.</p> <code>10</code> <code>task</code> <code>str</code> <p>Task type ('segmentation' or 'classification').</p> <code>'segmentation'</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>src/ununennium/datasets/synthetic.py</code> <pre><code>def __init__(\n    self,\n    num_samples: int = 1000,\n    num_channels: int = 12,\n    image_size: tuple[int, int] | int = 256,\n    num_classes: int = 10,\n    task: str = \"segmentation\",\n    seed: int | None = None,\n):\n    \"\"\"Initialize synthetic dataset.\n\n    Args:\n        num_samples: Number of samples in the dataset.\n        num_channels: Number of spectral bands.\n        image_size: Spatial dimensions (H, W) or single value for square.\n        num_classes: Number of segmentation/classification classes.\n        task: Task type ('segmentation' or 'classification').\n        seed: Random seed for reproducibility.\n    \"\"\"\n    self.num_samples = num_samples\n    self.num_channels = num_channels\n\n    if isinstance(image_size, int):\n        self.image_size = (image_size, image_size)\n    else:\n        self.image_size = image_size\n\n    self._num_classes = num_classes\n    self.task = task\n\n    if seed is not None:\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n</code></pre>"},{"location":"api/evaluation/","title":"API Reference: Evaluation","text":""},{"location":"api/evaluation/#ununennium.benchmarks.profiler","title":"<code>ununennium.benchmarks.profiler</code>","text":"<p>Profiling utilities for benchmarking.</p>"},{"location":"api/evaluation/#ununennium.benchmarks.profiler.MemoryProfiler","title":"<code>MemoryProfiler</code>","text":"<p>Track memory usage during operations.</p> Source code in <code>src/ununennium/benchmarks/profiler.py</code> <pre><code>class MemoryProfiler:\n    \"\"\"Track memory usage during operations.\"\"\"\n\n    def __init__(self, track_cuda: bool = True):\n        self.track_cuda = track_cuda\n        self.snapshots: list[dict[str, Any]] = []\n\n    def snapshot(self, label: str) -&gt; dict[str, Any]:\n        \"\"\"Take memory snapshot.\n\n        Args:\n            label: Snapshot label.\n\n        Returns:\n            Memory stats dictionary.\n        \"\"\"\n\n        result: dict[str, Any] = {\"label\": label, \"timestamp\": time.time()}\n\n        # CPU memory\n        if tracemalloc.is_tracing():\n            current, peak = tracemalloc.get_traced_memory()\n            result[\"cpu_current_mb\"] = current / 1024**2\n            result[\"cpu_peak_mb\"] = peak / 1024**2\n\n        # GPU memory\n        if self.track_cuda and torch.cuda.is_available():\n            result[\"cuda_allocated_mb\"] = torch.cuda.memory_allocated() / 1024**2\n            result[\"cuda_reserved_mb\"] = torch.cuda.memory_reserved() / 1024**2\n            result[\"cuda_max_allocated_mb\"] = torch.cuda.max_memory_allocated() / 1024**2\n\n        self.snapshots.append(result)\n        return result\n\n    @contextmanager\n    def track(self, label: str):\n        \"\"\"Context manager for memory tracking.\n\n        Args:\n            label: Section label.\n        \"\"\"\n\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n\n        tracemalloc.start()\n        self.snapshot(f\"{label}_start\")\n\n        try:\n            yield\n        finally:\n            self.snapshot(f\"{label}_end\")\n            tracemalloc.stop()\n\n    @property\n    def peak_memory(self) -&gt; float | None:\n        \"\"\"Get peak memory usage in MB.\"\"\"\n        if not self.snapshots:\n            return None\n        # Find max peak from snapshots\n        peaks = [s.get(\"cpu_peak_mb\", 0) for s in self.snapshots]\n        cuda_peaks = [s.get(\"cuda_max_allocated_mb\", 0) for s in self.snapshots]\n        return max(peaks + cuda_peaks)\n\n    def __enter__(self):\n        # Start tracking manually\n\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n        tracemalloc.start()\n        self.snapshot(\"start\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.snapshot(\"end\")\n        tracemalloc.stop()\n</code></pre>"},{"location":"api/evaluation/#ununennium.benchmarks.profiler.MemoryProfiler.peak_memory","title":"<code>peak_memory</code>  <code>property</code>","text":"<p>Get peak memory usage in MB.</p>"},{"location":"api/evaluation/#ununennium.benchmarks.profiler.MemoryProfiler.snapshot","title":"<code>snapshot(label)</code>","text":"<p>Take memory snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Snapshot label.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Memory stats dictionary.</p> Source code in <code>src/ununennium/benchmarks/profiler.py</code> <pre><code>def snapshot(self, label: str) -&gt; dict[str, Any]:\n    \"\"\"Take memory snapshot.\n\n    Args:\n        label: Snapshot label.\n\n    Returns:\n        Memory stats dictionary.\n    \"\"\"\n\n    result: dict[str, Any] = {\"label\": label, \"timestamp\": time.time()}\n\n    # CPU memory\n    if tracemalloc.is_tracing():\n        current, peak = tracemalloc.get_traced_memory()\n        result[\"cpu_current_mb\"] = current / 1024**2\n        result[\"cpu_peak_mb\"] = peak / 1024**2\n\n    # GPU memory\n    if self.track_cuda and torch.cuda.is_available():\n        result[\"cuda_allocated_mb\"] = torch.cuda.memory_allocated() / 1024**2\n        result[\"cuda_reserved_mb\"] = torch.cuda.memory_reserved() / 1024**2\n        result[\"cuda_max_allocated_mb\"] = torch.cuda.max_memory_allocated() / 1024**2\n\n    self.snapshots.append(result)\n    return result\n</code></pre>"},{"location":"api/evaluation/#ununennium.benchmarks.profiler.MemoryProfiler.track","title":"<code>track(label)</code>","text":"<p>Context manager for memory tracking.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Section label.</p> required Source code in <code>src/ununennium/benchmarks/profiler.py</code> <pre><code>@contextmanager\ndef track(self, label: str):\n    \"\"\"Context manager for memory tracking.\n\n    Args:\n        label: Section label.\n    \"\"\"\n\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n\n    tracemalloc.start()\n    self.snapshot(f\"{label}_start\")\n\n    try:\n        yield\n    finally:\n        self.snapshot(f\"{label}_end\")\n        tracemalloc.stop()\n</code></pre>"},{"location":"api/evaluation/#ununennium.benchmarks.profiler.Profiler","title":"<code>Profiler</code>","text":"<p>Hierarchical profiler for timing code sections.</p> Source code in <code>src/ununennium/benchmarks/profiler.py</code> <pre><code>class Profiler:\n    \"\"\"Hierarchical profiler for timing code sections.\"\"\"\n\n    def __init__(self):\n        self.timings: dict[str, list[float]] = defaultdict(list)\n        self._stack: list[str] = []\n\n    @contextmanager\n    def section(self, name: str):\n        \"\"\"Time a code section.\n\n        Args:\n            name: Section name.\n\n        Yields:\n            Context manager.\n        \"\"\"\n        full_name = \"/\".join([*self._stack, name])\n        self._stack.append(name)\n        start = time.perf_counter()\n        try:\n            yield\n        finally:\n            elapsed = (time.perf_counter() - start) * 1000\n            self._stack.pop()\n            self.timings[full_name].append(elapsed)\n\n    def report(self) -&gt; dict[str, dict[str, float]]:\n        \"\"\"Generate timing report.\n\n        Returns:\n            Dictionary with statistics per section.\n        \"\"\"\n        return {\n            name: {\n                \"mean_ms\": float(np.mean(times)),\n                \"std_ms\": float(np.std(times)),\n                \"min_ms\": float(np.min(times)),\n                \"max_ms\": float(np.max(times)),\n                \"count\": len(times),\n            }\n            for name, times in self.timings.items()\n        }\n\n    def reset(self) -&gt; None:\n        \"\"\"Clear all timings.\"\"\"\n        self.timings.clear()\n</code></pre>"},{"location":"api/evaluation/#ununennium.benchmarks.profiler.Profiler.report","title":"<code>report()</code>","text":"<p>Generate timing report.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, float]]</code> <p>Dictionary with statistics per section.</p> Source code in <code>src/ununennium/benchmarks/profiler.py</code> <pre><code>def report(self) -&gt; dict[str, dict[str, float]]:\n    \"\"\"Generate timing report.\n\n    Returns:\n        Dictionary with statistics per section.\n    \"\"\"\n    return {\n        name: {\n            \"mean_ms\": float(np.mean(times)),\n            \"std_ms\": float(np.std(times)),\n            \"min_ms\": float(np.min(times)),\n            \"max_ms\": float(np.max(times)),\n            \"count\": len(times),\n        }\n        for name, times in self.timings.items()\n    }\n</code></pre>"},{"location":"api/evaluation/#ununennium.benchmarks.profiler.Profiler.reset","title":"<code>reset()</code>","text":"<p>Clear all timings.</p> Source code in <code>src/ununennium/benchmarks/profiler.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Clear all timings.\"\"\"\n    self.timings.clear()\n</code></pre>"},{"location":"api/evaluation/#ununennium.benchmarks.profiler.Profiler.section","title":"<code>section(name)</code>","text":"<p>Time a code section.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Section name.</p> required <p>Yields:</p> Type Description <p>Context manager.</p> Source code in <code>src/ununennium/benchmarks/profiler.py</code> <pre><code>@contextmanager\ndef section(self, name: str):\n    \"\"\"Time a code section.\n\n    Args:\n        name: Section name.\n\n    Yields:\n        Context manager.\n    \"\"\"\n    full_name = \"/\".join([*self._stack, name])\n    self._stack.append(name)\n    start = time.perf_counter()\n    try:\n        yield\n    finally:\n        elapsed = (time.perf_counter() - start) * 1000\n        self._stack.pop()\n        self.timings[full_name].append(elapsed)\n</code></pre>"},{"location":"api/evaluation/#ununennium.benchmarks.throughput","title":"<code>ununennium.benchmarks.throughput</code>","text":"<p>Throughput benchmarking functions.</p>"},{"location":"api/evaluation/#ununennium.benchmarks.throughput.benchmark_inference","title":"<code>benchmark_inference(model, input_shape, n_iterations=100, warmup_iterations=10, device='cuda', mixed_precision=True)</code>","text":"<p>Benchmark model inference throughput.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to benchmark.</p> required <code>input_shape</code> <code>tuple[int, ...]</code> <p>Input tensor shape (B, C, H, W).</p> required <code>n_iterations</code> <code>int</code> <p>Number of benchmark iterations.</p> <code>100</code> <code>warmup_iterations</code> <code>int</code> <p>Number of warmup iterations.</p> <code>10</code> <code>device</code> <code>str</code> <p>Device to run on.</p> <code>'cuda'</code> <code>mixed_precision</code> <code>bool</code> <p>Whether to use AMP.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with throughput metrics.</p> Source code in <code>src/ununennium/benchmarks/throughput.py</code> <pre><code>def benchmark_inference(\n    model: nn.Module,\n    input_shape: tuple[int, ...],\n    n_iterations: int = 100,\n    warmup_iterations: int = 10,\n    device: str = \"cuda\",\n    mixed_precision: bool = True,\n) -&gt; dict[str, float]:\n    \"\"\"Benchmark model inference throughput.\n\n    Args:\n        model: Model to benchmark.\n        input_shape: Input tensor shape (B, C, H, W).\n        n_iterations: Number of benchmark iterations.\n        warmup_iterations: Number of warmup iterations.\n        device: Device to run on.\n        mixed_precision: Whether to use AMP.\n\n    Returns:\n        Dictionary with throughput metrics.\n    \"\"\"\n    model = model.to(device)\n    model.eval()\n\n    dummy_input = torch.randn(*input_shape, device=device)\n\n    # Warmup\n    for _ in range(warmup_iterations):\n        with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=mixed_precision):\n            _ = model(dummy_input)\n\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n\n    # Benchmark\n    profiler = Profiler()\n    memory = MemoryProfiler()\n\n    if device == \"cuda\":\n        torch.cuda.reset_peak_memory_stats()\n\n    with profiler.section(\"inference\"):\n        for _ in range(n_iterations):\n            with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=mixed_precision):\n                _ = model(dummy_input)\n\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n        memory.snapshot(\"post_benchmark\")\n\n    total_time_ms = sum(profiler.timings[\"inference\"])\n    batch_size = input_shape[0]\n\n    return {\n        \"iterations\": n_iterations,\n        \"batch_size\": batch_size,\n        \"total_time_ms\": total_time_ms,\n        \"avg_latency_ms\": total_time_ms / n_iterations,\n        \"samples_per_second\": (n_iterations * batch_size) / (total_time_ms / 1000),\n        \"p50_latency_ms\": float(np.percentile(profiler.timings[\"inference\"], 50)),\n        \"p95_latency_ms\": float(np.percentile(profiler.timings[\"inference\"], 95)),\n        \"p99_latency_ms\": float(np.percentile(profiler.timings[\"inference\"], 99)),\n        \"gpu_memory_peak_mb\": (\n            torch.cuda.max_memory_allocated() / 1024**2 if device == \"cuda\" else 0\n        ),\n    }\n</code></pre>"},{"location":"api/evaluation/#ununennium.benchmarks.throughput.benchmark_training","title":"<code>benchmark_training(model, loss_fn, input_shape, target_shape, n_iterations=50, warmup_iterations=5, device='cuda', mixed_precision=True)</code>","text":"<p>Benchmark training iteration throughput.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to benchmark.</p> required <code>loss_fn</code> <code>Module</code> <p>Loss function.</p> required <code>input_shape</code> <code>tuple[int, ...]</code> <p>Input tensor shape.</p> required <code>target_shape</code> <code>tuple[int, ...]</code> <p>Target tensor shape.</p> required <code>n_iterations</code> <code>int</code> <p>Number of benchmark iterations.</p> <code>50</code> <code>warmup_iterations</code> <code>int</code> <p>Number of warmup iterations.</p> <code>5</code> <code>device</code> <code>str</code> <p>Device to run on.</p> <code>'cuda'</code> <code>mixed_precision</code> <code>bool</code> <p>Whether to use AMP.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with throughput metrics.</p> Source code in <code>src/ununennium/benchmarks/throughput.py</code> <pre><code>def benchmark_training(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    input_shape: tuple[int, ...],\n    target_shape: tuple[int, ...],\n    n_iterations: int = 50,\n    warmup_iterations: int = 5,\n    device: str = \"cuda\",\n    mixed_precision: bool = True,\n) -&gt; dict[str, float]:\n    \"\"\"Benchmark training iteration throughput.\n\n    Args:\n        model: Model to benchmark.\n        loss_fn: Loss function.\n        input_shape: Input tensor shape.\n        target_shape: Target tensor shape.\n        n_iterations: Number of benchmark iterations.\n        warmup_iterations: Number of warmup iterations.\n        device: Device to run on.\n        mixed_precision: Whether to use AMP.\n\n    Returns:\n        Dictionary with throughput metrics.\n    \"\"\"\n    model = model.to(device)\n    model.train()\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n    scaler = torch.amp.GradScaler(\"cuda\") if mixed_precision else None\n\n    dummy_input = torch.randn(*input_shape, device=device)\n    dummy_target = torch.randint(0, 10, target_shape, device=device)\n\n    # Warmup\n    for _ in range(warmup_iterations):\n        with torch.amp.autocast(\"cuda\", enabled=mixed_precision):\n            output = model(dummy_input)\n            loss = loss_fn(output, dummy_target)\n\n        if scaler:\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            optimizer.step()\n        optimizer.zero_grad()\n\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n        torch.cuda.reset_peak_memory_stats()\n\n    # Benchmark\n    profiler = Profiler()\n\n    with profiler.section(\"training\"):\n        for _ in range(n_iterations):\n            with (\n                profiler.section(\"forward\"),\n                torch.autocast(device_type=\"cuda\", enabled=mixed_precision),\n            ):\n                output = model(dummy_input)\n                loss = loss_fn(output, dummy_target)\n\n            with profiler.section(\"backward\"):\n                if scaler:\n                    scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n\n            with profiler.section(\"optimizer\"):\n                if scaler:\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    optimizer.step()\n                optimizer.zero_grad()\n\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n\n    total_time_ms = sum(profiler.timings[\"training\"])\n    batch_size = input_shape[0]\n\n    return {\n        \"iterations\": n_iterations,\n        \"batch_size\": batch_size,\n        \"total_time_ms\": total_time_ms,\n        \"avg_iteration_ms\": total_time_ms / n_iterations,\n        \"samples_per_second\": (n_iterations * batch_size) / (total_time_ms / 1000),\n        \"forward_ms\": float(np.mean(profiler.timings.get(\"training/forward\", [0]))),\n        \"backward_ms\": float(np.mean(profiler.timings.get(\"training/backward\", [0]))),\n        \"optimizer_ms\": float(np.mean(profiler.timings.get(\"training/optimizer\", [0]))),\n        \"gpu_memory_peak_mb\": (\n            torch.cuda.max_memory_allocated() / 1024**2 if device == \"cuda\" else 0\n        ),\n    }\n</code></pre>"},{"location":"api/io/","title":"I/O Module API Reference","text":"<p>The <code>ununennium.io</code> module provides high-performance, cloud-native access to geospatial raster data. It abstracts GDAL complexity and optimizes for deep learning throughput.</p>"},{"location":"api/io/#1-readers","title":"1. Readers","text":""},{"location":"api/io/#read_geotiff","title":"<code>read_geotiff</code>","text":"<p>Reads a GeoTIFF file (local or S3) into a <code>GeoTensor</code>.</p> <pre><code>def read_geotiff(\n    path: str | Path,\n    bands: list[int] | None = None,\n    window: Window | None = None,\n    out_shape: tuple[int, int] | None = None,\n    resampling: Resampling = Resampling.bilinear\n) -&gt; GeoTensor\n</code></pre> <p>Parameters: *   <code>path</code> (str): Path to file. Supports VSI (e.g., <code>/vsis3/bucket/key</code>). *   <code>bands</code> (list[int]): 1-based indices of bands to read. Defaults to all. *   <code>window</code> (Window): Rasterio window for lazy reading of subsets. *   <code>out_shape</code> (tuple): Target \\((H, W)\\) for on-the-fly resizing. *   <code>resampling</code>: Kernel to use if <code>out_shape</code> differs from window size.</p> <p>Performance Note: Uses <code>rasterio.Env</code> with <code>GDAL_DISABLE_READDIR_ON_OPEN</code> for optimized cloud performance.</p>"},{"location":"api/io/#read_stac","title":"<code>read_stac</code>","text":"<p>Resolves and reads assets from a SpatioTemporal Asset Catalog (STAC) Item.</p> <pre><code>def read_stac(\n    item: pystac.Item,\n    assets: list[str] = [\"red\", \"green\", \"blue\"],\n    resolution: float | None = None\n) -&gt; GeoTensor\n</code></pre> <p>Features: *   Automatic Resampling: If assets have different native resolutions (e.g., Sentinel-2 10m/20m bands), they are upsampled to the finest resolution automatically. *   Stacking: Returns a single multi-channel tensor.</p>"},{"location":"api/io/#read_zarr","title":"<code>read_zarr</code>","text":"<p>Reads N-dimensional array data from a Zarr data store.</p> <pre><code>def read_zarr(\n    store: str | zarr.Store,\n    selection: tuple[slice, ...] | None = None\n) -&gt; GeoTensor\n</code></pre> <p>Use Case: Ideal for datacube access (Time Series) where temporal slicing is efficient.</p>"},{"location":"api/io/#2-writers","title":"2. Writers","text":""},{"location":"api/io/#write_geotiff","title":"<code>write_geotiff</code>","text":"<p>Persists a <code>GeoTensor</code> to disk.</p> <pre><code>def write_geotiff(\n    path: str | Path,\n    tensor: GeoTensor,\n    profile: dict | None = None,\n    tiled: bool = True,\n    compress: str = \"deflate\"\n) -&gt; None\n</code></pre> <p>Parameters: *   <code>path</code>: Destination path. *   <code>tensor</code>: Data to write. Must have <code>crs</code> and <code>transform</code> attributes. *   <code>profile</code>: Rasterio creation options (e.g., non-default block sizes). *   <code>tiled</code> (bool): Whether to create internal tiles (512x512). Critical for performance in subsequent reads. *   <code>compress</code> (str): Compression algorithm (<code>lzw</code>, <code>deflate</code>, <code>zstd</code>).</p>"},{"location":"api/io/#3-exception-handling","title":"3. Exception Handling","text":"<p>The module defines specific exceptions for robust error handling in pipelines.</p> <ul> <li><code>RasterNotFoundError</code>: File does not exist or is inaccessible.</li> <li><code>CRSMismatchError</code>: Attempting to stack bands with different projections.</li> <li><code>BandIndexError</code>: Requesting band N in an M-band image (where N &gt; M).</li> </ul>"},{"location":"api/losses/","title":"API Reference: Losses","text":""},{"location":"api/losses/#ununennium.losses.segmentation","title":"<code>ununennium.losses.segmentation</code>","text":"<p>Segmentation losses.</p>"},{"location":"api/losses/#ununennium.losses.segmentation.CombinedLoss","title":"<code>CombinedLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Combine multiple losses with weights.</p> Source code in <code>src/ununennium/losses/segmentation.py</code> <pre><code>class CombinedLoss(nn.Module):\n    \"\"\"Combine multiple losses with weights.\"\"\"\n\n    def __init__(self, losses: list[nn.Module], weights: list[float] | None = None):\n        super().__init__()\n        self.losses = nn.ModuleList(losses)\n        self.weights = weights or [1.0] * len(losses)\n\n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        total = torch.tensor(0.0, device=pred.device)\n        for loss, weight in zip(self.losses, self.weights, strict=False):\n            total = total + weight * loss(pred, target)\n        return total\n</code></pre>"},{"location":"api/losses/#ununennium.losses.segmentation.DiceLoss","title":"<code>DiceLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Dice loss for segmentation.</p> Source code in <code>src/ununennium/losses/segmentation.py</code> <pre><code>class DiceLoss(nn.Module):\n    \"\"\"Dice loss for segmentation.\"\"\"\n\n    def __init__(self, smooth: float = 1e-6, multiclass: bool = True):\n        super().__init__()\n        self.smooth = smooth\n        self.multiclass = multiclass\n\n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        if self.multiclass:\n            pred = F.softmax(pred, dim=1)\n            num_classes = pred.shape[1]\n\n            target_onehot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()\n\n            intersection = (pred * target_onehot).sum(dim=(0, 2, 3))\n            union = pred.sum(dim=(0, 2, 3)) + target_onehot.sum(dim=(0, 2, 3))\n\n            dice = (2 * intersection + self.smooth) / (union + self.smooth)\n            return 1 - dice.mean()\n        else:\n            pred = torch.sigmoid(pred)\n            intersection = (pred * target).sum()\n            union = pred.sum() + target.sum()\n            dice = (2 * intersection + self.smooth) / (union + self.smooth)\n            return 1 - dice\n</code></pre>"},{"location":"api/losses/#ununennium.losses.segmentation.FocalLoss","title":"<code>FocalLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Focal loss for handling class imbalance.</p> Source code in <code>src/ununennium/losses/segmentation.py</code> <pre><code>class FocalLoss(nn.Module):\n    \"\"\"Focal loss for handling class imbalance.\"\"\"\n\n    def __init__(self, alpha: float = 0.25, gamma: float = 2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        ce_loss = F.cross_entropy(pred, target, reduction=\"none\")\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()\n</code></pre>"},{"location":"api/metrics/","title":"API Reference: Metrics","text":""},{"location":"api/metrics/#ununennium.metrics.segmentation","title":"<code>ununennium.metrics.segmentation</code>","text":"<p>Segmentation metrics.</p>"},{"location":"api/metrics/#ununennium.metrics.segmentation.dice_score","title":"<code>dice_score(pred, target, num_classes, smooth=1e-06)</code>","text":"<p>Compute Dice score per class.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted labels or logits.</p> required <code>target</code> <code>Tensor</code> <p>Ground truth labels.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>smooth</code> <code>float</code> <p>Smoothing factor.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Dice score per class.</p> Source code in <code>src/ununennium/metrics/segmentation.py</code> <pre><code>def dice_score(\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    num_classes: int,\n    smooth: float = 1e-6,\n) -&gt; torch.Tensor:\n    \"\"\"Compute Dice score per class.\n\n    Args:\n        pred: Predicted labels or logits.\n        target: Ground truth labels.\n        num_classes: Number of classes.\n        smooth: Smoothing factor.\n\n    Returns:\n        Dice score per class.\n    \"\"\"\n    if pred.dim() == 4:\n        pred = pred.argmax(dim=1)\n\n    dice_per_class = torch.zeros(num_classes, device=pred.device)\n\n    for c in range(num_classes):\n        pred_c = (pred == c).float()\n        target_c = (target == c).float()\n\n        intersection = (pred_c * target_c).sum()\n        dice_per_class[c] = (2 * intersection + smooth) / (pred_c.sum() + target_c.sum() + smooth)\n\n    return dice_per_class\n</code></pre>"},{"location":"api/metrics/#ununennium.metrics.segmentation.iou_score","title":"<code>iou_score(pred, target, num_classes, ignore_index=-100)</code>","text":"<p>Compute Intersection over Union (IoU) per class.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted labels (B, H, W) or logits (B, C, H, W).</p> required <code>target</code> <code>Tensor</code> <p>Ground truth labels (B, H, W).</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>ignore_index</code> <code>int</code> <p>Index to ignore in computation.</p> <code>-100</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>IoU score per class (num_classes,).</p> Source code in <code>src/ununennium/metrics/segmentation.py</code> <pre><code>def iou_score(\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    num_classes: int,\n    ignore_index: int = -100,\n) -&gt; torch.Tensor:\n    \"\"\"Compute Intersection over Union (IoU) per class.\n\n    Args:\n        pred: Predicted labels (B, H, W) or logits (B, C, H, W).\n        target: Ground truth labels (B, H, W).\n        num_classes: Number of classes.\n        ignore_index: Index to ignore in computation.\n\n    Returns:\n        IoU score per class (num_classes,).\n    \"\"\"\n    if pred.dim() == 4:\n        pred = pred.argmax(dim=1)\n\n    iou_per_class = torch.zeros(num_classes, device=pred.device)\n\n    for c in range(num_classes):\n        pred_c = pred == c\n        target_c = target == c\n        mask = target != ignore_index\n\n        intersection = (pred_c &amp; target_c &amp; mask).sum().float()\n        union = ((pred_c | target_c) &amp; mask).sum().float()\n\n        if union &gt; 0:\n            iou_per_class[c] = intersection / union\n\n    return iou_per_class\n</code></pre>"},{"location":"api/metrics/#ununennium.metrics.segmentation.pixel_accuracy","title":"<code>pixel_accuracy(pred, target)</code>","text":"<p>Compute overall pixel accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted labels or logits.</p> required <code>target</code> <code>Tensor</code> <p>Ground truth labels.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Pixel accuracy as float.</p> Source code in <code>src/ununennium/metrics/segmentation.py</code> <pre><code>def pixel_accuracy(pred: torch.Tensor, target: torch.Tensor) -&gt; float:\n    \"\"\"Compute overall pixel accuracy.\n\n    Args:\n        pred: Predicted labels or logits.\n        target: Ground truth labels.\n\n    Returns:\n        Pixel accuracy as float.\n    \"\"\"\n    if pred.dim() == 4:\n        pred = pred.argmax(dim=1)\n\n    correct = (pred == target).sum().float()\n    total = target.numel()\n\n    return (correct / total).item()\n</code></pre>"},{"location":"api/preprocessing/","title":"API Reference: Preprocessing","text":""},{"location":"api/preprocessing/#ununennium.preprocessing.normalization","title":"<code>ununennium.preprocessing.normalization</code>","text":"<p>Normalization utilities for satellite imagery.</p>"},{"location":"api/preprocessing/#ununennium.preprocessing.normalization.denormalize","title":"<code>denormalize(data, stats, method='minmax')</code>","text":"<p>Reverse normalization using saved statistics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Normalized tensor.</p> required <code>stats</code> <code>dict[str, Tensor]</code> <p>Statistics dictionary from normalize().</p> required <code>method</code> <code>Literal['minmax', 'zscore', 'percentile']</code> <p>Normalization method that was used.</p> <code>'minmax'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Denormalized tensor.</p> Source code in <code>src/ununennium/preprocessing/normalization.py</code> <pre><code>def denormalize(\n    data: torch.Tensor,\n    stats: dict[str, torch.Tensor],\n    method: Literal[\"minmax\", \"zscore\", \"percentile\"] = \"minmax\",\n) -&gt; torch.Tensor:\n    \"\"\"Reverse normalization using saved statistics.\n\n    Args:\n        data: Normalized tensor.\n        stats: Statistics dictionary from normalize().\n        method: Normalization method that was used.\n\n    Returns:\n        Denormalized tensor.\n    \"\"\"\n    if method == \"minmax\":\n        min_vals = stats[\"min\"]\n        max_vals = stats[\"max\"]\n        return data * (max_vals - min_vals) + min_vals\n\n    elif method == \"zscore\":\n        mean = stats[\"mean\"]\n        std = stats[\"std\"]\n        return data * std + mean\n\n    elif method == \"percentile\":\n        low = stats[\"low\"]\n        high = stats[\"high\"]\n        return data * (high - low) + low\n\n    else:\n        raise ValueError(f\"Unknown normalization method: {method}\")\n</code></pre>"},{"location":"api/preprocessing/#ununennium.preprocessing.normalization.normalize","title":"<code>normalize(data, method='minmax', per_channel=True, percentiles=(2, 98))</code>","text":"<p>Normalize tensor data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor of shape (..., C, H, W) or (C, H, W).</p> required <code>method</code> <code>Literal['minmax', 'zscore', 'percentile']</code> <p>Normalization method to use.</p> <code>'minmax'</code> <code>per_channel</code> <code>bool</code> <p>Whether to normalize each channel independently.</p> <code>True</code> <code>percentiles</code> <code>tuple[float, float]</code> <p>Percentiles for percentile normalization.</p> <code>(2, 98)</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (normalized_data, stats_dict) where stats_dict contains</p> <code>dict[str, Tensor]</code> <p>the statistics needed for denormalization.</p> Example <p>normalized, stats = normalize(data, method=\"zscore\") original = denormalize(normalized, stats, method=\"zscore\")</p> Source code in <code>src/ununennium/preprocessing/normalization.py</code> <pre><code>def normalize(\n    data: torch.Tensor,\n    method: Literal[\"minmax\", \"zscore\", \"percentile\"] = \"minmax\",\n    per_channel: bool = True,\n    percentiles: tuple[float, float] = (2, 98),\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"Normalize tensor data.\n\n    Args:\n        data: Input tensor of shape (..., C, H, W) or (C, H, W).\n        method: Normalization method to use.\n        per_channel: Whether to normalize each channel independently.\n        percentiles: Percentiles for percentile normalization.\n\n    Returns:\n        Tuple of (normalized_data, stats_dict) where stats_dict contains\n        the statistics needed for denormalization.\n\n    Example:\n        &gt;&gt;&gt; normalized, stats = normalize(data, method=\"zscore\")\n        &gt;&gt;&gt; original = denormalize(normalized, stats, method=\"zscore\")\n    \"\"\"\n    if method == \"minmax\":\n        if per_channel:\n            # Shape: (C,) for each stat\n            dims = (*tuple(range(data.ndim - 2)), -2, -1)\n            min_vals = data.amin(dim=dims, keepdim=True)\n            max_vals = data.amax(dim=dims, keepdim=True)\n        else:\n            min_vals = data.min()\n            max_vals = data.max()\n\n        denominator = max_vals - min_vals\n        denominator = torch.where(denominator == 0, torch.ones_like(denominator), denominator)\n\n        normalized = (data - min_vals) / denominator\n        stats = {\"min\": min_vals, \"max\": max_vals}\n\n    elif method == \"zscore\":\n        if per_channel:\n            dims = (*tuple(range(data.ndim - 2)), -2, -1)\n            mean = data.mean(dim=dims, keepdim=True)\n            std = data.std(dim=dims, keepdim=True)\n        else:\n            mean = data.mean()\n            std = data.std()\n\n        std = torch.where(std == 0, torch.ones_like(std), std)\n        normalized = (data - mean) / std\n        stats = {\"mean\": mean, \"std\": std}\n\n    elif method == \"percentile\":\n        if per_channel:\n            # Per-channel percentile normalization\n            flat = data.flatten(-2, -1)\n            low = torch.quantile(flat, percentiles[0] / 100, dim=-1, keepdim=True)\n            high = torch.quantile(flat, percentiles[1] / 100, dim=-1, keepdim=True)\n            low = low.unsqueeze(-1)\n            high = high.unsqueeze(-1)\n        else:\n            low = torch.quantile(data, percentiles[0] / 100)\n            high = torch.quantile(data, percentiles[1] / 100)\n\n        denominator = high - low\n        denominator = torch.where(denominator == 0, torch.ones_like(denominator), denominator)\n\n        normalized = (data - low) / denominator\n        normalized = torch.clamp(normalized, 0, 1)\n        stats = {\"low\": low, \"high\": high}\n\n    else:\n        raise ValueError(f\"Unknown normalization method: {method}\")\n\n    return normalized, stats\n</code></pre>"},{"location":"api/preprocessing/#ununennium.preprocessing.indices","title":"<code>ununennium.preprocessing.indices</code>","text":"<p>Spectral indices for remote sensing analysis.</p>"},{"location":"api/preprocessing/#ununennium.preprocessing.indices.compute_index","title":"<code>compute_index(tensor, index_name, band_mapping=None)</code>","text":"<p>Compute a spectral index from a GeoTensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>GeoTensor</code> <p>Input GeoTensor with multi-spectral bands.</p> required <code>index_name</code> <code>str</code> <p>Name of the index ('ndvi', 'ndwi', 'evi', etc.).</p> required <code>band_mapping</code> <code>dict[str, int] | None</code> <p>Mapping from band names to indices. If None, uses Sentinel-2 L2A ordering.</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoTensor</code> <p>GeoTensor with computed index as single band.</p> Source code in <code>src/ununennium/preprocessing/indices.py</code> <pre><code>def compute_index(\n    tensor: GeoTensor,\n    index_name: str,\n    band_mapping: dict[str, int] | None = None,\n) -&gt; GeoTensor:\n    \"\"\"Compute a spectral index from a GeoTensor.\n\n    Args:\n        tensor: Input GeoTensor with multi-spectral bands.\n        index_name: Name of the index ('ndvi', 'ndwi', 'evi', etc.).\n        band_mapping: Mapping from band names to indices.\n            If None, uses Sentinel-2 L2A ordering.\n\n    Returns:\n        GeoTensor with computed index as single band.\n    \"\"\"\n    # Default Sentinel-2 L2A band ordering (10m + 20m bands)\n    if band_mapping is None:\n        band_mapping = {\n            \"blue\": 0,  # B02\n            \"green\": 1,  # B03\n            \"red\": 2,  # B04\n            \"rededge1\": 3,  # B05\n            \"rededge2\": 4,  # B06\n            \"rededge3\": 5,  # B07\n            \"nir\": 6,  # B08\n            \"rededge4\": 7,  # B8A\n            \"swir16\": 8,  # B11\n            \"swir22\": 9,  # B12\n        }\n\n    import numpy as np  # noqa: PLC0415\n    import torch as _torch  # noqa: PLC0415\n\n    raw_data = tensor.data\n    if isinstance(raw_data, _torch.Tensor):\n        data = raw_data\n    else:\n        data = _torch.from_numpy(np.asarray(raw_data))\n\n    if index_name.lower() == \"ndvi\":\n        result = ndvi(\n            data[..., band_mapping[\"nir\"], :, :],\n            data[..., band_mapping[\"red\"], :, :],\n        )\n    elif index_name.lower() == \"ndwi\":\n        result = ndwi(\n            data[..., band_mapping[\"green\"], :, :],\n            data[..., band_mapping[\"nir\"], :, :],\n        )\n    elif index_name.lower() == \"evi\":\n        result = evi(\n            data[..., band_mapping[\"nir\"], :, :],\n            data[..., band_mapping[\"red\"], :, :],\n            data[..., band_mapping[\"blue\"], :, :],\n        )\n    else:\n        raise ValueError(f\"Unknown index: {index_name}\")\n\n    # Add channel dimension\n    result = result.unsqueeze(-3)\n\n    return GeoTensor(\n        data=result,\n        crs=tensor.crs,\n        transform=tensor.transform,\n        band_names=[index_name.upper()],\n        nodata=tensor.nodata,\n    )\n</code></pre>"},{"location":"api/preprocessing/#ununennium.preprocessing.indices.evi","title":"<code>evi(nir, red, blue, g=2.5, c1=6.0, c2=7.5, L_factor=1.0, epsilon=1e-08)</code>","text":"<p>Calculate Enhanced Vegetation Index.</p> <p>EVI = G * (NIR - Red) / (NIR + C1 * Red - C2 * Blue + L)</p> <p>Parameters:</p> Name Type Description Default <code>nir</code> <code>Tensor</code> <p>Near-infrared band.</p> required <code>red</code> <code>Tensor</code> <p>Red band.</p> required <code>blue</code> <code>Tensor</code> <p>Blue band.</p> required <code>g</code> <code>float</code> <p>Gain factor.</p> <code>2.5</code> <code>c1</code> <code>float</code> <p>Red correction coefficient.</p> <code>6.0</code> <code>c2</code> <code>float</code> <p>Blue correction coefficient.</p> <code>7.5</code> <code>L_factor</code> <code>float</code> <p>Canopy background adjustment.</p> <code>1.0</code> <code>epsilon</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>EVI values.</p> Source code in <code>src/ununennium/preprocessing/indices.py</code> <pre><code>def evi(\n    nir: torch.Tensor,\n    red: torch.Tensor,\n    blue: torch.Tensor,\n    g: float = 2.5,\n    c1: float = 6.0,\n    c2: float = 7.5,\n    L_factor: float = 1.0,\n    epsilon: float = 1e-8,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate Enhanced Vegetation Index.\n\n    EVI = G * (NIR - Red) / (NIR + C1 * Red - C2 * Blue + L)\n\n    Args:\n        nir: Near-infrared band.\n        red: Red band.\n        blue: Blue band.\n        g: Gain factor.\n        c1: Red correction coefficient.\n        c2: Blue correction coefficient.\n        L_factor: Canopy background adjustment.\n        epsilon: Small value to avoid division by zero.\n\n    Returns:\n        EVI values.\n    \"\"\"\n    denominator = nir + c1 * red - c2 * blue + L_factor + epsilon\n    return g * (nir - red) / denominator\n</code></pre>"},{"location":"api/preprocessing/#ununennium.preprocessing.indices.nbr","title":"<code>nbr(nir, swir, epsilon=1e-08)</code>","text":"<p>Calculate Normalized Burn Ratio.</p> <p>NBR = (NIR - SWIR) / (NIR + SWIR)</p> <p>Parameters:</p> Name Type Description Default <code>nir</code> <code>Tensor</code> <p>Near-infrared band.</p> required <code>swir</code> <code>Tensor</code> <p>Short-wave infrared band.</p> required <code>epsilon</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>NBR values in range [-1, 1].</p> Source code in <code>src/ununennium/preprocessing/indices.py</code> <pre><code>def nbr(\n    nir: torch.Tensor,\n    swir: torch.Tensor,\n    epsilon: float = 1e-8,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate Normalized Burn Ratio.\n\n    NBR = (NIR - SWIR) / (NIR + SWIR)\n\n    Args:\n        nir: Near-infrared band.\n        swir: Short-wave infrared band.\n        epsilon: Small value to avoid division by zero.\n\n    Returns:\n        NBR values in range [-1, 1].\n    \"\"\"\n    return (nir - swir) / (nir + swir + epsilon)\n</code></pre>"},{"location":"api/preprocessing/#ununennium.preprocessing.indices.ndvi","title":"<code>ndvi(nir, red, epsilon=1e-08)</code>","text":"<p>Calculate Normalized Difference Vegetation Index.</p> <p>NDVI = (NIR - Red) / (NIR + Red)</p> <p>Parameters:</p> Name Type Description Default <code>nir</code> <code>Tensor</code> <p>Near-infrared band.</p> required <code>red</code> <code>Tensor</code> <p>Red band.</p> required <code>epsilon</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>NDVI values in range [-1, 1].</p> Source code in <code>src/ununennium/preprocessing/indices.py</code> <pre><code>def ndvi(\n    nir: torch.Tensor,\n    red: torch.Tensor,\n    epsilon: float = 1e-8,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate Normalized Difference Vegetation Index.\n\n    NDVI = (NIR - Red) / (NIR + Red)\n\n    Args:\n        nir: Near-infrared band.\n        red: Red band.\n        epsilon: Small value to avoid division by zero.\n\n    Returns:\n        NDVI values in range [-1, 1].\n    \"\"\"\n    return (nir - red) / (nir + red + epsilon)\n</code></pre>"},{"location":"api/preprocessing/#ununennium.preprocessing.indices.ndwi","title":"<code>ndwi(green, nir, epsilon=1e-08)</code>","text":"<p>Calculate Normalized Difference Water Index.</p> <p>NDWI = (Green - NIR) / (Green + NIR)</p> <p>Parameters:</p> Name Type Description Default <code>green</code> <code>Tensor</code> <p>Green band.</p> required <code>nir</code> <code>Tensor</code> <p>Near-infrared band.</p> required <code>epsilon</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>NDWI values in range [-1, 1].</p> Source code in <code>src/ununennium/preprocessing/indices.py</code> <pre><code>def ndwi(\n    green: torch.Tensor,\n    nir: torch.Tensor,\n    epsilon: float = 1e-8,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate Normalized Difference Water Index.\n\n    NDWI = (Green - NIR) / (Green + NIR)\n\n    Args:\n        green: Green band.\n        nir: Near-infrared band.\n        epsilon: Small value to avoid division by zero.\n\n    Returns:\n        NDWI values in range [-1, 1].\n    \"\"\"\n    return (green - nir) / (green + nir + epsilon)\n</code></pre>"},{"location":"api/preprocessing/#ununennium.preprocessing.indices.savi","title":"<code>savi(nir, red, L_factor=0.5, epsilon=1e-08)</code>","text":"<p>Calculate Soil-Adjusted Vegetation Index.</p> <p>SAVI = ((NIR - Red) / (NIR + Red + L)) * (1 + L)</p> <p>Parameters:</p> Name Type Description Default <code>nir</code> <code>Tensor</code> <p>Near-infrared band.</p> required <code>red</code> <code>Tensor</code> <p>Red band.</p> required <code>L_factor</code> <code>float</code> <p>Soil brightness correction factor.</p> <code>0.5</code> <code>epsilon</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>SAVI values.</p> Source code in <code>src/ununennium/preprocessing/indices.py</code> <pre><code>def savi(\n    nir: torch.Tensor,\n    red: torch.Tensor,\n    L_factor: float = 0.5,\n    epsilon: float = 1e-8,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate Soil-Adjusted Vegetation Index.\n\n    SAVI = ((NIR - Red) / (NIR + Red + L)) * (1 + L)\n\n    Args:\n        nir: Near-infrared band.\n        red: Red band.\n        L_factor: Soil brightness correction factor.\n        epsilon: Small value to avoid division by zero.\n\n    Returns:\n        SAVI values.\n    \"\"\"\n    return ((nir - red) / (nir + red + L_factor + epsilon)) * (1 + L_factor)\n</code></pre>"},{"location":"api/tiling/","title":"API Reference: Tiling","text":""},{"location":"api/tiling/#ununennium.tiling.tiler","title":"<code>ununennium.tiling.tiler</code>","text":"<p>Image tiling utilities for processing large rasters.</p>"},{"location":"api/tiling/#ununennium.tiling.tiler.Tiler","title":"<code>Tiler</code>","text":"<p>Tile large images into smaller patches with optional overlap.</p> <p>Handles the common pattern of splitting large satellite images into patches for processing, then reassembling results.</p> Example <p>tiler = Tiler(tile_size=256, overlap=32) for tile, window in tiler.tile(large_image): ...     result = model(tile) ...     tiler.add_result(result, window) output = tiler.merge()</p> Source code in <code>src/ununennium/tiling/tiler.py</code> <pre><code>class Tiler:\n    \"\"\"Tile large images into smaller patches with optional overlap.\n\n    Handles the common pattern of splitting large satellite images\n    into patches for processing, then reassembling results.\n\n    Example:\n        &gt;&gt;&gt; tiler = Tiler(tile_size=256, overlap=32)\n        &gt;&gt;&gt; for tile, window in tiler.tile(large_image):\n        ...     result = model(tile)\n        ...     tiler.add_result(result, window)\n        &gt;&gt;&gt; output = tiler.merge()\n    \"\"\"\n\n    def __init__(\n        self,\n        tile_size: int | tuple[int, int] = 256,\n        overlap: int = 0,\n        padding_mode: str = \"reflect\",\n    ):\n        \"\"\"Initialize tiler.\n\n        Args:\n            tile_size: Size of tiles (H, W) or single value for square.\n            overlap: Overlap between adjacent tiles.\n            padding_mode: Padding mode for edge tiles.\n        \"\"\"\n        if isinstance(tile_size, int):\n            self.tile_size = (tile_size, tile_size)\n        else:\n            self.tile_size = tile_size\n\n        self.overlap = overlap\n        self.padding_mode = padding_mode\n        self._results: list[tuple[torch.Tensor, tuple[int, int, int, int]]] = []\n        self._image_shape: tuple[int, ...] | None = None\n\n    def tile(\n        self,\n        image: torch.Tensor | GeoTensor,\n    ) -&gt; Iterator[tuple[torch.Tensor, tuple[int, int, int, int]]]:\n        \"\"\"Generate tiles from an image.\n\n        Args:\n            image: Input image tensor (C, H, W) or (B, C, H, W).\n\n        Yields:\n            Tuples of (tile, window) where window is (y, x, h, w).\n        \"\"\"\n        data: torch.Tensor\n        if isinstance(image, GeoTensor):\n            if isinstance(image.data, torch.Tensor):\n                data = image.data\n            else:\n                import numpy as np  # noqa: PLC0415\n\n                data = torch.from_numpy(np.asarray(image.data))\n        else:\n            data = image\n\n        # Handle batch dimension\n        if data.ndim == 4:\n            data = data[0]  # Process one image at a time\n\n        _, h, w = data.shape\n        self._image_shape = tuple(data.shape)\n        self._results = []\n\n        th, tw = self.tile_size\n        step_h = th - self.overlap\n        step_w = tw - self.overlap\n\n        for y in range(0, h, step_h):\n            for x in range(0, w, step_w):\n                # Calculate window\n                y_end = min(y + th, h)\n                x_end = min(x + tw, w)\n\n                # Extract tile\n                tile = data[:, y:y_end, x:x_end]\n\n                # Pad if necessary\n                if tile.shape[1] &lt; th or tile.shape[2] &lt; tw:\n                    pad_h = th - tile.shape[1]\n                    pad_w = tw - tile.shape[2]\n                    tile = torch.nn.functional.pad(\n                        tile,\n                        (0, pad_w, 0, pad_h),\n                        mode=self.padding_mode,  # type: ignore[arg-type]\n                    )\n\n                yield tile, (y, x, y_end - y, x_end - x)\n\n    def add_result(\n        self,\n        result: torch.Tensor,\n        window: tuple[int, int, int, int],\n    ) -&gt; None:\n        \"\"\"Add a processed tile result.\n\n        Args:\n            result: Processed tile.\n            window: Original window (y, x, h, w).\n        \"\"\"\n        self._results.append((result, window))\n\n    def merge(self) -&gt; torch.Tensor:\n        \"\"\"Merge all tile results into a single image.\n\n        Returns:\n            Merged output tensor.\n        \"\"\"\n        if self._image_shape is None:\n            raise ValueError(\"Must call tile() before merge()\")\n\n        # Determine output shape from first result\n        first_result = self._results[0][0]\n        out_channels = first_result.shape[0]\n        _, h, w = self._image_shape\n\n        # Initialize output and weight tensors\n        output = torch.zeros(out_channels, h, w, device=first_result.device)\n        weights = torch.zeros(1, h, w, device=first_result.device)\n\n        for result, (y, x, rh, rw) in self._results:\n            # Only use valid (non-padded) region\n            valid_result = result[:, :rh, :rw]\n\n            # Average overlapping regions\n            output[:, y : y + rh, x : x + rw] += valid_result\n            weights[:, y : y + rh, x : x + rw] += 1\n\n        # Normalize by weights\n        output = output / weights.clamp(min=1)\n\n        return output\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.tiler.Tiler.__init__","title":"<code>__init__(tile_size=256, overlap=0, padding_mode='reflect')</code>","text":"<p>Initialize tiler.</p> <p>Parameters:</p> Name Type Description Default <code>tile_size</code> <code>int | tuple[int, int]</code> <p>Size of tiles (H, W) or single value for square.</p> <code>256</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent tiles.</p> <code>0</code> <code>padding_mode</code> <code>str</code> <p>Padding mode for edge tiles.</p> <code>'reflect'</code> Source code in <code>src/ununennium/tiling/tiler.py</code> <pre><code>def __init__(\n    self,\n    tile_size: int | tuple[int, int] = 256,\n    overlap: int = 0,\n    padding_mode: str = \"reflect\",\n):\n    \"\"\"Initialize tiler.\n\n    Args:\n        tile_size: Size of tiles (H, W) or single value for square.\n        overlap: Overlap between adjacent tiles.\n        padding_mode: Padding mode for edge tiles.\n    \"\"\"\n    if isinstance(tile_size, int):\n        self.tile_size = (tile_size, tile_size)\n    else:\n        self.tile_size = tile_size\n\n    self.overlap = overlap\n    self.padding_mode = padding_mode\n    self._results: list[tuple[torch.Tensor, tuple[int, int, int, int]]] = []\n    self._image_shape: tuple[int, ...] | None = None\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.tiler.Tiler.add_result","title":"<code>add_result(result, window)</code>","text":"<p>Add a processed tile result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Tensor</code> <p>Processed tile.</p> required <code>window</code> <code>tuple[int, int, int, int]</code> <p>Original window (y, x, h, w).</p> required Source code in <code>src/ununennium/tiling/tiler.py</code> <pre><code>def add_result(\n    self,\n    result: torch.Tensor,\n    window: tuple[int, int, int, int],\n) -&gt; None:\n    \"\"\"Add a processed tile result.\n\n    Args:\n        result: Processed tile.\n        window: Original window (y, x, h, w).\n    \"\"\"\n    self._results.append((result, window))\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.tiler.Tiler.merge","title":"<code>merge()</code>","text":"<p>Merge all tile results into a single image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Merged output tensor.</p> Source code in <code>src/ununennium/tiling/tiler.py</code> <pre><code>def merge(self) -&gt; torch.Tensor:\n    \"\"\"Merge all tile results into a single image.\n\n    Returns:\n        Merged output tensor.\n    \"\"\"\n    if self._image_shape is None:\n        raise ValueError(\"Must call tile() before merge()\")\n\n    # Determine output shape from first result\n    first_result = self._results[0][0]\n    out_channels = first_result.shape[0]\n    _, h, w = self._image_shape\n\n    # Initialize output and weight tensors\n    output = torch.zeros(out_channels, h, w, device=first_result.device)\n    weights = torch.zeros(1, h, w, device=first_result.device)\n\n    for result, (y, x, rh, rw) in self._results:\n        # Only use valid (non-padded) region\n        valid_result = result[:, :rh, :rw]\n\n        # Average overlapping regions\n        output[:, y : y + rh, x : x + rw] += valid_result\n        weights[:, y : y + rh, x : x + rw] += 1\n\n    # Normalize by weights\n    output = output / weights.clamp(min=1)\n\n    return output\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.tiler.Tiler.tile","title":"<code>tile(image)</code>","text":"<p>Generate tiles from an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor | GeoTensor</code> <p>Input image tensor (C, H, W) or (B, C, H, W).</p> required <p>Yields:</p> Type Description <code>tuple[Tensor, tuple[int, int, int, int]]</code> <p>Tuples of (tile, window) where window is (y, x, h, w).</p> Source code in <code>src/ununennium/tiling/tiler.py</code> <pre><code>def tile(\n    self,\n    image: torch.Tensor | GeoTensor,\n) -&gt; Iterator[tuple[torch.Tensor, tuple[int, int, int, int]]]:\n    \"\"\"Generate tiles from an image.\n\n    Args:\n        image: Input image tensor (C, H, W) or (B, C, H, W).\n\n    Yields:\n        Tuples of (tile, window) where window is (y, x, h, w).\n    \"\"\"\n    data: torch.Tensor\n    if isinstance(image, GeoTensor):\n        if isinstance(image.data, torch.Tensor):\n            data = image.data\n        else:\n            import numpy as np  # noqa: PLC0415\n\n            data = torch.from_numpy(np.asarray(image.data))\n    else:\n        data = image\n\n    # Handle batch dimension\n    if data.ndim == 4:\n        data = data[0]  # Process one image at a time\n\n    _, h, w = data.shape\n    self._image_shape = tuple(data.shape)\n    self._results = []\n\n    th, tw = self.tile_size\n    step_h = th - self.overlap\n    step_w = tw - self.overlap\n\n    for y in range(0, h, step_h):\n        for x in range(0, w, step_w):\n            # Calculate window\n            y_end = min(y + th, h)\n            x_end = min(x + tw, w)\n\n            # Extract tile\n            tile = data[:, y:y_end, x:x_end]\n\n            # Pad if necessary\n            if tile.shape[1] &lt; th or tile.shape[2] &lt; tw:\n                pad_h = th - tile.shape[1]\n                pad_w = tw - tile.shape[2]\n                tile = torch.nn.functional.pad(\n                    tile,\n                    (0, pad_w, 0, pad_h),\n                    mode=self.padding_mode,  # type: ignore[arg-type]\n                )\n\n            yield tile, (y, x, y_end - y, x_end - x)\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.tiler.tile_image","title":"<code>tile_image(image, tile_size=256, overlap=0)</code>","text":"<p>Split an image into tiles.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image (C, H, W).</p> required <code>tile_size</code> <code>int | tuple[int, int]</code> <p>Size of tiles.</p> <code>256</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>List of tile tensors.</p> Source code in <code>src/ununennium/tiling/tiler.py</code> <pre><code>def tile_image(\n    image: torch.Tensor,\n    tile_size: int | tuple[int, int] = 256,\n    overlap: int = 0,\n) -&gt; list[torch.Tensor]:\n    \"\"\"Split an image into tiles.\n\n    Args:\n        image: Input image (C, H, W).\n        tile_size: Size of tiles.\n        overlap: Overlap between tiles.\n\n    Returns:\n        List of tile tensors.\n    \"\"\"\n    tiler = Tiler(tile_size=tile_size, overlap=overlap)\n    return [tile for tile, _ in tiler.tile(image)]\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.tiler.untile_image","title":"<code>untile_image(tiles, original_shape, tile_size=256, overlap=0)</code>","text":"<p>Reassemble tiles into a single image.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list[Tensor]</code> <p>List of tile tensors.</p> required <code>original_shape</code> <code>tuple[int, int, int]</code> <p>Original image shape (C, H, W).</p> required <code>tile_size</code> <code>int | tuple[int, int]</code> <p>Size of tiles.</p> <code>256</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Merged image tensor.</p> Source code in <code>src/ununennium/tiling/tiler.py</code> <pre><code>def untile_image(\n    tiles: list[torch.Tensor],\n    original_shape: tuple[int, int, int],\n    tile_size: int | tuple[int, int] = 256,\n    overlap: int = 0,\n) -&gt; torch.Tensor:\n    \"\"\"Reassemble tiles into a single image.\n\n    Args:\n        tiles: List of tile tensors.\n        original_shape: Original image shape (C, H, W).\n        tile_size: Size of tiles.\n        overlap: Overlap between tiles.\n\n    Returns:\n        Merged image tensor.\n    \"\"\"\n    tiler = Tiler(tile_size=tile_size, overlap=overlap)\n\n    # Generate windows\n    dummy = torch.zeros(original_shape)\n    windows = [window for _, window in tiler.tile(dummy)]\n\n    # Add results\n    for tile, window in zip(tiles, windows, strict=False):\n        tiler.add_result(tile, window)\n\n    return tiler.merge()\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler","title":"<code>ununennium.tiling.sampler</code>","text":"<p>Spatial sampling strategies for training.</p>"},{"location":"api/tiling/#ununennium.tiling.sampler.BalancedSampler","title":"<code>BalancedSampler</code>","text":"<p>               Bases: <code>Sampler</code></p> <p>Class-balanced spatial sampling.</p> <p>Samples locations to balance class distribution in batches.</p> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>class BalancedSampler(Sampler):\n    \"\"\"Class-balanced spatial sampling.\n\n    Samples locations to balance class distribution in batches.\n    \"\"\"\n\n    def __init__(\n        self,\n        class_locations: dict[int, list[tuple[float, float]]],\n        sample_size: tuple[float, float],\n        num_samples: int,\n        seed: int | None = None,\n    ):\n        \"\"\"Initialize balanced sampler.\n\n        Args:\n            class_locations: Mapping from class ID to list of (x, y) centers.\n            sample_size: Size of each sample.\n            num_samples: Total number of samples.\n            seed: Random seed.\n        \"\"\"\n        self.class_locations = class_locations\n        self.sample_size = sample_size\n        self.num_samples = num_samples\n        self.classes = list(class_locations.keys())\n\n        if seed is not None:\n            torch.manual_seed(seed)\n\n    def __iter__(self) -&gt; Iterator[BoundingBox]:\n        for i in range(self.num_samples):\n            # Round-robin through classes\n            cls = self.classes[i % len(self.classes)]\n            locations = self.class_locations[cls]\n\n            # Random location for this class\n            idx = int(torch.randint(len(locations), (1,)).item())\n            x, y = locations[idx]\n\n            # Add jitter\n            x += (torch.rand(1).item() - 0.5) * self.sample_size[0] * 0.5\n            y += (torch.rand(1).item() - 0.5) * self.sample_size[1] * 0.5\n\n            yield BoundingBox(\n                minx=x - self.sample_size[0] / 2,\n                miny=y - self.sample_size[1] / 2,\n                maxx=x + self.sample_size[0] / 2,\n                maxy=y + self.sample_size[1] / 2,\n            )\n\n    def __len__(self) -&gt; int:\n        return self.num_samples\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler.BalancedSampler.__init__","title":"<code>__init__(class_locations, sample_size, num_samples, seed=None)</code>","text":"<p>Initialize balanced sampler.</p> <p>Parameters:</p> Name Type Description Default <code>class_locations</code> <code>dict[int, list[tuple[float, float]]]</code> <p>Mapping from class ID to list of (x, y) centers.</p> required <code>sample_size</code> <code>tuple[float, float]</code> <p>Size of each sample.</p> required <code>num_samples</code> <code>int</code> <p>Total number of samples.</p> required <code>seed</code> <code>int | None</code> <p>Random seed.</p> <code>None</code> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>def __init__(\n    self,\n    class_locations: dict[int, list[tuple[float, float]]],\n    sample_size: tuple[float, float],\n    num_samples: int,\n    seed: int | None = None,\n):\n    \"\"\"Initialize balanced sampler.\n\n    Args:\n        class_locations: Mapping from class ID to list of (x, y) centers.\n        sample_size: Size of each sample.\n        num_samples: Total number of samples.\n        seed: Random seed.\n    \"\"\"\n    self.class_locations = class_locations\n    self.sample_size = sample_size\n    self.num_samples = num_samples\n    self.classes = list(class_locations.keys())\n\n    if seed is not None:\n        torch.manual_seed(seed)\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler.GridSampler","title":"<code>GridSampler</code>","text":"<p>               Bases: <code>Sampler</code></p> <p>Regular grid sampling.</p> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>class GridSampler(Sampler):\n    \"\"\"Regular grid sampling.\"\"\"\n\n    def __init__(\n        self,\n        bounds: BoundingBox,\n        sample_size: tuple[float, float],\n        stride: tuple[float, float] | None = None,\n    ):\n        \"\"\"Initialize grid sampler.\n\n        Args:\n            bounds: Bounding box to sample within.\n            sample_size: Size of each sample (width, height).\n            stride: Step size between samples. Defaults to sample_size.\n        \"\"\"\n        self.bounds = bounds\n        self.sample_size = sample_size\n        self.stride = stride or sample_size\n\n        # Calculate grid dimensions\n        self.nx = int((bounds.width - sample_size[0]) / self.stride[0]) + 1\n        self.ny = int((bounds.height - sample_size[1]) / self.stride[1]) + 1\n\n    def __iter__(self) -&gt; Iterator[BoundingBox]:\n        for iy in range(self.ny):\n            for ix in range(self.nx):\n                x = self.bounds.minx + ix * self.stride[0]\n                y = self.bounds.miny + iy * self.stride[1]\n\n                yield BoundingBox(\n                    minx=x,\n                    miny=y,\n                    maxx=x + self.sample_size[0],\n                    maxy=y + self.sample_size[1],\n                )\n\n    def __len__(self) -&gt; int:\n        return self.nx * self.ny\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler.GridSampler.__init__","title":"<code>__init__(bounds, sample_size, stride=None)</code>","text":"<p>Initialize grid sampler.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>BoundingBox</code> <p>Bounding box to sample within.</p> required <code>sample_size</code> <code>tuple[float, float]</code> <p>Size of each sample (width, height).</p> required <code>stride</code> <code>tuple[float, float] | None</code> <p>Step size between samples. Defaults to sample_size.</p> <code>None</code> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>def __init__(\n    self,\n    bounds: BoundingBox,\n    sample_size: tuple[float, float],\n    stride: tuple[float, float] | None = None,\n):\n    \"\"\"Initialize grid sampler.\n\n    Args:\n        bounds: Bounding box to sample within.\n        sample_size: Size of each sample (width, height).\n        stride: Step size between samples. Defaults to sample_size.\n    \"\"\"\n    self.bounds = bounds\n    self.sample_size = sample_size\n    self.stride = stride or sample_size\n\n    # Calculate grid dimensions\n    self.nx = int((bounds.width - sample_size[0]) / self.stride[0]) + 1\n    self.ny = int((bounds.height - sample_size[1]) / self.stride[1]) + 1\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler.RandomSampler","title":"<code>RandomSampler</code>","text":"<p>               Bases: <code>Sampler</code></p> <p>Random spatial sampling within bounds.</p> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>class RandomSampler(Sampler):\n    \"\"\"Random spatial sampling within bounds.\"\"\"\n\n    def __init__(\n        self,\n        bounds: BoundingBox,\n        sample_size: tuple[float, float],\n        num_samples: int,\n        seed: int | None = None,\n    ):\n        \"\"\"Initialize random sampler.\n\n        Args:\n            bounds: Bounding box to sample within.\n            sample_size: Size of each sample (width, height) in CRS units.\n            num_samples: Number of samples to generate.\n            seed: Random seed for reproducibility.\n        \"\"\"\n        self.bounds = bounds\n        self.sample_size = sample_size\n        self.num_samples = num_samples\n\n        if seed is not None:\n            torch.manual_seed(seed)\n\n    def __iter__(self) -&gt; Iterator[BoundingBox]:\n        for _ in range(self.num_samples):\n            # Random center point\n            x = torch.rand(1).item() * (self.bounds.width - self.sample_size[0]) + self.bounds.minx\n            y = torch.rand(1).item() * (self.bounds.height - self.sample_size[1]) + self.bounds.miny\n\n            yield BoundingBox(\n                minx=x,\n                miny=y,\n                maxx=x + self.sample_size[0],\n                maxy=y + self.sample_size[1],\n            )\n\n    def __len__(self) -&gt; int:\n        return self.num_samples\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler.RandomSampler.__init__","title":"<code>__init__(bounds, sample_size, num_samples, seed=None)</code>","text":"<p>Initialize random sampler.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>BoundingBox</code> <p>Bounding box to sample within.</p> required <code>sample_size</code> <code>tuple[float, float]</code> <p>Size of each sample (width, height) in CRS units.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to generate.</p> required <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility.</p> <code>None</code> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>def __init__(\n    self,\n    bounds: BoundingBox,\n    sample_size: tuple[float, float],\n    num_samples: int,\n    seed: int | None = None,\n):\n    \"\"\"Initialize random sampler.\n\n    Args:\n        bounds: Bounding box to sample within.\n        sample_size: Size of each sample (width, height) in CRS units.\n        num_samples: Number of samples to generate.\n        seed: Random seed for reproducibility.\n    \"\"\"\n    self.bounds = bounds\n    self.sample_size = sample_size\n    self.num_samples = num_samples\n\n    if seed is not None:\n        torch.manual_seed(seed)\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler.Sampler","title":"<code>Sampler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for spatial samplers.</p> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>class Sampler(ABC):\n    \"\"\"Abstract base class for spatial samplers.\"\"\"\n\n    @abstractmethod\n    def __iter__(self) -&gt; Iterator[BoundingBox | tuple[int, int]]:\n        \"\"\"Iterate over sample locations.\"\"\"\n        ...\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Number of samples.\"\"\"\n        ...\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler.Sampler.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterate over sample locations.</p> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>@abstractmethod\ndef __iter__(self) -&gt; Iterator[BoundingBox | tuple[int, int]]:\n    \"\"\"Iterate over sample locations.\"\"\"\n    ...\n</code></pre>"},{"location":"api/tiling/#ununennium.tiling.sampler.Sampler.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Number of samples.</p> Source code in <code>src/ununennium/tiling/sampler.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Number of samples.\"\"\"\n    ...\n</code></pre>"},{"location":"api/training/","title":"Training Module API Reference","text":"<p>The <code>ununennium.training</code> module implements a highly customizable, PyTorch-native training loop geared towards Earth Observation tasks. It supports mixed precision, gradient accumulation, and callbacks.</p>"},{"location":"api/training/#1-trainer","title":"1. Trainer","text":"<p><code>ununennium.training.trainer.Trainer</code></p> <p>The orchestrator of the training process.</p>"},{"location":"api/training/#signature","title":"Signature","text":"<pre><code>class Trainer:\n    def __init__(\n        self,\n        model: nn.Module,\n        optimizer: torch.optim.Optimizer,\n        loss_fn: Callable[[Tensor, Tensor], Tensor],\n        train_loader: DataLoader,\n        val_loader: DataLoader | None = None,\n        scheduler: LRScheduler | None = None,\n        callbacks: list[Callback] | None = None,\n        config: TrainerConfig | None = None\n    )\n</code></pre>"},{"location":"api/training/#configuration-trainerconfig","title":"Configuration (<code>TrainerConfig</code>)","text":"Field Default Description <code>epochs</code> <code>100</code> Max training epochs. <code>device</code> <code>'cuda'</code> Computation device. <code>mixed_precision</code> <code>True</code> Enable FP16 (AMP) for 2x speedup. <code>accumulation_steps</code> <code>1</code> Gradient accumulation for large batch simulation. <code>gradient_clip</code> <code>1.0</code> Max norm for gradient clipping."},{"location":"api/training/#methods","title":"Methods","text":""},{"location":"api/training/#fit","title":"<code>fit</code>","text":"<p>Starts the training loop.</p> <pre><code>def fit(self, epochs: int | None = None) -&gt; dict[str, list[float]]\n</code></pre> <p>Returns: A dictionary containing the history of all logged metrics (e.g., <code>train_loss</code>, <code>val_IoU</code>).</p>"},{"location":"api/training/#2-callbacks","title":"2. Callbacks","text":"<p>Callbacks allow hooking into the training lifecycle. Base class: <code>ununennium.training.callbacks.Callback</code>.</p>"},{"location":"api/training/#built-in-callbacks","title":"Built-in Callbacks","text":""},{"location":"api/training/#checkpointcallback","title":"<code>CheckpointCallback</code>","text":"<p>Saves model weights based on metric monitoring.</p> <pre><code>class CheckpointCallback(Callback):\n    def __init__(self, \n                 dirpath: str, \n                 monitor: str = \"val_loss\", \n                 mode: str = \"min\", \n                 save_top_k: int = 1)\n</code></pre> <ul> <li>save_top_k: Keep only the best \\(K\\) models.</li> <li>mode: <code>min</code> for loss, <code>max</code> for accuracy/IoU.</li> </ul>"},{"location":"api/training/#earlystopping","title":"<code>EarlyStopping</code>","text":"<p>Halts training if the monitored metric stops improving.</p> <pre><code>class EarlyStopping(Callback):\n    def __init__(self, \n                 monitor: str = \"val_loss\", \n                 patience: int = 10, \n                 min_delta: float = 0.0)\n</code></pre>"},{"location":"api/training/#3-distributed-training-ddp","title":"3. Distributed Training (DDP)","text":"<p>The <code>Trainer</code> is compatible with <code>torch.distributed</code>.</p> <p>Usage Pattern:</p> <pre><code># Launch with torchrun\n# torchrun --nproc_per_node=4 train.py\n\ndef main():\n    dist.init_process_group(backend=\"nccl\")\n    model = DDP(model.to(rank), device_ids=[rank])\n    # Trainer handles the rest implicitly via device placement\n</code></pre> <p>[!NOTE] Ensure your <code>DataLoader</code> uses <code>DistributedSampler</code> when running in DDP mode to prevent workers from seeing the same data.</p>"},{"location":"api/visualization/","title":"Visualization API","text":""},{"location":"api/visualization/#ununennium.utils.visualization","title":"<code>ununennium.utils.visualization</code>","text":"<p>Visualization utilities for geospatial data.</p>"},{"location":"api/visualization/#ununennium.utils.visualization.plot_bands","title":"<code>plot_bands(tensor, bands=None, cols=4, cmap='viridis')</code>","text":"<p>Plot individual bands in a grid.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>GeoTensor</code> <p>Input GeoTensor.</p> required <code>bands</code> <code>list[str] | None</code> <p>List of bands to plot.</p> <code>None</code> <code>cols</code> <code>int</code> <p>Number of columns.</p> <code>4</code> <code>cmap</code> <code>str</code> <p>Colormap.</p> <code>'viridis'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure.</p> Source code in <code>src/ununennium/utils/visualization.py</code> <pre><code>def plot_bands(\n    tensor: GeoTensor,\n    bands: list[str] | None = None,\n    cols: int = 4,\n    cmap: str = \"viridis\",\n) -&gt; Figure:\n    \"\"\"Plot individual bands in a grid.\n\n    Args:\n        tensor: Input GeoTensor.\n        bands: List of bands to plot.\n        cols: Number of columns.\n        cmap: Colormap.\n\n    Returns:\n        Matplotlib figure.\n    \"\"\"\n    if bands is None:\n        if tensor.band_names is None:\n            raise ValueError(\"bands must be specified if tensor.band_names is None\")\n        bands = tensor.band_names\n\n    if tensor.band_names is None:\n        raise ValueError(\"tensor.band_names is required for band plotting\")\n\n    n = len(bands)\n    rows = math.ceil(n / cols)\n\n    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n    axes_flat = axes.flatten()\n\n    for i, band in enumerate(bands):\n        idx = tensor.band_names.index(band)\n        data_tensor = tensor.data[idx]\n        if isinstance(data_tensor, torch.Tensor):\n            data_np = data_tensor.float().cpu().numpy()\n        else:\n            data_np = data_tensor.astype(float)  # type: ignore[union-attr]\n\n        ax = axes_flat[i]\n        im = ax.imshow(data_np, cmap=cmap)\n        ax.set_title(band)\n        ax.axis(\"off\")\n        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n    # Hide empty axes\n    for i in range(n, len(axes_flat)):\n        axes_flat[i].axis(\"off\")\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/visualization/#ununennium.utils.visualization.plot_rgb","title":"<code>plot_rgb(tensor, sensor=None, bands=None, brightness=1.0, ax=None)</code>","text":"<p>Plot an RGB composite from a GeoTensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>GeoTensor</code> <p>Input GeoTensor.</p> required <code>sensor</code> <code>str | None</code> <p>Sensor name to automatically select RGB bands.</p> <code>None</code> <code>bands</code> <code>tuple[str, str, str] | None</code> <p>Explicit list of 3 band names for RGB.</p> <code>None</code> <code>brightness</code> <code>float</code> <p>Brightness factor.</p> <code>1.0</code> <code>ax</code> <code>Axes | None</code> <p>Matplotlib axes.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Matplotlib axes.</p> Source code in <code>src/ununennium/utils/visualization.py</code> <pre><code>def plot_rgb(\n    tensor: GeoTensor,\n    sensor: str | None = None,\n    bands: tuple[str, str, str] | None = None,\n    brightness: float = 1.0,\n    ax: plt.Axes | None = None,\n) -&gt; plt.Axes:\n    \"\"\"Plot an RGB composite from a GeoTensor.\n\n    Args:\n        tensor: Input GeoTensor.\n        sensor: Sensor name to automatically select RGB bands.\n        bands: Explicit list of 3 band names for RGB.\n        brightness: Brightness factor.\n        ax: Matplotlib axes.\n\n    Returns:\n        Matplotlib axes.\n    \"\"\"\n    if ax is None:\n        _fig, ax = plt.subplots(figsize=(10, 10))\n\n    if bands is None:\n        if sensor:\n            bands = get_rgb_bands(sensor)\n        elif tensor.band_names and len(tensor.band_names) &gt;= 3:\n            bands = (tensor.band_names[0], tensor.band_names[1], tensor.band_names[2])\n\n    if bands is None:\n        raise ValueError(\"Must provide 3 bands for RGB plotting.\")\n    if tensor.band_names is None:\n        raise ValueError(\"tensor.band_names is required for RGB plotting.\")\n\n    # Select indices\n    try:\n        indices = [tensor.band_names.index(b) for b in bands]\n    except ValueError as e:\n        raise ValueError(f\"Band not found in tensor: {e}\") from e\n\n    data = tensor.data\n    if isinstance(data, torch.Tensor):\n        rgb = data[indices, :, :].float()\n    else:\n        import numpy as np  # noqa: PLC0415\n\n        rgb = torch.from_numpy(np.array(data)[indices, :, :]).float()\n\n    # Normalize\n    p2 = torch.quantile(rgb, 0.02)\n    p98 = torch.quantile(rgb, 0.98)\n    rgb = (rgb - p2) / (p98 - p2)\n    rgb = torch.clamp(rgb * brightness, 0, 1)\n\n    # To channel last numpy\n    rgb_np = rgb.permute(1, 2, 0).cpu().numpy()\n\n    ax.imshow(rgb_np)  # type: ignore[union-attr]\n    ax.set_title(f\"RGB Composite ({', '.join(bands)})\")  # type: ignore[union-attr]\n    ax.axis(\"off\")  # type: ignore[union-attr]\n    return ax  # type: ignore[return-value]\n</code></pre>"},{"location":"architecture/data_model/","title":"GeoTensor and GeoBatch Data Model","text":"<p>This document details the core data abstractions in Ununennium: <code>GeoTensor</code> and <code>GeoBatch</code>.</p>"},{"location":"architecture/data_model/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"architecture/data_model/#geotensor-definition","title":"GeoTensor Definition","text":"<p>A GeoTensor T is defined as a tuple:</p> <pre><code>T = (D, C, A, B, N, M)\n</code></pre> <p>Where: - D \u2208 \u211d^(C\u00d7H\u00d7W) - Data tensor with channels, height, width - C - Coordinate Reference System (CRS) - A - Affine transformation matrix (2\u00d73) - B - Bounding box in CRS units - N - NoData value - M - Band metadata dictionary</p>"},{"location":"architecture/data_model/#affine-transform","title":"Affine Transform","text":"<p>The affine transform maps pixel coordinates to geographic coordinates:</p> <pre><code>\u250c x \u2510   \u250c a  b  c \u2510 \u250c col \u2510\n\u2502   \u2502 = \u2502         \u2502 \u2502     \u2502\n\u2514 y \u2518   \u2514 d  e  f \u2518 \u2514 row \u2518\n</code></pre> <p>Where: - <code>a</code> = pixel width (x resolution) - <code>e</code> = pixel height (y resolution, typically negative) - <code>c</code> = x coordinate of upper-left corner - <code>f</code> = y coordinate of upper-left corner - <code>b, d</code> = rotation parameters (usually 0)</p>"},{"location":"architecture/data_model/#coordinate-transformation","title":"Coordinate Transformation","text":"<p>For a pixel at (col, row):</p> <pre><code>x = a \u00d7 col + b \u00d7 row + c\ny = d \u00d7 col + e \u00d7 row + f\n</code></pre> <p>Inverse transformation for geographic coordinates (x, y):</p> <pre><code>col = (x \u00d7 e - y \u00d7 b - c \u00d7 e + f \u00d7 b) / (a \u00d7 e - b \u00d7 d)\nrow = (x \u00d7 d - y \u00d7 a - c \u00d7 d + f \u00d7 a) / (b \u00d7 d - a \u00d7 e)\n</code></pre>"},{"location":"architecture/data_model/#shape-conventions","title":"Shape Conventions","text":"Tensor Type Shape Description Single image (C, H, W) Channels, Height, Width Batch (B, C, H, W) Batch, Channels, Height, Width Temporal (B, T, C, H, W) Batch, Time, Channels, Height, Width Segmentation labels (H, W) Integer class labels Instance masks (N, H, W) N instance masks"},{"location":"architecture/data_model/#geotensor-api","title":"GeoTensor API","text":"<pre><code>@dataclass\nclass GeoTensor:\n    data: torch.Tensor          # (C, H, W) or (B, C, H, W)\n    crs: CRS | None             # pyproj.CRS object\n    transform: Affine | None    # Affine transform\n    band_names: list[str]       # Band identifiers\n    nodata: float | None        # NoData sentinel value\n\n    # Properties\n    @property\n    def shape(self) -&gt; tuple[int, ...]\n    @property\n    def num_bands(self) -&gt; int\n    @property\n    def height(self) -&gt; int\n    @property\n    def width(self) -&gt; int\n    @property\n    def resolution(self) -&gt; tuple[float, float]\n    @property\n    def bounds(self) -&gt; BoundingBox\n\n    # Methods\n    def to(self, device: str) -&gt; GeoTensor\n    def crop(self, bbox: BoundingBox) -&gt; GeoTensor\n    def select_bands(self, bands: list[int | str]) -&gt; GeoTensor\n    def reproject(self, target_crs: CRS) -&gt; GeoTensor\n    def resample(self, scale: float) -&gt; GeoTensor\n</code></pre>"},{"location":"architecture/data_model/#geobatch-api","title":"GeoBatch API","text":"<pre><code>@dataclass\nclass GeoBatch:\n    images: torch.Tensor        # (B, C, H, W)\n    labels: torch.Tensor        # (B, H, W) or (B, C, H, W)\n    crs: list[CRS]              # CRS per sample\n    transforms: list[Affine]    # Transform per sample\n\n    @property\n    def batch_size(self) -&gt; int\n    @classmethod\n    def collate(cls, samples: list[tuple]) -&gt; GeoBatch\n</code></pre>"},{"location":"architecture/data_model/#memory-layout","title":"Memory Layout","text":"Format Layout Use Case <code>torch.contiguous_format</code> C, H, W contiguous Default operations <code>torch.channels_last</code> H, W, C contiguous CNN inference optimization <p>Memory-efficient operations:</p> <pre><code># Avoid: Creates copy\ntensor.permute(1, 2, 0).contiguous()\n\n# Prefer: Memory format change\ntensor.to(memory_format=torch.channels_last)\n</code></pre>"},{"location":"architecture/data_model/#type-conversions","title":"Type Conversions","text":"Source Target Method NumPy \u2192 GeoTensor <code>GeoTensor(data=np_array)</code> GeoTensor \u2192 NumPy <code>tensor.numpy()</code> GeoTensor \u2192 PIL <code>tensor.to_pil()</code> GeoTensor \u2192 xarray <code>tensor.to_xarray()</code>"},{"location":"architecture/data_model/#boundingbox","title":"BoundingBox","text":"<pre><code>@dataclass\nclass BoundingBox:\n    minx: float   # West boundary\n    miny: float   # South boundary\n    maxx: float   # East boundary\n    maxy: float   # North boundary\n\n    @property\n    def width(self) -&gt; float\n    @property\n    def height(self) -&gt; float\n    @property\n    def area(self) -&gt; float\n    @property\n    def center(self) -&gt; tuple[float, float]\n\n    def intersection(self, other: BoundingBox) -&gt; BoundingBox | None\n    def union(self, other: BoundingBox) -&gt; BoundingBox\n    def buffer(self, distance: float) -&gt; BoundingBox\n    def contains(self, x: float, y: float) -&gt; bool\n</code></pre>"},{"location":"architecture/data_model/#performance-considerations","title":"Performance Considerations","text":"Operation Time Complexity Memory Complexity Band selection O(C) O(selected_bands \u00d7 H \u00d7 W) Cropping O(1) O(crop_H \u00d7 crop_W \u00d7 C) Reprojection O(H \u00d7 W) O(H \u00d7 W \u00d7 C) CRS transform O(1) O(1)"},{"location":"architecture/io_system/","title":"I/O System","text":""},{"location":"architecture/io_system/#supported-formats","title":"Supported Formats","text":"<ul> <li>GeoTIFF</li> <li>COG (Cloud-Optimized GeoTIFF)</li> <li>STAC (future)</li> <li>Zarr (future)</li> </ul> <p>See API Reference for details.</p>"},{"location":"architecture/overview/","title":"System Architecture","text":""},{"location":"architecture/overview/#1-high-level-design-pattern","title":"1. High-Level Design Pattern","text":"<p>Ununennium follows a Layered Architecture pattern, enforcing strict separation of concerns between physical data handling (Geospatial Layer) and abstract pattern recognition (Deep Learning Layer).</p> <pre><code>graph TD\n    subgraph \"Data Layer (Physical)\"\n        IO[IO Module&lt;br&gt;GDAL/Rasterio/Zarr]\n        CRS[CRS Handling&lt;br&gt;PROJ/Transformation]\n        Physical[Radiometry&lt;br&gt;L1C to L2A]\n    end\n\n    subgraph \"Tensor Layer (Bridge)\"\n        GeoBatch[GeoBatch&lt;br&gt;Collated Data]\n        GeoTensor[GeoTensor&lt;br&gt;Tensor + Metadata]\n    end\n\n    subgraph \"Learning Layer (Abstract)\"\n        Model[Model Zoo&lt;br&gt;PyTorch Modules]\n        Loss[Loss Functions&lt;br&gt;Physics-Informed]\n        Train[Trainer&lt;br&gt;Optimization Loop]\n    end\n\n    IO --&gt; CRS\n    CRS --&gt; GeoTensor\n    Physical --&gt; GeoTensor\n    GeoTensor --&gt; GeoBatch\n    GeoBatch --&gt; Model\n    Model --&gt; Loss\n    Loss --&gt; Train\n</code></pre>"},{"location":"architecture/overview/#2-component-deep-dive","title":"2. Component Deep Dive","text":""},{"location":"architecture/overview/#21-the-geospatial-primitives-core","title":"2.1 The Geospatial Primitives (<code>core</code>)","text":"<p>The fundamental unit is the <code>GeoTensor</code>. Unlike a standard <code>torch.Tensor</code> \\((B, C, H, W)\\), a <code>GeoTensor</code> carries: *   <code>.transform</code>: Affine matrix defining the pixel-to-world mapping. *   <code>.crs</code>: The projection definition. *   <code>.bounds</code>: The exact spatial envelope.</p> <p>This allows operations like <code>crop</code>, <code>reproject</code>, and <code>stitch</code> to be mathematically exact rather than index-based approximations.</p>"},{"location":"architecture/overview/#22-the-input-pipeline-io-tiling","title":"2.2 The Input Pipeline (<code>io</code>, <code>tiling</code>)","text":"<p>We solve the \"Small RAM, Big Earth\" problem via lazy loading. 1.  Sampler: Generates <code>BoundingBox</code> queries (not pixels). 2.  Reader: Reads only the requested window from the COG/Zarr store. 3.  Augmenter: Applies geometric transforms. Crucially, if rotation is applied, the <code>GeoTensor.transform</code> matrix is updated, preserving geolocation accuracy even after augmentation.</p>"},{"location":"architecture/overview/#23-the-modeling-engine-models","title":"2.3 The Modeling Engine (<code>models</code>)","text":"<p>Models are decoupled from input size. *   Backbones: Feature extractors (ResNet, ViT) pre-trained on ImageNet or Sentinel-2 (MoCo). *   Heads: Task-specific adapters (Segmentation, Regression) that attach to backbones. *   Registry: A dynamic factory pattern <code>create_model(\"name\")</code> allows string-based instantiation for configuration-driven experiments.</p>"},{"location":"architecture/overview/#24-the-training-loop-training","title":"2.4 The Training Loop (<code>training</code>)","text":"<p>We implement a Hook-based Trainer. *   Lifecycle events (<code>on_epoch_start</code>, <code>on_batch_end</code>) allow injecting custom logic (logging, visualization, EMAs) without modifying the core loop. *   State Management: The Trainer handles checkpointing, allowing seamless resumption of week-long training runs on interruption (common in Spot Instance training).</p>"},{"location":"architecture/overview/#3-data-flow","title":"3. Data Flow","text":"<ol> <li>Ingest: Raw GeoTIFFs \\(\\to\\) <code>GeoTensor</code> (Lazy).</li> <li>Sample: <code>GridGeoSampler</code> selects tiles \\(\\to\\) <code>Window</code> objects.</li> <li>Load: <code>DataLoader</code> triggers IO \\(\\to\\) <code>GeoTensor</code> (Dense).</li> <li>Collate: Stack into <code>GeoBatch</code>.</li> <li>Forward: <code>GeoBatch</code> \\(\\to\\) <code>Model</code> \\(\\to\\) <code>Prediction</code> (Logits).</li> <li>Loss: <code>PhysicsLoss</code>(Prediction, GroundTruth, PhysicsConstraints).</li> <li>Backward: Autograd \\(\\to\\) Optimizer Step.</li> </ol>"},{"location":"architecture/overview/#4-scalability-strategy","title":"4. Scalability Strategy","text":"<ul> <li>Vertical Scailing: <code>GradScaler</code> (AMP) and <code>GradientAccumulation</code> allow training huge models (ViT-L) on single GPUs.</li> <li>Horizontal Scaling: <code>DDP</code> (Distributed Data Parallel) synchronizes gradients across clusters.</li> <li>Data Scaling: Zarr and Cloud-Optimized GeoTIFFs (COG) enable random access to Petabyte datasets without bottlenecking the head node filesystem.</li> </ul>"},{"location":"architecture/training_system/","title":"Training System","text":""},{"location":"architecture/training_system/#trainer","title":"Trainer","text":"<p>Flexible training with callbacks.</p>"},{"location":"architecture/training_system/#callbacks","title":"Callbacks","text":"<ul> <li>CheckpointCallback</li> <li>EarlyStoppingCallback</li> <li>ProgressCallback</li> </ul> <p>See API Reference for details.</p>"},{"location":"models/backbones/","title":"Backbones","text":"<p>Feature extraction backbones for model architectures.</p>"},{"location":"models/backbones/#resnet","title":"ResNet","text":"<pre><code>from ununennium.models.backbones import ResNetBackbone\n\nbackbone = ResNetBackbone(variant=\"resnet50\", in_channels=12)\n</code></pre>"},{"location":"models/backbones/#efficientnet","title":"EfficientNet","text":"<pre><code>from ununennium.models.backbones import EfficientNetBackbone\n\nbackbone = EfficientNetBackbone(variant=\"efficientnet_b0\")\n</code></pre>"},{"location":"models/foundation_models/","title":"Foundation Models","text":"<p>Adapters for pre-trained foundation models.</p>"},{"location":"models/foundation_models/#supported-models","title":"Supported Models","text":"<ul> <li>MAE (Masked Autoencoder)</li> <li>Temporal ViT</li> <li>SatMAE</li> <li>Prithvi</li> </ul>"},{"location":"models/foundation_models/#usage","title":"Usage","text":"<pre><code># Coming in future releases\nfrom ununennium.models.foundation import MAEAdapter\n\nadapter = MAEAdapter(pretrained=\"satmae\")\n</code></pre>"},{"location":"models/gan/","title":"GAN Models","text":"<p>Generative Adversarial Networks for image translation.</p>"},{"location":"models/gan/#pix2pix","title":"Pix2Pix","text":"<p>Paired image-to-image translation.</p> <pre><code>from ununennium.models.gan import Pix2Pix\n\nmodel = Pix2Pix(in_channels=12, out_channels=3)\nfake_b = model(real_a)\n</code></pre>"},{"location":"models/gan/#cyclegan","title":"CycleGAN","text":"<p>Unpaired image-to-image translation.</p> <pre><code>from ununennium.models.gan import CycleGAN\n\nmodel = CycleGAN(in_channels_a=2, in_channels_b=3)\nfake_optical = model(sar_image, direction=\"A2B\")\n</code></pre>"},{"location":"models/model_zoo/","title":"Ununennium Model Zoo &amp; Architecture Theory","text":""},{"location":"models/model_zoo/#1-design-philosophy","title":"1. Design Philosophy","text":"<p>The Ununennium Model Zoo is not merely a collection of weights; it is a repository of Production-Grade, Geospatially-Aware neural architectures. Each model is curated to handle the specific challenges of Earth Observation: *   Multi-Modal Inputs: Native support for \\(N\\)-channel inputs (Optical + SAR + DEM). *   Scale Invariance: Architectures designed to handle GSD (Ground Sampling Distance) variations. *   Rotation Equivariance: Recognition that \"up\" on a map is arbitrary.</p>"},{"location":"models/model_zoo/#2-segmentation-architectures","title":"2. Segmentation Architectures","text":""},{"location":"models/model_zoo/#21-the-u-net-family-standard-resnet-efficientnet","title":"2.1 The U-Net Family (Standard, ResNet, EfficientNet)","text":"<p>The workhorse of semantic segmentation.</p> <p>Mathematical Formulation: Let \\(E\\) be the encoder function and \\(D\\) be the decoder. $$ z_l = E_l(z_{l-1}) \\quad \\text{(Downsampling)} $$ $$ x_l = D_l(x_{l+1}, z_l) \\quad \\text{(Upsampling + Skip)} $$</p> <p>The skip connection \\(z_l\\) is critical in EO because determining boundaries (e.g., road vs. building) requires high-frequency spatial details that are lost in the bottleneck.</p> <p>Ununennium Variations: *   <code>unet_resnet50</code>: Deep residual backbone. Best for large datasets. *   <code>unet_efficientnet_b4</code>: Compound scaling. Best accuracy/FLOPS trade-off. *   Dynamic Upsampling: We use standard <code>bilinear</code> interpolation rather than <code>ConvTranspose</code> to avoid checkerboard artifacts (Odena et al., 2016), which are detrimental for geospatial vectorization.</p>"},{"location":"models/model_zoo/#22-deeplabv3-atrous-convolution","title":"2.2 DeepLabV3+ (Atrous Convolution)","text":"<p>Designed to capture multi-scale context without losing spatial resolution.</p> <p>Atrous (Dilated) Convolution: $$ y[i] = \\sum_k x[i + r \\cdot k] w[k] $$ Where \\(r\\) is the dilation rate. This expands the Receptive Field (RF) exponentially without increasing parameters.</p> <p>ASPP (Atrous Spatial Pyramid Pooling): Parallel branches with rates \\(r=\\{6, 12, 18\\}\\). *   EO Application: Detecting large objects (lakes) and small objects (cars) simultaneously in the same scene.</p>"},{"location":"models/model_zoo/#3-generative-adversarial-networks-gans","title":"3. Generative Adversarial Networks (GANs)","text":""},{"location":"models/model_zoo/#31-pix2pix-conditional-gan","title":"3.1 Pix2Pix (Conditional GAN)","text":"<p>Used for paired translation (e.g., SAR \\(\\to\\) Optical, Cloud \\(\\to\\) Clear).</p> <p>cGAN Objective: $$ \\mathcal{L}{cGAN}(G, D) = \\mathbb{E}{x,y}[\\log D(x, y)] + \\mathbb{E}{x,z}[\\log(1 - D(x, G(x, z)))] $$ L1 Term: $$ \\mathcal{L}{L1}(G) = \\mathbb{E}_{x,y,z}[| y - G(x, z) |_1] $$</p> <p>We use the PatchGAN discriminator, which classifies \\(N \\times N\\) patches as real/fake. This enforces high-frequency texture correctness (texture consistency) critical for visual interpretation.</p>"},{"location":"models/model_zoo/#32-cyclegan-unpaired-translation","title":"3.2 CycleGAN (Unpaired Translation)","text":"<p>Used for domain adaptation (e.g., Summer \\(\\to\\) Winter) where paired data is impossible.</p> <p>Cycle Consistency Loss: $$ x \\to G(x) \\to F(G(x)) \\approx x $$ $$ | F(G(x)) - x |_1 $$</p> <p>This constraint prevents \"mode collapse\" and ensures the semantic content (geometry of roads/buildings) is preserved while the style (season/sensor) changes.</p>"},{"location":"models/model_zoo/#4-physics-informed-neural-networks-pinns","title":"4. Physics-Informed Neural Networks (PINNs)","text":"<p>PINNs integrate differential equations directly into the loss function.</p> <p>Total Loss: $$ \\mathcal{L} = \\mathcal{L}{data} + \\lambda \\mathcal{L}{PDE} $$</p> <p>Where \\(\\mathcal{L}_{PDE}\\) is the residual of the governing equation (e.g., Diffusion, Navier-Stokes). *   Application: Modeling atmospheric dispersion, groundwater flow, or sea surface temperature interpolation. *   Advantage: Generalizes outside the training data because it obeys physical laws.</p>"},{"location":"models/model_zoo/#5-vision-transformers-vit","title":"5. Vision Transformers (ViT)","text":"<p>Moving beyond convolutions.</p> <p>Self-Attention Mechanism: $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$</p> <p>In EO, ViTs excel at Global Context. A pixel depends on the entire scene (e.g., identifying an airport requires seeing the runway arrangement, not just local asphalt texture). *   Mae (Masked Autoencoders): We support MAE pre-training on unlabeled satellite imagery to learn robust representation foundations.</p>"},{"location":"models/pinn/","title":"Physics-Informed Neural Networks","text":"<p>PDE-constrained learning for physical consistency.</p>"},{"location":"models/pinn/#usage","title":"Usage","text":"<pre><code>from ununennium.models.pinn import PINN, DiffusionEquation, MLP\n\nequation = DiffusionEquation(diffusivity=0.1)\nnetwork = MLP([2, 64, 64, 1])\npinn = PINN(network=network, equation=equation)\n\nloss = pinn.compute_loss(x_data, u_data, x_collocation)\n</code></pre>"},{"location":"models/pinn/#available-equations","title":"Available Equations","text":"<ul> <li>DiffusionEquation</li> <li>AdvectionEquation</li> <li>AdvectionDiffusionEquation</li> </ul>"},{"location":"models/task_heads/","title":"Task Heads","text":"<p>Output heads for different ML tasks.</p>"},{"location":"models/task_heads/#classification","title":"Classification","text":"<pre><code>from ununennium.models.heads import ClassificationHead\n\nhead = ClassificationHead(in_channels=2048, num_classes=10)\n</code></pre>"},{"location":"models/task_heads/#segmentation","title":"Segmentation","text":"<pre><code>from ununennium.models.heads import SegmentationHead\n\nhead = SegmentationHead(encoder_channels=[256, 512, 1024, 2048], num_classes=10)\n</code></pre>"},{"location":"tasks/change_detection/","title":"Change Detection","text":"<p>Identify changes between multi-temporal images.</p> <p>Coming in future releases.</p>"},{"location":"tasks/classification/","title":"Scene Classification","text":"<p>Multi-label or single-label image classification.</p>"},{"location":"tasks/classification/#example","title":"Example","text":"<pre><code>from ununennium.models import create_model\n\nmodel = create_model(\"resnet50_classifier\", in_channels=12, num_classes=10)\n</code></pre>"},{"location":"tasks/detection/","title":"Object Detection","text":"<p>Localize objects with bounding boxes.</p> <p>Coming in future releases.</p>"},{"location":"tasks/elevation/","title":"Elevation Estimation","text":"<p>DEM/DSM generation from stereo imagery.</p> <p>Coming in future releases.</p>"},{"location":"tasks/restoration/","title":"Image Restoration","text":"<p>Denoising and atmospheric correction.</p> <p>Coming in future releases.</p>"},{"location":"tasks/segmentation/","title":"Semantic Segmentation","text":"<p>Pixel-wise classification for land cover and land use mapping.</p>"},{"location":"tasks/segmentation/#task-definition","title":"Task Definition","text":"<p>Given an input image <code>X \u2208 \u211d^(C\u00d7H\u00d7W)</code>, predict a label map <code>Y \u2208 {1,...,K}^(H\u00d7W)</code>.</p>"},{"location":"tasks/segmentation/#mathematical-formulation","title":"Mathematical Formulation","text":"<pre><code>f_\u03b8: \u211d^(C\u00d7H\u00d7W) \u2192 \u211d^(K\u00d7H\u00d7W)\nY = argmax_k f_\u03b8(X)_k\n</code></pre>"},{"location":"tasks/segmentation/#common-architectures","title":"Common Architectures","text":""},{"location":"tasks/segmentation/#encoder-decoder-networks","title":"Encoder-Decoder Networks","text":"Architecture Parameters mIoU (LandCover) Speed U-Net 14M - 32M 0.72 - 0.78 Fast DeepLabV3+ 26M - 43M 0.74 - 0.80 Moderate FPN 28M - 45M 0.73 - 0.79 Moderate HRNet 65M - 78M 0.77 - 0.82 Slow"},{"location":"tasks/segmentation/#skip-connection-types","title":"Skip Connection Types","text":"Type Description Use Case Concatenation <code>[F_enc, F_dec]</code> U-Net Summation <code>F_enc + F_dec</code> ResNet-style Attention <code>\u03b1 \u00d7 F_enc + F_dec</code> High-resolution details"},{"location":"tasks/segmentation/#loss-functions","title":"Loss Functions","text":""},{"location":"tasks/segmentation/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<pre><code>L_CE = -\u03a3_k y_k log(p_k)\n</code></pre>"},{"location":"tasks/segmentation/#dice-loss","title":"Dice Loss","text":"<pre><code>L_Dice = 1 - (2 \u00d7 |P \u2229 G| + \u03b5) / (|P| + |G| + \u03b5)\n</code></pre>"},{"location":"tasks/segmentation/#focal-loss","title":"Focal Loss","text":"<p>Addresses class imbalance:</p> <pre><code>L_Focal = -\u03b1_t (1 - p_t)^\u03b3 log(p_t)\n</code></pre> \u03b3 Effect 0 Standard CE 1 Mild down-weighting 2 Strong down-weighting (default) 5 Very strong"},{"location":"tasks/segmentation/#combined-loss","title":"Combined Loss","text":"<pre><code>L_total = \u03bb_1 \u00d7 L_CE + \u03bb_2 \u00d7 L_Dice + \u03bb_3 \u00d7 L_Boundary\n</code></pre> <p>Typical weights: <code>\u03bb_1 = 1.0, \u03bb_2 = 1.0, \u03bb_3 = 0.5</code></p>"},{"location":"tasks/segmentation/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"tasks/segmentation/#per-class-metrics","title":"Per-Class Metrics","text":"Metric Formula Range IoU TP/(TP+FP+FN) [0, 1] Dice 2TP/(2TP+FP+FN) [0, 1] Precision TP/(TP+FP) [0, 1] Recall TP/(TP+FN) [0, 1]"},{"location":"tasks/segmentation/#global-metrics","title":"Global Metrics","text":"Metric Formula mIoU (1/K) \u03a3_k IoU_k Pixel Accuracy \u03a3_k TP_k / N Mean Accuracy (1/K) \u03a3_k (TP_k / N_k)"},{"location":"tasks/segmentation/#benchmark-results","title":"Benchmark Results","text":""},{"location":"tasks/segmentation/#eurosat-land-use-classification","title":"EuroSAT Land Use Classification","text":"Model OA (%) Kappa U-Net ResNet-50 94.2 0.93 DeepLabV3+ 95.1 0.94 Ununennium U-Net 95.8 0.95"},{"location":"tasks/segmentation/#deepglobe-land-cover","title":"DeepGlobe Land Cover","text":"Model mIoU Dice U-Net 0.721 0.812 FPN 0.735 0.823 Ununennium U-Net 0.763 0.847"},{"location":"tasks/segmentation/#training-tips","title":"Training Tips","text":"<ol> <li>Class Weighting: Use inverse frequency weights</li> <li>Data Augmentation: Geometric + radiometric</li> <li>Multi-scale: Training with multiple resolutions</li> <li>Auxiliary Loss: Intermediate supervision</li> </ol>"},{"location":"tasks/segmentation/#example","title":"Example","text":"<pre><code>from ununennium.models import create_model\nfrom ununennium.losses import DiceLoss, CombinedLoss\nimport torch.nn as nn\n\nmodel = create_model(\"unet_resnet50\", in_channels=12, num_classes=10)\nloss_fn = CombinedLoss([\n    nn.CrossEntropyLoss(weight=class_weights),\n    DiceLoss(),\n], weights=[0.5, 0.5])\n</code></pre>"},{"location":"tasks/super_resolution/","title":"Super-Resolution","text":"<p>Enhance spatial resolution of satellite imagery.</p> <p>See GAN Models.</p>"},{"location":"tasks/time_series/","title":"Time Series Analysis","text":"<p>Analyze temporal patterns in satellite imagery.</p> <p>Coming in future releases.</p>"},{"location":"tasks/translation/","title":"Image Translation","text":"<p>Convert between sensor modalities (SAR to optical, etc).</p> <p>See GAN Tutorial.</p>"},{"location":"theory/calibration/","title":"Radiometric Calibration and Correction: A Theoretical Treatise","text":""},{"location":"theory/calibration/#abstract","title":"Abstract","text":"<p>Remote sensing is the science of inferring physical properties of the Earth's surface from measurements of electromagnetic radiation. Unlike computer vision, where pixel values are arbitrary intensities (0-255), remote sensing data represents physical quantities (Radiance, Reflectance, Backscatter). To train Deep Learning models that generalize across time (seasonality) and space (atmosphere), we must strip away the confounding effects of the atmosphere, sun angle, and sensor geometry. This document details the physics of Radiative Transfer, the methods for Atmospheric Correction (L1C \\(\\to\\) L2A), and the rigorous harmonization of multi-sensor constellations.</p>"},{"location":"theory/calibration/#1-the-physics-of-optical-remote-sensing","title":"1. The Physics of Optical Remote Sensing","text":"<p>The fundamental measurement is the spectral radiance \\(L_\\lambda\\) reaching the sensor aperture. This signal is a complex sum of surface interactions and atmospheric scattering.</p>"},{"location":"theory/calibration/#11-the-radiative-transfer-equation-rte","title":"1.1 The Radiative Transfer Equation (RTE)","text":"<p>For a simplified plane-parallel atmosphere, the radiance \\(L_{TOA}\\) at the Top-Of-Atmosphere is:</p> \\[ L_{TOA} = L_{atm} + T^\\uparrow (L_{surface} + L_{neighbor}) \\] <p>Where: *   \\(L_{atm}\\) (Path Radiance): Light scattered by the atmosphere directly into the sensor (Rayleigh + Aerosol). Never touches the ground. \"Haze\". *   \\(T^\\uparrow\\): Upward atmospheric transmittance. *   \\(L_{surface}\\): Radiance derived from the target pixel interaction. *   \\(L_{neighbor}\\) (Adjacency Effect): Light reflected from neighboring pixels scattered into the sensor's view of the target pixel.</p>"},{"location":"theory/calibration/#12-reflectance","title":"1.2 Reflectance","text":"<p>We convert Radiance (Unit: \\(W \\cdot m^{-2} \\cdot sr^{-1} \\cdot \\mu m^{-1}\\)) to Reflectance (Unitless Ratio), which is an intrinsic property of the material.</p>"},{"location":"theory/calibration/#121-toa-reflectance-rho_toa","title":"1.2.1 TOA Reflectance (\\(\\rho_{TOA}\\))","text":"<p>Corrects for solar illumination geometry.</p> \\[ \\rho_{TOA} = \\frac{\\pi \\cdot L_{TOA} \\cdot d^2}{E_{sun} \\cdot \\cos(\\theta_s)} \\] <ul> <li>\\(d\\): Earth-Sun distance in astronomical units (varies with day of year).</li> <li>\\(E_{sun}\\): Exoatmospheric solar irradiance (Solar Constant).</li> <li>\\(\\theta_s\\): Solar Zenith Angle (angle from vertical).</li> </ul> <p>Problem: \\(\\rho_{TOA}\\) still includes the atmosphere. A forest looks brighter on a hazy day than a clear day due to path radiance.</p>"},{"location":"theory/calibration/#122-boa-reflectance-rho_boa-surface-reflectance-sr","title":"1.2.2 BOA Reflectance (\\(\\rho_{BOA}\\)) / Surface Reflectance (SR)","text":"<p>Corrects for the atmosphere to retrieve the \"true\" surface color.</p> \\[ \\rho_{BOA} \\approx \\frac{\\rho_{TOA} - \\rho_{path}}{T^\\downarrow \\cdot T^\\uparrow(1 + S \\cdot \\rho_{BOA})} \\] <ul> <li>\\(\\rho_{path}\\): Reflectance of the atmosphere.</li> <li>\\(T^\\downarrow, T^\\uparrow\\): Downward/Upward transmittance (absorption by Ozone, Water Vapor).</li> <li>\\(S\\): Spherical Albedo of the atmosphere (accounts for multiple scattering).</li> </ul>"},{"location":"theory/calibration/#2-atmospheric-correction-methodologies","title":"2. Atmospheric Correction Methodologies","text":""},{"location":"theory/calibration/#21-physics-based-models-6s-modtran","title":"2.1 Physics-Based Models (6S, MODTRAN)","text":"<p>These solve the RTE utilizing Look-Up Tables (LUTs) pre-computed for various aerosol models (Continental, Maritime, Urban). *   Input: AOT (Aerosol Optical Thickness), Water Vapor column, Ozone. *   Process: Interpolate LUTs \\(\\to\\) Invert RTE. *   Software: Sen2Cor (ESA standard), LaSRC (Landsat standard).</p>"},{"location":"theory/calibration/#22-image-based-methods-dark-object-subtraction-dos","title":"2.2 Image-Based Methods (Dark Object Subtraction - DOS)","text":"<p>Assuming there is at least one \"perfectly black\" object (deep water, dense shadow) in the scene (\\(\\rho_{BOA} \\approx 0\\)). Any radiance observed over this dark object is assumed to be Path Radiance (\\(L_{atm}\\)).</p> \\[ L_{atm} \\approx L_{min} - 0.01 \\cdot (E_{sun} \\cos \\theta / \\pi) \\] <ul> <li>Pros: No external data needed.</li> <li>Cons: Assumes uniform atmosphere across the scene (untrue for large tiles).</li> </ul>"},{"location":"theory/calibration/#23-deep-learning-correction","title":"2.3 Deep Learning Correction","text":"<p>Recent SOTA uses Reference-based Supervised Learning. *   Input: L1C (TOA). *   Target: L2A (BOA) generated by 6S. *   Architecture: U-Net or ResNet. The network implicitly learns the inverse atmospheric function. *   Ununennium: Supports <code>PretrainedAtmosphericNet</code> for on-the-fly correction if L2A is unavailable.</p>"},{"location":"theory/calibration/#3-bidirectional-reflectance-distribution-function-brdf","title":"3. Bidirectional Reflectance Distribution Function (BRDF)","text":"<p>Most standard corrections assume surfaces are Lambertian (scatter light equally in all directions). They are not. *   Hotspot Effect: Surfaces look brighter when the sun is directly behind the sensor (no shadows visible). *   Sunglint: Specular reflection over water.</p> <p>The BRDF Definition: $$ f_r(\\omega_i, \\omega_r) = \\frac{dL_r(\\omega_r)}{dE_i(\\omega_i)} $$</p> <p>NBAR (Nadir BRDF-Adjusted Reflectance): To normalize time-series (e.g., winter sun vs summer sun), we model BRDF (e.g., using Ross-Li kernels) and normalize all observations to a standard geometry (Nadir view, 45 degree sun). This is critical for detecting subtle change vs. illumination artifacts.</p>"},{"location":"theory/calibration/#4-multi-sensor-harmonization","title":"4. Multi-Sensor Harmonization","text":"<p>To combine Sentinel-2 (ESA) and Landsat-9 (NASA) into a dense time series, we must harmonize them.</p>"},{"location":"theory/calibration/#41-spectral-bandpass-mismatch","title":"4.1 Spectral Bandpass Mismatch","text":"<ul> <li>Sentinel-2 Red: 664.5 nm (\\(\\pm\\) 15 nm).</li> <li>Landsat-8 Red: 654.5 nm (\\(\\pm\\) 18 nm).</li> </ul> <p>The Landsat band is wider and shifted blue. Vegetation spectra have steep slopes (Red Edge). A Shift of 10nm changes the value significantly.</p> <p>SBAF (Spectral Band Adjustment Factor): Ununennium implements linear transformation coefficients derived from Hyperspectral (Hyperion) profiles of global land cover.</p> \\[ \\rho_{S2} = \\alpha_0 + \\alpha_1 \\rho_{L8} \\]"},{"location":"theory/calibration/#42-radiometric-resolution","title":"4.2 Radiometric Resolution","text":"<ul> <li>Sentinel-2: 12-bit (0-4096).</li> <li>Landsat-8: 12-bit (scaled 0-55000 in Coll-2).</li> <li>Landsat-7: 8-bit (0-255).</li> </ul> <p>Ununennium Standard: We normalize ALL optical inputs to floating point \\([0, 1]\\) reflectance (and often standardization to \\(z\\)-scores) before the first network layer.</p>"},{"location":"theory/calibration/#5-synthetic-aperture-radar-sar-physics","title":"5. Synthetic Aperture Radar (SAR) Physics","text":"<p>Radars utilize active microwave pulses. The physics is fundamentally different (scattering mechanisms vs chemical absorption).</p>"},{"location":"theory/calibration/#51-the-radar-equation","title":"5.1 The Radar Equation","text":"<p>The received power \\(P_r\\) is:</p> \\[ P_r = \\frac{P_t G^2 \\lambda^2 \\sigma}{(4\\pi)^3 R^4} \\] <ul> <li>\\(P_t\\): Transmitted power.</li> <li>\\(R\\): Range (Distance).</li> <li>\\(\\sigma\\): Radar Cross Section (RCS) of the target.</li> </ul>"},{"location":"theory/calibration/#52-calibration-levels","title":"5.2 Calibration Levels","text":"<ol> <li>Beta Naught (\\(\\beta^0\\)): Radar Brightness. Sensor geometry specific. Uncalibrated for terrain.</li> <li>Sigma Naught (\\(\\sigma^0\\)): Ground Backscatter coefficient. Normalized to ground area assuming a flat earth.<ul> <li>Problem: Slopes facing the radar are bright (compressed area); slopes facing away are dark (stretched area).</li> </ul> </li> <li>Gamma Naught (\\(\\gamma^0\\)): Radiometric Terrain Corrected (RTC). Normalized by the actual illuminated area from a DEM.</li> </ol> \\[ \\gamma^0 = \\frac{\\sigma^0}{\\cos \\theta_{local}} \\] <p>Ununennium's SAR processing pipeline mandates Gamma Naught RTC for all terrain analysis models.</p>"},{"location":"theory/calibration/#53-speckle-statistics","title":"5.3 Speckle Statistics","text":"<p>SAR images look \"grainy\". This is not additive thermal noise; it is coherent interference (Speckle). In a resolution cell, many scatterers interfere constructively or destructively.</p> <ul> <li>PDF: Multi-look intensity follows a Gamma distribution.</li> <li>Correction: We apply a LOG transform (\\(10 \\log_{10}(x)\\)) to convert multiplicative Gamma noise into additive Gaussian-like noise, making MSE loss functions applicable.</li> </ul>"},{"location":"theory/calibration/#6-ununennium-implementation","title":"6. Ununennium Implementation","text":""},{"location":"theory/calibration/#61-preprocessingnormalization","title":"6.1 <code>preprocessing.normalization</code>","text":"<p>We support sensor-aware normalization schemes.</p> <pre><code># MinMax is naive.\n# Percentile is robust to outliers/clouds.\nnorm_img = normalize(img, method=\"percentile\", p=(2, 98))\n</code></pre>"},{"location":"theory/calibration/#62-preprocessingharmonization","title":"6.2 <code>preprocessing.harmonization</code>","text":"<pre><code># Convert Landsat-8 tensor to look like Sentinel-2\ns2_like = harmonize(l8_tensor, source=\"L8\", target=\"S2\")\n</code></pre>"},{"location":"theory/calibration/#7-deep-learning-implications","title":"7. Deep Learning Implications","text":"<ol> <li>Domain Adaptation: Minimizing calibration error reduces the domain gap between sensors. A model trained on harmonized Landsat+Sentinel data generalizes better than one trained on raw DNs.</li> <li>Physical Constraints: In PINNs or generative in-painting, we enforce \\(0 \\le \\rho \\le 1\\).</li> <li>Data Augmentation: Since atmosphere is additive (mostly), we can simulate haze augmentation by adding biases to RGB channels.</li> </ol>"},{"location":"theory/calibration/#8-references","title":"8. References","text":"<ol> <li>Vermote, E. F., et al. (1997). \"Second Simulation of the Satellite Signal in the Solar Spectrum, 6S: An overview\". IEEE TGRS.</li> <li>Chavez, P. S. (1988). \"An improved dark-object subtraction technique for atmospheric scattering correction of multispectral data\". Remote Sensing of Environment.</li> <li>Claverie, M., et al. (2018). \"The Harmonized Landsat and Sentinel-2 surface reflectance data set\". Remote Sensing of Environment.</li> <li>Small, D. (2011). \"Flattening Gamma: Radiometric terrain correction for SAR imagery\". IEEE TGRS.</li> </ol>"},{"location":"theory/error_propagation/","title":"Error Propagation and Sensitivity Analysis in Geospatial Pyplines: A Theoretical Treatise","text":""},{"location":"theory/error_propagation/#abstract","title":"Abstract","text":"<p>In multi-stage Earth Observation (EO) pipelines, errors are not merely additive additive nuisances; they are dynamic quantities that propagate, amplify, or attenuate through non-linear transformations. A \\(10m\\) registration error in L1C data does not simply result in a \\(10m\\) shift in the final output; it interacts with resampling kernels, classification boundaries, and vectorization algorithms to produce complex uncertainty manifolds. This document establishes the Calculus of Error for Ununennium, deriving the theoretical interactions between Geodesy, Radiometry, and Deep Learning inaccuracies, and presenting frameworks for rigorous Sensitivity Analysis.</p>"},{"location":"theory/error_propagation/#1-the-cascade-effect-error-budgeting","title":"1. The Cascade Effect (Error Budgeting)","text":"<p>A typical EO pipeline \\(\\mathcal{P}\\) is a composition of functions:</p> \\[ Y = \\mathcal{P}(X) = (f_{vec} \\circ f_{infer} \\circ f_{norm} \\circ f_{resample} \\circ f_{reg})(X) \\] <p>Where \\(X\\) is the raw sensor data. If \\(X\\) is perturbed by noise \\(\\epsilon\\), the output error is approximated by the Taylor expansion:</p> \\[ \\Delta Y \\approx \\sum_{i} \\frac{\\partial \\mathcal{P}}{\\partial x_i} \\Delta x_i + \\frac{1}{2} \\sum_{i,j} \\frac{\\partial^2 \\mathcal{P}}{\\partial x_i \\partial x_j} \\Delta x_i \\Delta x_j \\]"},{"location":"theory/error_propagation/#11-positional-error-geolocation","title":"1.1 Positional Error (Geolocation)","text":"<p>Satellite imagery has inherent geolocation uncertainty (RMSE). *   Sentinel-2: \\(\\approx 12m\\) (\\(2\\sigma\\)) without GCPs (Ground Control Points). *   Landsat-8: \\(\\approx 18m\\) (\\(2\\sigma\\)).</p> <p>Propagation into Classification: Consider a binary boundary (e.g., Coastline). Let the true boundary be at position \\(x_b\\). The sensor reports \\(x_{obs} = x_b + \\mathcal{N}(0, \\sigma^2_{pos})\\). If we train a CNN to predict \"Water\" vs \"Land\": 1.  Training Noise: The label is effectively \"smeared\" by \\(\\sigma_{pos}\\). The CNN learns a fuzzy decision boundary of width \\(\\approx 2\\sigma_{pos}\\). 2.  Inference Error: Even a perfect classifier will misclassify a strip of pixels along the coast with width equal to the registration error.</p> <p>Ununennium Strategy: We explicitly model Aleatoric Uncertainty at boundaries. $$ \\sigma_{pred}^2 = \\sigma_{model}^2 + (\\nabla I \\cdot \\sigma_{pos})^2 $$ Where \\(\\nabla I\\) is the image gradient. High gradient areas (edges) inherit high positional error.</p>"},{"location":"theory/error_propagation/#2-radiometric-error-propagation","title":"2. Radiometric Error Propagation","text":"<p>Surface Reflectance (\\(\\rho_{surf}\\)) is derived from TOA Randomness (\\(L_{TOA}\\)).</p> \\[ \\rho_{surf} = \\frac{\\pi (L_{TOA} - L_{path})}{E_{sun} \\cos \\theta T} \\] <p>Errors in atmospheric parameters (\\(\\Delta \\tau\\) - Aerosol Optical Depth) propagate non-linearly.</p>"},{"location":"theory/error_propagation/#21-the-ndvi-instability","title":"2.1 The NDVI Instability","text":"<p>Consider the Normalized Difference Vegetation Index: $$ \\text{NDVI} = \\frac{N - R}{N + R} $$</p> <p>The sensitivity to noise in the Red band (\\(\\Delta R\\)) is:</p> \\[ \\frac{\\partial \\text{NDVI}}{\\partial R} = \\frac{-(N+R) - (N-R)}{(N+R)^2} = \\frac{-2N}{(N+R)^2} \\] <p>Critical Observation: Sensitivity approaches infinity as \\((N+R) \\to 0\\) (Dark targets like water/shadows). *   Implication: A tiny sensor noise in a shadow pixel results in massive NDVI variance. A threshold-based water mask (<code>NDVI &lt; 0</code>) is extremely unstable in dark regions.</p>"},{"location":"theory/error_propagation/#3-resampling-and-aliasing-error","title":"3. Resampling and Aliasing Error","text":"<p>When reprojecting data (e.g., from UTM Zone 32 to Zone 33), we apply a resampling kernel \\(h(x)\\).</p>"},{"location":"theory/error_propagation/#31-spectral-mixing-error","title":"3.1 Spectral Mixing Error","text":"<p>Bilinear interpolation creates \"synthetic\" spectral signatures. Let Pixel A be \"Pure Forest\" (\\(\\rho=0.2\\)) and Pixel B be \"Pure Water\" (\\(\\rho=0.8\\)). A resampled pixel located at \\(0.5A + 0.5B\\) has \\(\\rho=0.5\\). *   The Artifact: There is no material on Earth with \\(\\rho=0.5\\). It might look like \"Urban\" or \"Soil\". *   Result: The classifier predicts \"Building\" in the middle of a lake.</p>"},{"location":"theory/error_propagation/#32-volume-estimation-bias","title":"3.2 Volume Estimation Bias","text":"<p>When summing pixels to estimate area (e.g., Water Volume), resampling introduces bias if the kernel is not Flux-Conserving. *   Nearest: Unbiased expectation, high variance. *   Bilinear: Biased at edges (smoothing reduces peaks). *   Average: Unbiased flux, blurs shapes.</p> <p>For Carbon Stock Estimation, Ununennium mandates Area-Weighted Resampling to ensure \\(\\sum \\text{Carbon}_{in} = \\sum \\text{Carbon}_{out}\\).</p>"},{"location":"theory/error_propagation/#4-probabilistic-error-modeling-monte-carlo","title":"4. Probabilistic Error Modeling (Monte Carlo)","text":"<p>Since analytical derivation of \\(\\frac{\\partial \\mathcal{P}}{\\partial X}\\) for a Deep ResNet101 is impossible, we use Monte Carlo Simulation.</p>"},{"location":"theory/error_propagation/#41-the-method-of-realizations","title":"4.1 The Method of Realizations","text":"<p>To estimate the confidence interval of a result (e.g., \"Total Deforested Area = 500 ha\"):</p> <ol> <li>Define Error Distributions:<ul> <li>\\(\\text{Pos} \\sim \\mathcal{N}(0, 10m)\\)</li> <li>\\(\\text{Rad} \\sim \\mathcal{N}(0, 0.01)\\)</li> <li>\\(\\text{Atm} \\sim U(0.1, 0.4)\\)</li> </ul> </li> <li>Generate \\(N\\) Perturbed Inputs: \\(X_k = X + \\delta_k\\)</li> <li>Run Pipeline \\(N\\) times: \\(Y_k = \\mathcal{P}(X_k)\\)</li> <li>Compute Statistics:     $$ \\mu_Y = \\frac{1}{N} \\sum Y_k $$     $$ \\sigma_Y = \\sqrt{\\frac{1}{N-1} \\sum (Y_k - \\mu_Y)^2} $$</li> </ol> <p>Ununennium Implementation: The <code>ununennium.benchmarks.uncertainty</code> module enables wrapping any <code>GeoBatch</code> in a <code>StochasticTensor</code> which automatically forks the pipeline \\(N\\) times.</p>"},{"location":"theory/error_propagation/#5-temporal-error-propagation-time-series","title":"5. Temporal Error Propagation (Time Series)","text":"<p>In change detection, we compare \\(t_1\\) and \\(t_2\\).</p> \\[ \\Delta = y_{t2} - y_{t1} \\] <p>The variance of the difference is the sum of the variances (assuming independence):</p> \\[ \\text{Var}(\\Delta) = \\text{Var}(y_{t2}) + \\text{Var}(y_{t1}) \\]"},{"location":"theory/error_propagation/#51-misregistration-induced-change","title":"5.1 Misregistration Induced Change","text":"<p>If \\(t_1\\) and \\(t_2\\) are misaligned by vector \\(\\vec{d}\\), the \"False Change\" signal is proportional to the local texture gradient.</p> \\[ E_{reg} \\approx \\vec{d} \\cdot \\nabla I \\] <p>Pseudo-invariant Feature (PIF) Validation: To measure this error, we track objects that should not change (e.g., Roads, Large Buildings). Any \\(\\Delta\\) observed on PIFs is pure error. Ununennium's <code>metrics.calibration</code> uses PIFs to normalize Time Series before change detection.</p>"},{"location":"theory/error_propagation/#6-vectorization-and-topology-error","title":"6. Vectorization and Topology Error","text":"<p>The final step is often converting probability rasters to Polygons (GeoJSON).</p>"},{"location":"theory/error_propagation/#61-the-staircase-effect","title":"6.1 The Staircase Effect","text":"<p>A raster is a grid. A grid representation of a straight line at \\(45^\\circ\\) has length \\(L \\sqrt{2}\\). The true length is \\(L\\). *   Perimeter Bias: Raster perimeters always overestimate fractal coastlines. *   Correction: We apply Douglas-Peucker simplification or use Active Contour losses (Level Sets) during training to encourage smooth, vector-friendly boundaries.</p>"},{"location":"theory/error_propagation/#62-topological-violations","title":"6.2 Topological Violations","text":"<p>A pixel-wise Argmax can produce: 1.  Holes: Single pixel dropouts inside a building. 2.  Islands: Single pixel noise in the ocean. 3.  Self-Intersection: (During polygonization).</p> <p>Morphological Post-Processing: We apply <code>Opening</code> (Erosion \\(\\circ\\) Dilation) and <code>Closing</code> (Dilation \\(\\circ\\) Erosion) to enforce topological consistency. $$ Y_{clean} = \\text{Close}(\\text{Open}(Y_{raw}, K), K) $$</p>"},{"location":"theory/error_propagation/#7-validated-error-bounds-olofssons-protocol","title":"7. Validated Error Bounds (Olofsson's Protocol)","text":"<p>For Area Estimation (e.g., REDD+ Reporting), simply counting pixels is scientifically invalid. We must use the Adjusted Estimator.</p>"},{"location":"theory/error_propagation/#71-stratified-estimator","title":"7.1 Stratified Estimator","text":"<p>Using the Confusion Matrix \\(P_{ij}\\) (proportion of area of class \\(i\\) mapped as class \\(j\\)):</p> \\[ \\hat{A}_k = A_{total} \\sum_{i} W_i \\frac{n_{ik}}{n_{i.}} \\] <ul> <li>\\(W_i\\): Mapped proportion of class \\(i\\).</li> <li>\\(n_{ik}\\): Validation samples mapped as \\(i\\) but truly \\(k\\).</li> </ul> <p>This estimator corrects for the bias of the classifier (e.g., if the model systematically over-predicts Forest).</p>"},{"location":"theory/error_propagation/#8-conclusion","title":"8. Conclusion","text":"<p>Error in EO is inevitable. High-fidelity modeling does not mean eliminating error, but bounding it. *   Positional Error limits the smallest detectable feature. *   Radiometric Error limits the subtlest detectable change. *   Sampling Error limits the confidence of area estimates.</p> <p>Ununennium provides the primitives to not just predict \\(Y\\), but to report \\(Y \\pm \\delta\\), elevating Deep Learning from a black-box oracle to a calibrated scientific instrument.</p>"},{"location":"theory/error_propagation/#9-references","title":"9. References","text":"<ol> <li>Olofsson, P., et al. (2014). \"Good practices for estimating area and assessing accuracy of land change\". Remote Sensing of Environment.</li> <li>Congalton, R. G. (1991). \"A review of assessing the accuracy of classifications of remotely sensed data\". Remote Sensing of Environment.</li> <li>Foody, G. M. (2002). \"Status of land cover classification accuracy assessment\". Remote Sensing of Environment.</li> <li>McRoberts, R. E. (2011). \"Satellite image-based maps: Scientific inference or pretty pictures?\". Remote Sensing of Environment.</li> <li>Heuvelink, G. B. (1998). Error propagation in environmental modelling with GIS. Taylor &amp; Francis.</li> </ol>"},{"location":"theory/geodesic_calculations/","title":"Geodesic Calculations in Earth Observation: A Theoretical Treatise","text":""},{"location":"theory/geodesic_calculations/#abstract","title":"Abstract","text":"<p>The fundamental canvas of Earth Observation is not a flat plane, but an irregular, dynamic oblate spheroid. The naive assumption of Euclidean geometry\u2014standard in most Computer Vision libraries\u2014introduces catastrophic metric errors when applied to planetary scales. This document details the rigorous Geodesic Mathematics underpinning the Ununennium library. We explore the evolution of Earth models from spherical approximations to modern geoids, derive the complete solution to the Direct and Inverse Geodesic Problems (Vincenty\u2019s and Karney\u2019s algorithms), and analyze the non-trivial implications of ellipsoidal geometry on Deep Learning architectures, specifically regarding area distortion, directional convolution, and metric learning.</p>"},{"location":"theory/geodesic_calculations/#1-the-shape-of-the-earth-from-sphere-to-geoid","title":"1. The Shape of the Earth: From Sphere to Geoid","text":""},{"location":"theory/geodesic_calculations/#11-the-spherical-approximation","title":"1.1 The Spherical Approximation","text":"<p>Historically, for navigation over short distances, Earth was approximated as a sphere of radius \\(R \\approx 6371 \\text{km}\\). *   Meridional Radius: Constant. *   Metric: Simple Spherical Trigonometry (Great Circles). *   Error: At \\(45^\\circ\\) latitude, the difference between a sphere and the true ellipsoid is approx 21km in radius. This results in distance errors of up to 0.5% (~5m per km), which is unacceptable for cadastral surveying or precision agriculture.</p>"},{"location":"theory/geodesic_calculations/#12-the-oblate-spheroid-ellipsoid-of-revolution","title":"1.2 The Oblate Spheroid (Ellipsoid of Revolution)","text":"<p>Due to rotation, centrifugal force causes the Earth to bulge at the equator and flatten at the poles. Ideally, this shape is an Oblate Spheroid.</p> <p>Defined by two parameters: 1.  Semi-major axis (\\(a\\)): Equatorial radius. 2.  Semi-minor axis (\\(b\\)): Polar radius.</p> <p>Flattening (\\(f\\)): $$ f = \\frac{a - b}{a} $$</p> <p>First Eccentricity (\\(e\\)): $$ e^2 = \\frac{a^2 - b^2}{a^2} = 2f - f^2 $$</p> <p>Second Eccentricity (\\(e'\\)): $$ e'^2 = \\frac{a^2 - b^2}{b^2} = \\frac{e^2}{1-e^2} $$</p>"},{"location":"theory/geodesic_calculations/#standard-ellipsoids","title":"Standard Ellipsoids","text":"Ellipsoid \\(a\\) (meters) \\(1/f\\) Usage WGS84 6378137.0 298.257223563 GPS, Global Standard GRS80 6378137.0 298.257222101 North America (NAD83) Airy 1830 6377563.4 299.3249646 UK (OSGB36) Krassovsky 6378245.0 298.3 Russia, Eastern Bloc <p>Ununennium defaults to WGS84 unless strictly specified otherwise by the CRS.</p>"},{"location":"theory/geodesic_calculations/#13-the-geoid-the-gravity-model","title":"1.3 The Geoid (The Gravity Model)","text":"<p>The ellipsoid is a geometric abstraction. The physical Earth is defined by gravity. The Geoid is the equipotential surface of the Earth's gravity field that equates to Mean Sea Level (MSL) if the oceans were at rest and extended through the continents.</p> <p>$$ h = H + N $$ *   \\(h\\) (Ellipsoidal Height): Native height from GPS (geometric). *   \\(H\\) (Orthometric Height): Height above Sea Level (physical/gravitational). *   \\(N\\) (Geoid Undulation): The separation between Ellipsoid and Geoid.</p> <p>\\(N\\) ranges from -106m (in the Indian Ocean) to +85m (near Iceland). Deep Learning Implication: When training models on DSM/DTM (Digital Surface Models), mixing ellipsoidal heights (from raw LiDAR) with orthometric heights (from contour maps) creates significant bias. Ununennium's <code>io</code> module checks for vertical datum compatibility (e.g., EGM96 vs EGM2008).</p>"},{"location":"theory/geodesic_calculations/#2-the-geodesic-problem","title":"2. The Geodesic Problem","text":"<p>The shortest path between two points on an ellipsoid is a Geodesic. Unlike a Great Circle on a sphere, a geodesic on an ellipsoid does not return to its starting point; it oscillates between maximum northern and southern latitudes.</p>"},{"location":"theory/geodesic_calculations/#21-the-metric-tensor","title":"2.1 The Metric Tensor","text":"<p>The differential distance \\(ds\\) on the surface is given by the First Fundamental Form:</p> \\[ ds^2 = M(\\phi)^2 d\\phi^2 + N(\\phi)^2 \\cos^2\\phi d\\lambda^2 \\] <p>Where the radii of curvature are: *   Meridional Radius (\\(M\\)): Radius of curvature along the meridian (North-South).     $$ M(\\phi) = \\frac{a(1-e^2)}{(1-e^2 \\sin^2\\phi)^{3/2}} $$ *   Prime Vertical Radius (\\(N\\)): Radius of curvature perpendicular to the meridian (East-West).     $$ N(\\phi) = \\frac{a}{\\sqrt{1-e^2 \\sin^2\\phi}} $$</p> <p>Notably, \\(M(\\phi)\\) varies by ~1% from pole to equator. A \"degree of latitude\" is longer at the poles (\\(111.7\\) km) than at the equator (\\(110.6\\) km). This 1.1km discrepancy per degree is the primary source of error in spherical assumptions.</p>"},{"location":"theory/geodesic_calculations/#3-distance-algorithms","title":"3. Distance Algorithms","text":""},{"location":"theory/geodesic_calculations/#31-the-haversine-spherical","title":"3.1 The Haversine (Spherical)","text":"<p>Fast, differentiable, but inaccurate (\\(0.5\\%\\) error).</p> \\[ \\theta = 2 \\arcsin \\sqrt{\\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos\\phi_1 \\cos\\phi_2 \\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)} $$ $$ d = R \\cdot \\theta \\] <p>Used in Ununennium only for \"Quick Look\" operations or lightweight neighbor search.</p>"},{"location":"theory/geodesic_calculations/#32-vincentys-formulae-iterative-ellipsoidal","title":"3.2 Vincenty's Formulae (Iterative Ellipsoidal)","text":"<p>Vincenty (1975) proposed an iterative solution to the inverse problem accurate to \\(0.5\\) mm.</p> <p>Algorithm (Inverse Problem): Given \\((\\phi_1, \\lambda_1)\\) and \\((\\phi_2, \\lambda_2)\\), find distance \\(s\\) and azimuths \\(\\alpha_1, \\alpha_2\\).</p> <ol> <li>Compute reduced latitudes: \\(U_1 = \\arctan((1-f)\\tan\\phi_1)\\), \\(U_2 = \\arctan((1-f)\\tan\\phi_2)\\).</li> <li>Initialize longitude difference on auxiliary sphere \\(\\Lambda = \\lambda_2 - \\lambda_1\\).</li> <li> <p>Iterate until convergence (\\(\\Delta \\Lambda &lt; 10^{-12}\\)):</p> <ul> <li>\\(\\sin\\sigma = \\sqrt{(\\cos U_2 \\sin\\Lambda)^2 + (\\cos U_1 \\sin U_2 - \\sin U_1 \\cos U_2 \\cos \\Lambda)^2}\\)</li> <li>\\(\\cos\\sigma = \\sin U_1 \\sin U_2 + \\cos U_1 \\cos U_2 \\cos \\Lambda\\)</li> <li>\\(\\sigma = \\arctan2(\\sin\\sigma, \\cos\\sigma)\\)</li> <li>\\(\\sin\\alpha = \\frac{\\cos U_1 \\cos U_2 \\sin \\Lambda}{\\sin\\sigma}\\)</li> <li>\\(\\cos^2\\alpha = 1 - \\sin^2\\alpha\\)</li> <li>\\(\\cos(2\\sigma_m) = \\cos\\sigma - \\frac{2\\sin U_1 \\sin U_2}{\\cos^2\\alpha}\\) (handle division by zero if equatorial)</li> <li>\\(C = \\frac{f}{16} \\cos^2\\alpha [4 + f(4 - 3\\cos^2\\alpha)]\\)</li> <li>\\(\\Lambda_{new} = L + (1-C)f\\sin\\alpha \\{ \\sigma + C\\sin\\sigma [\\cos(2\\sigma_m) + C\\cos\\sigma(-1 + 2\\cos^2(2\\sigma_m))] \\}\\)</li> </ul> </li> <li> <p>Calculate \\(s\\) using Bessel's formula corrections.</p> </li> </ol> <p>Limitations: Vincenty's iteration fails to converge for nearly antipodal points (opposite sides of the Earth).</p>"},{"location":"theory/geodesic_calculations/#33-karneys-algorithm-2013","title":"3.3 Karney's Algorithm (2013)","text":"<p>Charles Karney solved the antipodal convergence problem using a generalized series expansion. *   Accuracy: 15 nm (nanometers). *   Ununennium Implementation: We wrap the <code>geographiclib</code> C++ implementation via <code>pyproj</code> for maximum speed and stability. All critical <code>measure_distance</code> calls in the library use Karney's method.</p>"},{"location":"theory/geodesic_calculations/#4-area-calculations-and-distortion","title":"4. Area Calculations and Distortion","text":"<p>Calculating area on the ellipsoid requires integration of the Gaussian curvature.</p>"},{"location":"theory/geodesic_calculations/#41-polygon-area","title":"4.1 Polygon Area","text":"<p>The area of a geodesic polygon is given by the discrete sum of excesses. $$ A = \\sum_{edges} (Area under geodesic segment) $$ This uses the Danielsen (1989) integrals involving Fourier series expansions of the oscillatory terms.</p>"},{"location":"theory/geodesic_calculations/#42-map-projection-distortion","title":"4.2 Map Projection Distortion","text":"<p>Deep Learning models (CNNs) operate on raster grids, which are planar projections of the ellipsoid. $$ (x, y) = P(\\phi, \\lambda) $$</p> <p>Tissot's Indicatrix describes the distortion at any point. 1.  Conformal Projections (e.g., Mercator, stereographic):     *   Preserve Angles (Shapers). A circle on Earth \\(\\to\\) Circle on Map.     *   Distort Area. \\(Scale \\propto \\sec(\\phi)\\).     *   Issue: A pixel at 60\u00b0N represents \\(4\\times\\) less real-world area than a pixel at the Equator. A \"House\" class at 60\u00b0N looks 4x bigger (in pixels) than a \"House\" at the Equator.</p> <ol> <li>Equal-Area Projections (e.g., Albers Conic, Gall-Peters):<ul> <li>Preserve Area.</li> <li>Distort Shape (Shearing).</li> <li>Issue: Objects are squashed or stretched. Rotation-variant CNNs may fail to recognize a squashed building.</li> </ul> </li> </ol> <p>Ununennium Strategy: We strongly recommend Equal-Area projections (like Albers or Sinusoidal) for statistical reporting (e.g., \"Total Deforested Area\"). For training, if the AOI is global, we implement Dynamic Scale Augmentation or Latitude-Weighted Loss.</p>"},{"location":"theory/geodesic_calculations/#latitude-weighted-loss","title":"latitude-Weighted Loss","text":"<p>To counter Mercator distortion in global models: $$ w(pixel) = \\frac{A_{true}}{A_{map}} = \\cos(\\phi) $$ We downweight high-latitude pixels because 1 \"map meter\" corresponds to fewer \"real meters\".</p>"},{"location":"theory/geodesic_calculations/#5-coding-operations-and-rhumb-lines","title":"5. Coding Operations and Rhumb Lines","text":""},{"location":"theory/geodesic_calculations/#51-rhumb-lines-loxodromes","title":"5.1 Rhumb Lines (Loxodromes)","text":"<p>A path of constant bearing (azimuth). On a Mercator projection, this is a straight line. *   Geodesic: Shortest path (Curved on map). *   Rhumb Line: Constant compass heading (Straight on map).</p> <p>Use Case in AI: Tracking ships in AIS (Automatic Identification System) data. Ships often follow Rhumb lines (constant heading) rather than Geodesics (requires constant steering adjustment). Predicting vessel trajectories using LSTM/Transformers requires discerning the navigation mode.</p>"},{"location":"theory/geodesic_calculations/#6-implementation-in-geotensor","title":"6. Implementation in <code>geotensor</code>","text":"<p>The <code>GeoTensor</code> class enables algebraic operations that respect geodetic reality.</p>"},{"location":"theory/geodesic_calculations/#61-the-geodesic-buffer","title":"6.1 The Geodesic Buffer","text":"<p><code>GeoTensor.buffer(distance_meters)</code> Naive buffering (dilating by \\(N\\) pixels) creates circles of varying physical sizes depending on latitude. Ununennium computes the buffer in the Geodetic space and rasterizes it back, ensuring a 50m buffer is exactly 50m everywhere.</p>"},{"location":"theory/geodesic_calculations/#62-bearing-calculation","title":"6.2 Bearing Calculation","text":"<p><code>GeoTensor.bearing_to(other_tensor)</code> Calculates the forward azimuth \\(\\alpha_1\\) for every pixel to a target. *   Input: Two \\((B, H, W)\\) tensors of lat/lons. *   Output: One \\((B, H, W)\\) tensor of bearings \\([0, 360)\\). *   Math: Uses vectorised <code>atan2</code> on the sphere (or Vincenty for precision).</p> \\[ \\theta = \\operatorname{atan2}(\\sin\\Delta\\lambda \\cdot \\cos\\phi_2, \\cos\\phi_1 \\cdot \\sin\\phi_2 - \\sin\\phi_1 \\cdot \\cos\\phi_2 \\cdot \\cos\\Delta\\lambda) \\]"},{"location":"theory/geodesic_calculations/#7-performance-considerations","title":"7. Performance Considerations","text":"<p>Ellipsoidal math is computationally expensive. *   Haversine: ~20 FLOPS. *   Vincenty: ~2000 FLOPS (Iterative). *   Karney: ~3000 FLOPS.</p> <p>Batching Strategy: For heavy metric learning (e.g., Geodesic Triplet Loss), calculating exact geodesic distances matrix-wise \\((N \\times N)\\) is too slow. Approximation: We project local batches to an Azimuthal Equidistant Projection centered on the batch centroid. In this local projected space, Euclidean distance \\(\\approx\\) Geodesic distance. $$ d_{geo}(x, y) \\approx | P_{local}(x) - P_{local}(y) |_2 $$ This reduces complexity to \\(O(1)\\) per pair, allowing massive batch sizes.</p>"},{"location":"theory/geodesic_calculations/#8-conclusion","title":"8. Conclusion","text":"<p>Geometry is the physics of geospatial data. Ununennium refuses to treat Earth as a plane. By implementing rigorous WGS84 ellipsoidal mathematics (Karney/Vincenty) and handling projection distortions explicitly, we ensure that the Deep Learning models we build are not just pattern matchers, but physical measurement instruments.</p>"},{"location":"theory/geodesic_calculations/#9-references","title":"9. References","text":"<ol> <li>Vincenty, T. (1975). \"Direct and Inverse Solutions of Geodesics on the Ellipsoid with application of nested equations\". Survey Review, 23(176), 88-93.</li> <li>Karney, C. F. F. (2013). \"Algorithms for geodesics\". Journal of Geodesy, 87(1), 43-55.</li> <li>Snyder, J. P. (1987). Map Projections: A Working Manual. USGS Professional Paper 1395.</li> <li>Danielsen, J. (1989). \"The Area under the Geodesic\". Survey Review, 30(232), 61-66.</li> <li>Tissot, N. A. (1859). M\u00e9moire sur la repr\u00e9sentation des surfaces et les projections des cartes g\u00e9ographiques.</li> <li>Rapp, R. H. (1991). Geometric Geodesy, Part I. Ohio State University Department of Geodetic Science.</li> </ol>"},{"location":"theory/metric_definitions/","title":"Metric Definitions and Evaluation Theory in Geospatial AI: A Theoretical Treatise","text":""},{"location":"theory/metric_definitions/#abstract","title":"Abstract","text":"<p>Evaluation metrics are the compass by which we navigate the optimization landscape. In Geospatial AI, standard computer vision metrics (like Accuracy) are often misleading due to extreme class imbalance (the \"99% Water\" problem) and the geometric nature of the predictions. A model with 99% pixel accuracy can fail to detect 50% of buildings if the objects are small. This treatise rigorously defines the set-theoretic, topological, and probabilistic metrics used in Ununennium, analyzing their mathematical properties, failure modes, and optimal use-cases.</p>"},{"location":"theory/metric_definitions/#1-confusion-matrix-primitives","title":"1. Confusion Matrix Primitives","text":"<p>Let \\(\\mathcal{D} = \\{ (y_i, \\hat{y}_i) \\}_{i=1}^N\\) be the set of Ground Truth (\\(y\\)) and Prediction (\\(\\hat{y}\\)) pairs. For a class \\(c\\), we define: *   TP (True Positive): \\(y=c \\land \\hat{y}=c\\) *   FP (False Positive): \\(y \\neq c \\land \\hat{y}=c\\) (Type I Error) *   FN (False Negative): \\(y=c \\land \\hat{y} \\neq c\\) (Type II Error) *   TN (True Negative): \\(y \\neq c \\land \\hat{y} \\neq c\\)</p>"},{"location":"theory/metric_definitions/#11-the-accuracy-paradox","title":"1.1 The Accuracy Paradox","text":"<p>$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$ In a dataset with 99% background and 1% target (e.g., Ships in Ocean), a trivial model predicting \"All Background\" achieves 99% accuracy. Thus, Accuracy is deprecated for all Ununennium tasks except balanced scene classification.</p>"},{"location":"theory/metric_definitions/#2-set-theoretic-metrics-overlap","title":"2. Set-Theoretic Metrics (Overlap)","text":"<p>These metrics view the image as a set of indices \\(\\Omega\\). Let \\(A = \\{i | y_i = 1\\}\\) and \\(B = \\{i | \\hat{y}_i = 1\\}\\).</p>"},{"location":"theory/metric_definitions/#21-jaccard-index-iou-intersection-over-union","title":"2.1 Jaccard Index (IoU - Intersection over Union)","text":"<p>Measures the ratio of intersection to union.</p> \\[ \\text{IoU}(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{TP}{TP + FP + FN} \\] <ul> <li>Range: \\([0, 1]\\).</li> <li>Properties: Symmetric, Scale Invariant.</li> <li>Hardness: IoU penalizes errors more harshly than F1. \\(\\text{IoU} \\le \\text{F1}\\) always.</li> </ul>"},{"location":"theory/metric_definitions/#22-dice-coefficient-f1-score","title":"2.2 Dice Coefficient (F1 Score)","text":"<p>The Harmonic Mean of Precision and Recall.</p> \\[ \\text{Dice}(A, B) = \\frac{2 |A \\cap B|}{|A| + |B|} = \\frac{2TP}{2TP + FP + FN} \\] <ul> <li>Use Case: Often used as a Loss Function (Soft Dice) because it is differentiable and convex-ish.</li> <li>Relation to IoU: \\(Dice = \\frac{2 \\cdot IoU}{1 + IoU}\\).</li> </ul>"},{"location":"theory/metric_definitions/#3-geometric-and-boundary-metrics-topology","title":"3. Geometric and Boundary Metrics (Topology)","text":"<p>For applications like Building Footprint extraction, pixel overlap is insufficient. A model that predicts a blob covering 90% of a building gets 0.9 IoU but fails to capture the square shape.</p>"},{"location":"theory/metric_definitions/#31-hausdorff-distance","title":"3.1 Hausdorff Distance","text":"<p>Measures the maximum distance from a point in one set to the nearest point in the other set.</p> \\[ d_H(A, B) = \\max \\left( \\sup_{a \\in A} \\inf_{b \\in B} d(a, b), \\sup_{b \\in B} \\inf_{a \\in A} d(a, b) \\right) \\] <ul> <li>Interpretation: The \"worst case\" error. If the model imagines a single outlier pixel 1km away, \\(d_H = 1km\\), even if IoU \\(\\approx 1.0\\).</li> <li>Robustness: Highly sensitive to outliers.</li> </ul>"},{"location":"theory/metric_definitions/#32-boundary-f1-bf1-score","title":"3.2 Boundary F1 (BF1) Score","text":"<p>Computes Precision and Recall only within a distance buffer \\(d\\) of the boundaries.</p> \\[ \\text{Boundary}(S) = \\{ x \\in S \\mid \\exists y \\notin S, d(x, y) &lt; 1 \\text{px} \\} \\] <p>We define a match if prediction boundary is within tolerance \\(\\theta\\) of ground truth boundary. *   Application: Critical for Road Network extraction, where the width of the road changes but the centerline topology matters.</p>"},{"location":"theory/metric_definitions/#4-detection-metrics-object-level","title":"4. Detection Metrics (Object Level)","text":"<p>For Object Detection (Bounding Boxes), we operate on discrete objects, not pixels.</p>"},{"location":"theory/metric_definitions/#41-average-precision-ap","title":"4.1 Average Precision (AP)","text":"<p>AP is the area under the Precision-Recall Curve (PR Curve).</p> \\[ AP = \\int_0^1 p(r) dr \\] <p>In practice, we compute interpolated AP (COCO style): $$ AP = \\frac{1}{11} \\sum_{r \\in {0, 0.1, ..., 1.0}} p_{interp}(r) $$ Where \\(p_{interp}(r) = \\max_{\\tilde{r} \\ge r} p(\\tilde{r})\\).</p>"},{"location":"theory/metric_definitions/#42-mean-average-precision-map","title":"4.2 Mean Average Precision (mAP)","text":"<p>The mean of AP across all classes. *   mAP@50: IoU threshold = 0.5. *   mAP@[50:95]: Average over IoU thresholds 0.50 to 0.95 (step 0.05). Rewards tight localization.</p>"},{"location":"theory/metric_definitions/#5-calibration-metrics-probabilistic","title":"5. Calibration Metrics (Probabilistic)","text":"<p>Measures the reliability of the confidence scores \\(\\hat{p}\\).</p>"},{"location":"theory/metric_definitions/#51-brier-score","title":"5.1 Brier Score","text":"<p>The Mean Squared Error of the probability vector.</p> \\[ BS = \\frac{1}{N} \\sum_{i=1}^N ( \\hat{p}_i - y_i )^2 \\] <p>It decomposes into: $$ BS = \\text{Reliability} - \\text{Resolution} + \\text{Uncertainty} $$ *   Reliability: How close are probabilities to true frequencies. *   Resolution: How distinct are the forecasts from the global average.</p>"},{"location":"theory/metric_definitions/#52-expected-calibration-error-ece","title":"5.2 Expected Calibration Error (ECE)","text":"<p>(See Uncertainty Theory for derivation). ECE helps detect if a model is \"hallucinating\" high confidence on wrong answers.</p>"},{"location":"theory/metric_definitions/#6-time-series-metrics","title":"6. Time Series Metrics","text":"<p>For comparing temporal signals \\(T_1\\) and \\(T_2\\).</p>"},{"location":"theory/metric_definitions/#61-dynamic-time-warping-dtw","title":"6.1 Dynamic Time Warping (DTW)","text":"<p>Euclidean distance assumes indices align (\\(i\\) matches \\(i\\)). DTW allows non-linear alignment (warping) to handle temporal shifts (e.g., crop season starting 10 days later).</p> \\[ DTW(T_1, T_2) = \\min_{\\pi} \\sqrt{ \\sum_{(i,j) \\in \\pi} (T_1[i] - T_2[j])^2 } \\] <p>Where \\(\\pi\\) is the warping path.</p>"},{"location":"theory/metric_definitions/#7-aggregation-strategies-macro-vs-micro","title":"7. Aggregation Strategies (Macro vs Micro)","text":"<p>When calculating metrics across \\(N\\) images and \\(C\\) classes:</p>"},{"location":"theory/metric_definitions/#71-micro-averaging","title":"7.1 Micro-Averaging","text":"<p>Pools all pixels/objects from all images into one giant Confusion Matrix, then computes metric. $$ F1_{micro} $$ *   Behavior: Dominated by frequent classes. If \"Water\" is 90% of pixels, Micro-F1 essentially measures Water performance.</p>"},{"location":"theory/metric_definitions/#72-macro-averaging","title":"7.2 Macro-Averaging","text":"<p>Computes metric for each class/image independently, then averages. $$ F1_{macro} = \\frac{1}{C} \\sum_{c=1}^C F1_c $$ *   Behavior: Treats all classes equally. \"Rare Bird\" has same weight as \"Water\". *   Ununennium Default: We report Macro metrics for validation to ensure performance on rare classes is visible.</p>"},{"location":"theory/metric_definitions/#8-ununennium-implementation","title":"8. Ununennium Implementation","text":"<p>The <code>ununennium.metrics</code> module provides a unified API backed by <code>torchmetrics</code>.</p> <pre><code>class MetricCollection(nn.Module):\n    def __init__(self, num_classes):\n        self.iou = IoU(num_classes, reduction='elementwise_mean')\n        self.f1 = F1Score(num_classes, average='macro')\n        self.brier = BrierScore()\n\n    def update(self, preds, target):\n        self.iou.update(preds, target)\n        self.f1.update(preds, target)\n\n    def compute(self):\n        return {\n            \"mIoU\": self.iou.compute(),\n            \"MacroF1\": self.f1.compute()\n        }\n</code></pre>"},{"location":"theory/metric_definitions/#9-conclusion","title":"9. Conclusion","text":"<p>A single number can never capture the performance of a complex geospatial model. *   Use IoU for spatial overlap. *   Use Boundary F1 for shape fidelity. *   Use mAP for object counting. *   Use ECE for confidence reliability.</p> <p>Standardizing these metrics allows us to move beyond \"it looks good\" to empirical science.</p>"},{"location":"theory/metric_definitions/#10-references","title":"10. References","text":"<ol> <li>Powers, D. M. (2011). \"Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation\".</li> <li>Csurka, G., et al. (2013). \"What is a good evaluation measure for semantic segmentation?\". BMVC.</li> <li>Everingham, M., et al. (2010). \"The Pascal Visual Object Classes (VOC) Challenge\". IJCV.</li> <li>Niculescu-Mizil, A., &amp; Caruana, R. (2005). \"Predicting good probabilities with supervised learning\". ICML.</li> <li>Taha, A. A., &amp; Hanbury, A. (2015). \"Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool\". BMC Medical Imaging.</li> </ol>"},{"location":"theory/resampling_kernels/","title":"Signal Processing and Resampling Kernels in Remote Sensing: A Theoretical Treatise","text":""},{"location":"theory/resampling_kernels/#abstract","title":"Abstract","text":"<p>Resampling is the digital signal processing operation of transforming a discrete sampled signal from one coordinate system to another. In Deep Learning for Remote Sensing, this occurs constantly: reprojecting maps to common CRS, resizing images for fixed-size CNN inputs, or aligning multi-temporal acquisitions. While often treated as a trivial implementation detail (<code>cv2.resize</code>), the choice of the resampling kernel \\(h(x)\\) has profound spectral and spatial implications. Poor resampling introduces Aliasing (Moir\u00e9 patterns), Ringing (Gibbs phenomenon), and Spectral Mixing, which establish hard ceilings on model performance. This treatise explores the Fourier Theory of resampling and guides the selection of optimal kernels for EO data.</p>"},{"location":"theory/resampling_kernels/#1-the-sampling-theorem-and-reconstruction","title":"1. The Sampling Theorem and Reconstruction","text":"<p>A continuous signal \\(f(x)\\) is sampled at intervals \\(T\\) to create a discrete sequence \\(f[n] = f(nT)\\). To recover or resample this signal at a new point \\(x'\\), we must perform Reconstruction.</p> \\[ \\hat{f}(x) = \\sum_{n=-\\infty}^{\\infty} f[n] \\cdot h\\left(\\frac{x - nT}{T}\\right) \\] <p>Here, \\(h(t)\\) is the Reconstruction Kernel.Ideally, to perfectly reconstruct a band-limited signal, \\(h(t)\\) must be the normalized sinc function (Whittaker\u2013Shannon interpolation formulation):</p> \\[ h(t) = \\operatorname{sinc}(t) = \\frac{\\sin(\\pi t)}{\\pi t} \\] <p>However, the sinc function has infinite support (extends to \\(\\pm \\infty\\)), making it computationally impossible. All practical kernels are finite approximations of the sinc.</p>"},{"location":"theory/resampling_kernels/#2-kernel-taxonomy-in-frequency-domain","title":"2. Kernel Taxonomy in Frequency Domain","text":"<p>We analyze kernels based on their spatial shape \\(h(x)\\) and their Frequency Response \\(H(f) = \\mathcal{F}\\{h(x)\\}\\).</p>"},{"location":"theory/resampling_kernels/#21-nearest-neighbor-box-kernel","title":"2.1 Nearest Neighbor (Box Kernel)","text":"<p>The 0th-order interpolator.</p> \\[ h(t) = \\begin{cases} 1 &amp; |t| \\le 0.5 \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <ul> <li>Frequency Response: \\(H(f) = \\operatorname{sinc}(f)\\).</li> <li>analysis:<ul> <li>Passband: Poor. It begins attenuating frequencies well before the Nyquist limit.</li> <li>Stopband: Terrible. It has infinite side-lobes that allow high-frequency aliases to fold back into the signal.</li> </ul> </li> <li>Artifacts: \"Jaggies\" (aliasing), Blockiness, sub-pixel spatial shifts (\\(\\pm 0.5\\) px jitter).</li> <li>Use Case: Categorical Masks. We cannot interpolate the integer \"Class 5 (Forest)\" and \"Class 1 (Water)\" to get \"Class 3 (Urban)\". We must strictly preserve values.</li> </ul>"},{"location":"theory/resampling_kernels/#22-bilinear-interpolation-triangle-kernel","title":"2.2 Bilinear Interpolation (Triangle Kernel)","text":"<p>The 1st-order interpolator. Convolution of two box kernels.</p> \\[ h(t) = \\begin{cases} 1 - |t| &amp; |t| &lt; 1 \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <ul> <li>Frequency Response: \\(H(f) = \\operatorname{sinc}^2(f)\\).</li> <li>Analysis:<ul> <li>The squared sinc decays faster (\\(1/f^2\\)), reducing aliasing compared to Nearest.</li> <li>Low-Pass Filter: It strongly attenuates high frequencies within the passband. This results in Blurring.</li> </ul> </li> <li>Use Case: fast on-the-fly resizing where sharpness is secondary to speed.</li> </ul>"},{"location":"theory/resampling_kernels/#23-bicubic-convolution-keys-cubic","title":"2.3 Bicubic Convolution (Keys' Cubic)","text":"<p>A 3rd-order polynomial approximation of the sinc. Defined by a parameter \\(\\alpha\\) (usually -0.5 or -0.75, which matches the slope of the sinc function at \\(x=1\\)).</p> \\[ h(t) = \\begin{cases} (\\alpha+2)|t|^3 - (\\alpha+3)|t|^2 + 1 &amp; |t| &lt; 1 \\\\ \\alpha|t|^3 - 5\\alpha|t|^2 + 8\\alpha|t| - 4\\alpha &amp; 1 \\le |t| &lt; 2 \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <ul> <li>Analysis:<ul> <li>Sharper passband than bilinear.</li> <li>Better stopband suppression.</li> <li>Negative Lobes: The kernel goes negative. This can produce output values outside the input range (Over/Undershoot).</li> </ul> </li> <li>Artifacts: Ringing (Gibbs Phenomenon). High-contrast edges (e.g., coastline) will have a \"halo\" or \"ghost\" line parallel to them.</li> <li>Use Case: Visualization (RGB), DEMs (requires continuous derivatives).</li> </ul>"},{"location":"theory/resampling_kernels/#24-lanczos-resampling","title":"2.4 Lanczos Resampling","text":"<p>A Windowed Sinc function. \\(\\operatorname{sinc}(x)\\) multiplied by a \"Lanczos Window\" (central lobe of a larger sinc).</p> \\[ L(x) = \\begin{cases} \\operatorname{sinc}(x) \\operatorname{sinc}(x/a) &amp; -a &lt; x &lt; a \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <p>Usually \\(a=3\\) (Lanczos-3). *   Analysis: The closest practical approximation to the ideal low-pass filter. *   Pros: Extreme sharpness, minimal aliasing. *   Cons: Computatioanlty expensive (larger support window), still suffers from Ringing.</p>"},{"location":"theory/resampling_kernels/#3-aliasing-and-the-nyquist-limit","title":"3. Aliasing and The Nyquist Limit","text":"<p>Aliasing occurs when we downsample (decimate) a signal without removing frequencies higher than the new Nyquist rate (\\(f_{new} = f_{old} / \\text{scale}\\)).</p> <ul> <li>Spatial Moir\u00e9: High-frequency patterns (e.g., rows of crops, tile roofs) create low-frequency interference patterns when resized carelessly.</li> <li>Deep Learning Impact: CNNs are texture-biased. If a Corn field (periodic texture) is downsampled aliased to look like a swamp (noisy texture), the model fails.</li> </ul> <p>The Golden Rule of Downsampling: Always apply a Low-Pass Filter (Blur) before decimation to remove frequencies \\(&gt; f_{new}\\). *   Average / Area-Weighted Resampling: This is mathematically equivalent to integration (box filter) followed by sampling. It conserves the total radiometric energy (Flux). *   Ununennium Standard: For downscaling satellite imagery (e.g., converting 10m Sentinel to 30m Landsat grid), we strictly use Average or Gauss resampling mechanisms to preserve physical flux.</p>"},{"location":"theory/resampling_kernels/#4-phase-shift-errors-the-shift-problem","title":"4. Phase Shift Errors (The \"Shift\" Problem)","text":"<p>In Nearest Neighbor resampling, the grid snaps to the closest integer. $$ x_{new} = \\operatorname{round}(x_{old}) $$ This introduces a random shift error \\(\\epsilon \\sim U[-0.5, 0.5]\\) pixels.</p> <p>In Change Detection (Siamese Networks), this is fatal. *   Image T1 (Jan): Shifted -0.4 px. *   Image T2 (Feb): Shifted +0.4 px. *   Total Misalignment: 0.8 px. *   Result: False positive \"Change\" detected along every distinct edge in the image.</p> <p>Solution: Ununennium's <code>GeoTensor</code> operations use sub-pixel precise alignment utilizing Bilinear/Bicubic kernels even for small shifts, strictly avoiding Nearest Neighbor for co-registration tasks.</p>"},{"location":"theory/resampling_kernels/#5-spline-interpolation","title":"5. Spline Interpolation","text":"<p>Beyond convolution kernels, we can fit piecewise polynomials (Splines) to the data points. B-Splines provide maximum smoothness (\\(C^n\\) continuity). *   Application: Creating Digital Elevation Models (DEMs) from sparse LiDAR points. *   Relevance: Calculating Slope and Aspect (derivatives of elevation) requires the surface to be differentiable. Nearest Neighbor produces valid elevation but garbage slope (0 or infinity). Cubic Splines ensure smooth derivatives.</p>"},{"location":"theory/resampling_kernels/#6-implementation-in-ununennium","title":"6. Implementation in Ununennium","text":"<p>We wrap both <code>rasterio</code> (GDAL) and <code>torch.nn.functional</code> resampling modes.</p>"},{"location":"theory/resampling_kernels/#61-geotensorreproject","title":"6.1 <code>GeoTensor.reproject()</code>","text":"<p>Automatically selects the kernel based on the semantic type of the tensor (stored in metadata).</p> Tensor Type Kernel Rationale <code>Mask</code> (Class ID) <code>nearest</code> Preserves integers. <code>Mask</code> (Probability) <code>bilinear</code> Smooth probability field. <code>Image</code> (Optical) <code>cubic</code> Visual sharpness. <code>Image</code> (Radiance) <code>average</code> Energy conservation (if downsampling). <code>SAR</code> (Db) <code>average</code> Noise reduction (multilooking)."},{"location":"theory/resampling_kernels/#62-the-areaweighted-resampler","title":"6.2 The <code>AreaWeighted</code> Resampler","text":"<p>For flux-conserving transformations (e.g., Population Count rasters, Emission grids):</p> <pre><code>def reproject_flux(tensor, src_transform, dst_transform):\n    \"\"\"\n    Ensures sum(input) == sum(output).\n    Standard interpolation preserves density (value), not sum.\n    \"\"\"\n    # ... Implementation utilizing fractional overlap calculation ...\n</code></pre>"},{"location":"theory/resampling_kernels/#7-conclusion","title":"7. Conclusion","text":"<p>Resampling is not just \"making pixels bigger or smaller\". It is a filtering operation that fundamentally alters the spectral and spatial content of the data. Use Nearest only for masks. Use Average for downscaling physics. Use Lanczos/Cubic for upscaling/visualization. Ignoring this leads to aliased training data, shift artifacts, and physically impossible spectral values.</p>"},{"location":"theory/resampling_kernels/#8-references","title":"8. References","text":"<ol> <li>Shannon, C. E. (1949). \"Communication in the Presence of Noise\". Proceedings of the IRE.</li> <li>Keys, R. (1981). \"Cubic convolution interpolation for digital image processing\". IEEE TASSP.</li> <li>Turkowski, K. (1990). \"Filters for common resampling tasks\". Graphics Gems.</li> <li>Parker, J. A., et al. (1983). \"Comparison of interpolating methods for image resampling\". IEEE TMN.</li> </ol>"},{"location":"theory/sampling_theory/","title":"Sampling Theory in Geospatial Machine Learning: A Theoretical Treatise","text":""},{"location":"theory/sampling_theory/#abstract","title":"Abstract","text":"<p>Sampling is the mechanism by which we bridge the continuous reality of the physical world and the discrete, finite datasets required for machine learning. In the geospatial domain, sampling is complicated by Spatial Autocorrelation, Heterogeneity, and Class Imbalance. A naive Random Sampling strategy, while theoretically sound for i.i.d. data, is often catastrophic for Earth Observation (EO) models, leading to vast redundancy, missed rare events, and inflated accuracy metrics. This document presents a comprehensive theoretical framework for Spatial Sampling Design, covering Design-based vs. Model-based inference, advanced tiling strategies, and temporal sampling requirements for dynamic phenomena.</p>"},{"location":"theory/sampling_theory/#1-the-sampling-problem-in-continuum","title":"1. The sampling Problem in Continuum","text":"<p>Geo-data is inherently continuous. A satellite image is a discrete discretization of a continuous radiance field \\(L(lat, lon, t)\\). Our labels (e.g., \"Forest\") are discrete representations of continuous land cover gradients.</p>"},{"location":"theory/sampling_theory/#11-the-change-of-support-problem-cosp","title":"1.1 The \"Change of Support\" Problem (COSP)","text":"<p>The \"Support\" refers to the size, shape, and orientation of the physical area associated with a data value. *   Point Support: A ground measurement (e.g., weather station thermometer). *   Areal Support: A satellite pixel (e.g., \\(10m \\times 10m\\) average).</p> <p>The Ecological Fallacy: Relationships observed at one support level (e.g., correlation between NDVI and Biomass at 30m) do not necessarily hold at another support level (e.g., 1km). *   Implication: A model trained on Sentinel-2 (10m) cannot simply be applied to MODIS (250m) by upsampling the input. The variance structure changes non-linearly.</p>"},{"location":"theory/sampling_theory/#2-theoretical-frameworks-of-inference","title":"2. Theoretical Frameworks of Inference","text":""},{"location":"theory/sampling_theory/#21-design-based-inference","title":"2.1 Design-Based Inference","text":"<ul> <li>Assumption: The population values \\(y\\) are fixed constants. Randomness comes entirely from the selection probability of the sample sites \\(s\\).</li> <li>Goal: Estimate global population parameters (e.g., Total Wheat Area).</li> <li>Key: Selection probabilities \\(\\pi_i\\) must be known and non-zero.</li> <li>** Estimator:** Horvitz-Thompson Estimator:     $$ \\hat{Y} = \\sum_{i \\in s} \\frac{y_i}{\\pi_i} $$</li> </ul>"},{"location":"theory/sampling_theory/#22-model-based-inference","title":"2.2 Model-Based Inference","text":"<ul> <li>Assumption: The observed values \\(y\\) are realizations of a random process (spatial superpopulation).</li> <li>Goal: Predict \\(y\\) at unobserved locations (Krige/CNN predictions).</li> <li>Key: The model must correctly characterize the spatial covariance structure.</li> <li>Role of Sampling: To minimize the variance of the prediction error (\\(E[ (\\hat{Y} - Y)^2 ]\\)).</li> </ul> <p>Ununennium effectively operates in the Model-Based paradigm (training neural networks) but uses Design-Based principles for validation to ensure unbiased metric reporting.</p>"},{"location":"theory/sampling_theory/#3-spatial-sampling-designs","title":"3. Spatial Sampling Designs","text":"<p>How do we select \\(N\\) locations (patches) from the study area \\(\\mathcal{D}\\)?</p>"},{"location":"theory/sampling_theory/#31-simple-random-sampling-srs","title":"3.1 Simple Random Sampling (SRS)","text":"<p>$$ P(x_i \\in S) = \\frac{1}{|\\mathcal{D}|} $$ *   Pros: Unbiased mean estimation. *   Cons:     1.  Clustering: purely random points often form accidental clusters.     2.  Inefficiency: Due to autocorrelation, clustered points carry redundant information.     3.  Gaps: Leaves large areas unsampled.</p>"},{"location":"theory/sampling_theory/#32-systematic-sampling-grid","title":"3.2 Systematic Sampling (Grid)","text":"<p>Selects points on a regular lattice with spacing \\(\\Delta\\). *   Pros: Maximizes spatial coverage (minimizes Max(min(dist))). Even distribution of error. *   Cons:     1.  Aliasing: If the landscape has a periodic feature (e.g., center-pivot irrigation, city blocks) matching the grid spacing, the sample will be completely biased.     2.  Variance Estimation: No unbiased estimator for variance exists for a single random start systematic sample.</p>"},{"location":"theory/sampling_theory/#33-stratified-random-sampling","title":"3.3 Stratified Random Sampling","text":"<p>Partition \\(\\mathcal{D}\\) into strata \\(H_1, ..., H_L\\) (e.g., land cover classes: Forest, Urban, Water). Draw \\(n_h\\) samples from each stratum.</p> <p>Neyman Optimal Allocation: To minimize variance of the global estimate, sample size \\(n_h\\) should be proportional to the stratum size \\(N_h\\) and the stratum standard deviation \\(\\sigma_h\\).</p> \\[ n_h = n \\frac{N_h \\sigma_h}{\\sum_{k=1}^L N_k \\sigma_k} \\] <ul> <li>Implication for DL: We should sample more from heterogeneous classes (Urban) and less from homogenous classes (Water), even if Water covers 70% of the Earth.</li> </ul>"},{"location":"theory/sampling_theory/#34-spatially-balanced-sampling-poisson-disk-grts","title":"3.4 Spatially Balanced Sampling (Poisson Disk / GRTS)","text":"<p>Generalized Random Tessellation Stratified (GRTS) and Poisson Disk sampling generate designs that are spatially balanced (no holes, no clumps) but maintain probability properties. *   Algorithm (Poisson Disk): Generate a point. Reject any subsequent point closer than radius \\(r\\). *   Result: \"Blue Noise\" distribution. Ideal for training set construction to force diversity.</p>"},{"location":"theory/sampling_theory/#4-tiling-strategies-for-cnns","title":"4. Tiling Strategies for CNNs","text":"<p>In Deep Learning, our \"samples\" are not points but \\(H \\times W\\) image patches (tiles).</p>"},{"location":"theory/sampling_theory/#41-the-epoch-definition-problem","title":"4.1 The Epoch Definition Problem","text":"<p>In standard computer vision (ImageNet), an \"Epoch\" is one pass over all images. In EO, we have one continuous image (The World). What is an epoch? *   Approach A (Grid): Fixed sliding window stride \\(S\\). Epoch = one full cover.     *   Drawback: Massive redundancy (\\(95\\%\\) of ocean tiles looking identical). *   Approach B (Random): \\(N\\) random crops per \"epoch\".     *   Drawback: No guarantee of coverage.</p> <p>Ununennium approach: We define an epoch as \\(N\\) iterations, where samples are drawn via Importance Sampling from a probability map \\(P(x,y)\\).</p>"},{"location":"theory/sampling_theory/#42-importance-sampling-hard-example-mining","title":"4.2 Importance Sampling (Hard Example Mining)","text":"<p>We construct a sampling probability map \\(M(x,y)\\).</p> \\[ M(x,y) = \\alpha P_{freq}(x,y) + \\beta P_{edge}(x,y) + \\gamma P_{error}(x,y) \\] <ol> <li> <p>Class Frequency Balancing (\\(P_{freq}\\)):     Rare classes get higher weight.     $$ w_c = \\frac{1}{\\ln(f_c + 1.2)} $$     This \"Soft Inverse Frequency\" prevents over-sampling extreme outliers while boosting rare classes.</p> </li> <li> <p>Structural Complexity (\\(P_{edge}\\)):     We compute the gradient magnitude \\(|\\nabla I|\\) or entropy of the image.</p> <ul> <li>Logic: Uniform green fields are easy. Edges are hard. Sample edges.</li> </ul> </li> <li> <p>Active Learning (\\(P_{error}\\)):     Feed back previous epoch's loss/uncertainty.</p> <ul> <li>Logic: Focus training on areas where the model is confused (High Entropy).</li> </ul> </li> </ol>"},{"location":"theory/sampling_theory/#43-overlap-strategy-and-edge-effects","title":"4.3 Overlap Strategy and Edge Effects","text":"<p>CNNs suffer from padding artifacts at the edges of tiles. *   Feature Degradation: Zero-padding distorts features near boundaries. *   Inference Strategy:     1.  Overlap-Tile: Extract tiles of size \\(D \\times D\\) with stride \\(S &lt; D\\).     2.  Center-Crop Prediction: Predict on \\(D \\times D\\), but only keep the center \\(S \\times S\\).     3.  Gaussian Blending: Accumulate predictions with a Gaussian weight mask falling to zero at edges.</p> <p>$$ P_{final}(i, j) = \\frac{\\sum_k w_k(i,j) P_k(i,j)}{\\sum_k w_k(i,j)} $$ Where \\(k\\) indexes the overlapping tiles.</p>"},{"location":"theory/sampling_theory/#5-temporal-sampling-time-series","title":"5. Temporal Sampling (Time Series)","text":"<p>For Satellite Image Time Series (SITS), the sampling dimension extends to Time \\(t\\).</p>"},{"location":"theory/sampling_theory/#51-the-nyquist-shannon-limit-in-phenology","title":"5.1 The Nyquist-Shannon Limit in Phenology","text":"<p>To classify crops, we must capture the phenological curve (NDVI trajectory). *   Signal: Vegetation growth cycle (~120 days). *   Bandwidth: Key inflection points (emergence, flowering, harvest) happen in ~1-2 weeks. *   Requirement: Revisit rate \\(&lt; \\frac{1}{2} \\lambda_{min} \\approx 5-10\\) days.</p> <p>Sentinel-2 (5 days) satisfies this. Landsat (16 days) often fails, especially with cloud cover.</p>"},{"location":"theory/sampling_theory/#52-handling-irregular-sampling","title":"5.2 Handling Irregular Sampling","text":"<p>Satellite time series are never regularly sampled due to clouds. $$ T = {t_1, t_2, ..., t_N}, \\quad \\Delta t_i \\neq \\text{const} $$</p> <p>Methods in Ununennium: 1.  Linear Interpolation (Gap Filling): Simple, assumes linearity (bad for rapid changes). 2.  Fourier/Harmonic Regression: Fits sines/cosines (good for seasonality). 3.  Attention Models (Transformers):     Temporal Attention mechanisms (like in PSE+TAE) naturally handle irregular positions by encoding the time differences \\(\\Delta t\\) as Positional Embeddings.</p> <pre><code>$$ PE(t) = [\\sin(\\omega_1 t), \\cos(\\omega_1 t), ...] $$\n</code></pre>"},{"location":"theory/sampling_theory/#6-the-trainvaltest-split-geometry","title":"6. The Train/Val/Test Split Geometry","text":"<p>Creating a valid Test set in spatial data is arguably the hardest sampling problem.</p>"},{"location":"theory/sampling_theory/#61-spatial-leakage","title":"6.1 Spatial Leakage","text":"<p>If using Random Sampling (Pixel-based split): *   Train Pixel \\((i, j)\\) *   Test Pixel \\((i, j+1)\\) *   Correlation \\(\\approx 1.0\\). *   Test Accuracy = 99%, Real World Accuracy = 60%.</p>"},{"location":"theory/sampling_theory/#62-block-sampling-with-buffers","title":"6.2 Block Sampling with Buffers","text":"<p>We must split by Blocks (or Scenes), separated by a Buffer Distance.</p> \\[ \\text{Buffer} &gt; \\text{Range of Variogram}(\\alpha) \\] <p>Algorithm: 1.  Compute empirical variogram of the target variable. Determine effective range \\(R\\). 2.  Divide ROI into grid cells of size \\(S &gt; R\\). 3.  Randomly assign cells to Train/Val/Test. 4.  Erode the training masks by \\(R/2\\) and validation masks by \\(R/2\\) to create a dead zone.</p>"},{"location":"theory/sampling_theory/#63-spatial-k-fold","title":"6.3 Spatial k-Fold","text":"<p>Standard k-Fold is biased. We use Spatially Blocked k-Fold. *   This ensures that in Fold \\(k\\), the model is tested on a geographic region effectively unseen during training. *   It measures Spatial Generalization (interpolation/extrapolation capability) rather than memorization.</p>"},{"location":"theory/sampling_theory/#7-mathematical-bounds-of-generalization","title":"7. Mathematical Bounds of Generalization","text":"<p>Learning Theory (VC-Dimension, Rademacher Complexity) assumes i.i.d. For Spatial Mixing processes (\\(\\beta\\)-mixing), the error bound loosens.</p> \\[ E_{gen} \\le E_{train} + \\mathcal{O}\\left( \\sqrt{\\frac{d \\log N_{eff}}{N_{eff}}} \\right) \\] <p>where \\(N_{eff} \\ll N\\) is the effective sample size due to autocorrelation. *   Conclusion: You need vastly more labeled pointers in a clustered spatial dataset to achieve the same generalization guarantee as in an i.i.d. dataset (like MNIST).</p>"},{"location":"theory/sampling_theory/#8-implementation","title":"8. Implementation","text":"<p>Ununennium's <code>tiling</code> module provides the <code>GeoSampler</code> class hierarchy.</p> <pre><code>class ConstrainedRandomGeoSampler(GeoSampler):\n    \"\"\"\n    Samples patches such that:\n    1. Intersection with ROI &gt; threshold\n    2. Cloud cover &lt; threshold\n    3. Foreground class presence &gt; threshold\n    \"\"\"\n    def __init__(self, dataset, prob_map, roi, size, stride):\n        # Implementation uses R-Tree index for O(log N) queries\n        # Rejection sampling with early exit\n        pass\n</code></pre> <p>Also provided is <code>SpatialKFold</code> in <code>ununennium.model_selection</code>.</p>"},{"location":"theory/sampling_theory/#9-references","title":"9. References","text":"<ol> <li>Cochran, W. G. (1977). Sampling Techniques. John Wiley &amp; Sons.</li> <li>Stehman, S. V. (1999). \"Basic probability sampling designs for thematic map accuracy assessment\". International Journal of Remote Sensing.</li> <li>Olofsson, P., et al. (2014). \"Good practices for estimating area and assessing accuracy of land change\". Remote Sensing of Environment.</li> <li>Brus, D. J. (2019). \"sampling for digital soil mapping: A tutorial relevant to digital soil mapping and other domains\".</li> <li>Wang, J., et al. (2016). \"The spatial autocorrelation problem in the reliability of machine learning models\".</li> </ol>"},{"location":"theory/spatial_autocorrelation/","title":"Spatial Autocorrelation in Geospatial Deep Learning: A Theoretical Treatise","text":""},{"location":"theory/spatial_autocorrelation/#abstract","title":"Abstract","text":"<p>Spatial autocorrelation is the defining characteristic of geospatial data, encapsulating the axiom that \"near things are more related than distant things\" (Tobler, 1970). In the context of Deep Learning (DL), it presents a fundamental paradox: it is the very feature that allows Convolutional Neural Networks (CNNs) to work (by exploiting local structure), yet it simultaneously invalidates the core statistical assumptions of Independent and Identically Distributed (i.i.d.) data upon which modern optimization and validation theories rest. This document provides a rigorous, comprehensive analysis of spatial autocorrelation, its mathematical quantification, its pernicious effects on error estimation, and the advanced strategies required to manage it in production-grade Earth Observation (EO) systems.</p>"},{"location":"theory/spatial_autocorrelation/#1-foundations-of-spatial-dependence","title":"1. Foundations of Spatial Dependence","text":""},{"location":"theory/spatial_autocorrelation/#11-the-independent-and-identically-distributed-iid-fallacy","title":"1.1 The Independent and Identically Distributed (i.i.d.) Fallacy","text":"<p>Classical statistical learning theory assumes that a dataset \\(D = \\{(x_1, y_1), ..., (x_N, y_N)\\}\\) consists of samples drawn independently from a joint probability distribution \\(P(X, Y)\\).</p> \\[ P(D) = \\prod_{i=1}^N P(x_i, y_i) \\] <p>This assumption simplifies the calculation of the likelihood function \\(\\mathcal{L}(\\theta)\\) and ensures that the empirical risk converges to the expected risk as \\(N \\to \\infty\\).</p> <p>The Geospatial Reality: In satellite imagery, \\(x_i\\) (a pixel or patch) is functionally dependent on \\(x_j\\) where distance \\(d(i, j)\\) is small. *   Atmospheric Continuity: Haze and clouds are continuous fields. *   ecological Continuity: A forest does not end abruptly at a pixel boundary; biases in species distribution persist over kilometers. *   Sensor Noise: CCD strips introduce row-correlated noise.</p> <p>Consequently, the effective sample size \\(N_{eff}\\) is fundamentally different from the observed sample size \\(N\\).</p>"},{"location":"theory/spatial_autocorrelation/#12-toblers-first-law-tfl","title":"1.2 Tobler's First Law (TFL)","text":"<p>\"Everything is related to everything else, but near things are more related than distant things.\" \u2014 Waldo Tobler, A Computer Movie Simulating Urban Growth in the Detroit Region (1970)</p> <p>While TFL is intuitive, its mathematical formalization leads to the concept of the Spatial Weight Matrix (\\(W\\)) or the Covariance Function (\\(C(h)\\)) in Geostatistics. High autocorrelation implies that the information content of new samples decays rapidly as sampling density increases within a fixed region.</p>"},{"location":"theory/spatial_autocorrelation/#13-the-modifiable-areal-unit-problem-maup","title":"1.3 The Modifiable Areal Unit Problem (MAUP)","text":"<p>Spatial autocorrelation is inextricably linked to MAUP (Openshaw, 1984). The correlation observed depends on the scale and aggregation of units. 1.  Scale Effect: Correlations change as we aggregate pixels into larger patches. A strong correlation at 10m resolution (individual tree crowns) may vanish at 1km resolution (forest stand). 2.  Zoning Effect: The shape of the aggregation units (square tiles vs. hexagons vs. administrative boundaries) alters the measured statistics.</p> <p>Ununennium addresses MAUP by enforcing resolution-invariant sampling strategies and providing multi-scale uncertainty metrics.</p>"},{"location":"theory/spatial_autocorrelation/#2-statistical-quantification-measures","title":"2. Statistical Quantification Measures","text":"<p>To mitigate autocorrelation, we first must measure it. We employ several statistics, each sensitive to different aspects of spatial structure.</p>"},{"location":"theory/spatial_autocorrelation/#21-global-morans-i","title":"2.1 Global Moran's \\(I\\)","text":"<p>Moran's \\(I\\) (Moran, 1950) is the spatial equivalent of the Pearson correlation coefficient. It measures the global tendency of similar values to cluster.</p>"},{"location":"theory/spatial_autocorrelation/#211-formal-definition","title":"2.1.1 Formal Definition","text":"\\[ I = \\frac{N}{S_0} \\frac{\\sum_{i=1}^N \\sum_{j=1}^N w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_{i=1}^N (x_i - \\bar{x})^2} \\] <p>Where: *   \\(N\\): Total number of spatial units (e.g., image patches). *   \\(x_i\\): Attribute value at location \\(i\\) (e.g., prediction error, class label). *   \\(\\bar{x}\\): Global mean of \\(x\\). *   \\(w_{ij}\\): Spatial weight describing the proximity of \\(i\\) and \\(j\\). *   \\(S_0\\): Sum of all spatial weights, \\(S_0 = \\sum_{i=1}^N \\sum_{j=1}^N w_{ij}\\).</p>"},{"location":"theory/spatial_autocorrelation/#212-the-spatial-weight-matrix-w","title":"2.1.2 The Spatial Weight Matrix (\\(W\\))","text":"<p>The choice of \\(W\\) dictates the \"neighborhood.\" *   Contiguity Weights: \\(w_{ij} = 1\\) if \\(i\\) and \\(j\\) share a boundary (Queen or Rook). *   Distance Weights: \\(w_{ij} = 1/d_{ij}^\\alpha\\) (Inverse Distance Weighting). *   K-Nearest Neighbors: \\(w_{ij} = 1\\) if \\(j \\in \\text{KNN}(i)\\).</p> <p>In deep learning, we typically use a Kernel-based Weight, typically Gaussian:</p> \\[ w_{ij} = \\exp\\left(-\\frac{d_{ij}^2}{2\\sigma^2}\\right) \\]"},{"location":"theory/spatial_autocorrelation/#213-hypothesis-testing","title":"2.1.3 Hypothesis Testing","text":"<p>The expected value of Moran's \\(I\\) under the null hypothesis (complete spatial randomness) is:</p> \\[ E[I] = \\frac{-1}{N-1} \\] <p>As \\(N \\to \\infty\\), \\(E[I] \\to 0\\). Values significantly greater than \\(E[I]\\) indicate positive autocorrelation (clustering), while values less indicate negative autocorrelation (dispersion).</p> <p>We calculate the Z-score for significance testing:</p> \\[ Z_I = \\frac{I - E[I]}{\\sqrt{\\text{Var}(I)}} \\] <p>If \\(|Z_I| &gt; 1.96\\), we reject the null hypothesis at \\(p &lt; 0.05\\). This is a critical check before trusting any grid-search or hyperparameter tuning result.</p>"},{"location":"theory/spatial_autocorrelation/#22-gearys-c-ratio","title":"2.2 Geary's \\(C\\) Ratio","text":"<p>Geary's \\(C\\) (Geary, 1954) is based on squared differences rather than cross-products.</p> \\[ C = \\frac{(N-1) \\sum_{i=1}^N \\sum_{j=1}^N w_{ij} (x_i - x_j)^2}{2 S_0 \\sum_{i=1}^N (x_i - \\bar{x})^2} \\] <p>Key Differences: *   Range: \\(0 \\le C \\le 2\\) (approx). *   Interpretation:     *   \\(C &lt; 1\\): Positive autocorrelation (similar values are close).     *   \\(C = 1\\): Randomness.     *   \\(C &gt; 1\\): Negative autocorrelation. *   Sensitivity: Geary's \\(C\\) is more sensitive to local deviations, whereas Moran's \\(I\\) is more sensitive to global trends.</p>"},{"location":"theory/spatial_autocorrelation/#23-getis-ord-general-g","title":"2.3 Getis-Ord General \\(G\\)","text":"<p>The General \\(G\\) statistic distinguishes between \"High-High\" clusters (hotspots) and \"Low-Low\" clusters (coldspots), which Moran's \\(I\\) cannot differentiate (both are just \"positive\").</p> \\[ G = \\frac{\\sum_{i=1}^N \\sum_{j=1, j \\neq i}^N w_{ij} x_i x_j}{\\sum_{i=1}^N \\sum_{j=1, j \\neq i}^N x_i x_j} \\]"},{"location":"theory/spatial_autocorrelation/#3-local-indicators-of-spatial-association-lisa","title":"3. Local Indicators of Spatial Association (LISA)","text":"<p>Global statistics summarize the entire study area into a single number. However, geospatial processes are rarely stationary; relationships vary across space (Spatial Heterogeneity). LISA statistics (Anselin, 1995) decompose global measures into local contributions.</p>"},{"location":"theory/spatial_autocorrelation/#31-local-morans-i_i","title":"3.1 Local Moran's \\(I_i\\)","text":"\\[ I_i = z_i \\sum_{j} w_{ij} z_j \\] <p>Where \\(z_i = (x_i - \\bar{x}) / \\sigma\\) is the standardized score.</p> <p>The Summation Property: $$ \\sum_{i=1}^N I_i \\propto I_{global} $$</p> <p>Cluster Map Generation: By computing \\(I_i\\) for every pixel (or tile) and testing significance, we generate a LISA Cluster Map with four categories: 1.  HH (High-High): High values surrounded by high values (Hotspot). 2.  LL (Low-Low): Low values surrounded by low values (Coldspot). 3.  HL (High-Low): High value outlier in a low value field (Spatial Outlier). 4.  LH (Low-High): Low value outlier in a high value field (Spatial Outlier).</p> <p>Ununennium Use Case: We use HL and LH regions for Hard Negative Mining. These represent unexpected features (e.g., a building in the middle of a forest, a hole in a cloud) that constitute the most informative training examples.</p>"},{"location":"theory/spatial_autocorrelation/#32-getis-ord-g_i-gi-star","title":"3.2 Getis-Ord \\(G_i^*\\) (Gi-Star)","text":"<p>The \\(G_i^*\\) statistic is widely used for heatmap generation.</p> \\[ G_i^* = \\frac{\\sum_{j=1}^N w_{ij} x_j - \\bar{X} \\sum_{j=1}^N w_{ij}}{S \\sqrt{\\frac{N \\sum_{j=1}^N w_{ij}^2 - (\\sum_{j=1}^N w_{ij})^2}{N-1}}} \\] <p>Unlike Local Moran, \\(G_i^*\\) includes the value at \\(i\\) in the summation.</p>"},{"location":"theory/spatial_autocorrelation/#4-variography-and-geostatistics","title":"4. Variography and Geostatistics","text":"<p>For continuous fields, we model spatial dependence using the Semivariogram.</p>"},{"location":"theory/spatial_autocorrelation/#41-the-experimental-variogram","title":"4.1 The Experimental Variogram","text":"\\[ \\hat{\\gamma}(h) = \\frac{1}{2|N(h)|} \\sum_{(i,j) \\in N(h)} (x_i - x_j)^2 \\] <p>Where \\(N(h)\\) is the set of pairs separated by distance vector \\(h\\) (lag).</p>"},{"location":"theory/spatial_autocorrelation/#42-analytical-models","title":"4.2 Analytical Models","text":"<p>To utilize the variogram, we fit a theoretical model to the experimental points:</p> <ol> <li> <p>Spherical Model: Linear rise until range, then constant.     $$ \\gamma(h) = \\begin{cases} c_0 + c \\left( \\frac{3h}{2a} - \\frac{h^3}{2a^3} \\right) &amp; h \\le a \\ c_0 + c &amp; h &gt; a \\end{cases} $$</p> </li> <li> <p>Exponential Model: Asymptotic approach to sill.     $$ \\gamma(h) = c_0 + c \\left( 1 - \\exp\\left(\\frac{-h}{a}\\right) \\right) $$</p> </li> <li> <p>Gaussian Model: Parabolic rise (very smooth phenomena).     $$ \\gamma(h) = c_0 + c \\left( 1 - \\exp\\left(\\frac{-h^2}{a^2}\\right) \\right) $$</p> </li> </ol>"},{"location":"theory/spatial_autocorrelation/#43-key-parameters-interpretation","title":"4.3 Key Parameters Interpretation","text":"<ul> <li>Nugget (\\(c_0\\)): The discontinuous jump at the origin. Represents measurement error + micro-scale variation smaller than the sampling interval.</li> <li>Sill (\\(c_0 + c\\)): The variance of the random field.</li> <li>Range (\\(a\\)): The distance at which correlation becomes negligible.</li> </ul> <p>Critical for Deep Learning: The effective range determines the minimum required buffer size between training and validation chips. If you split your dataset into Train/Val with a buffer \\(d &lt; a\\), your validation score is contaminated by leakage.</p>"},{"location":"theory/spatial_autocorrelation/#5-the-impact-on-cross-validation","title":"5. The Impact on Cross-Validation","text":"<p>Standard K-Fold Cross-Validation (CV) assumes independence. In the presence of autocorrelation, random K-Fold leads to Optimism Bias.</p>"},{"location":"theory/spatial_autocorrelation/#51-the-bias-mechanism","title":"5.1 The Bias Mechanism","text":"<p>Let error \\(\\epsilon \\sim N(0, \\Sigma)\\), where \\(\\Sigma\\) is not diagonal (non-zero off-diagonal elements). If \\(i \\in \\text{Train}\\) and \\(j \\in \\text{Test}\\) are close, \\(\\rho_{ij}\\) is high. The model \"remembers\" \\(i\\) and essentially \"interpolates\" \\(j\\) rather than \"generalizing\" to it.</p> <p>Studies (e.g., Roberts et al., 2017) show that random CV can overestimate accuracy by 20-30% in tasks like crop classification or biomass estimation.</p>"},{"location":"theory/spatial_autocorrelation/#52-block-cross-validation-blockcv","title":"5.2 Block Cross-Validation (BlockCV)","text":"<p>Algorithm: 1.  Tessellate the ROI into independent blocks \\(B_1, ..., B_K\\). 2.  Blocks must be larger than the variogram range \\(a\\). 3.  Assign entire blocks to folds.</p>"},{"location":"theory/spatial_autocorrelation/#53-buffered-block-cross-validation","title":"5.3 Buffered Block Cross-Validation","text":"<p>Even with blocks, pixels at the edge of \\(B_{train}\\) correlate with boundary pixels of \\(B_{test}\\). Solution: Remove a \"Dead Zone\" buffer of width \\(R\\) between blocks.</p> \\[ \\text{Buffer Width} \\ge \\text{Autocorrelation Range}(\\gamma) \\] <p>Ununennium implements <code>SpatialKFold</code> which automates this buffering logic.</p>"},{"location":"theory/spatial_autocorrelation/#6-variance-inflation-and-effective-sample-size","title":"6. Variance Inflation and Effective Sample Size","text":"<p>When data is positively autocorrelated, each new sample adds less than 1 unit of information.</p>"},{"location":"theory/spatial_autocorrelation/#61-effective-sample-size-n_eff","title":"6.1 Effective Sample Size (\\(N_{eff}\\))","text":"<p>For a spatial series with autocorrelation \\(\\rho\\) at lag 1:</p> \\[ N_{eff} \\approx N \\frac{1 - \\rho}{1 + \\rho} \\] <p>Implication for Significance Testing: If you have \\(N=10,000\\) pixels but \\(\\rho=0.95\\): $$ N_{eff} \\approx 10,000 \\cdot \\frac{0.05}{1.95} \\approx 256 $$ Your standard error (\\(\\frac{\\sigma}{\\sqrt{N}}\\)) is vastly underestimated. This leads to Type I Errors (false positives) where we claim a model improvement is statistically significant when it is merely fitting the spatial noise.</p> <p>Correction: All p-values in Ununennium's reporting module are computed using \\(N_{eff}\\).</p>"},{"location":"theory/spatial_autocorrelation/#7-deep-learning-specifics-receptive-fields-and-autocorrelation","title":"7. Deep Learning Specifics: Receptive Fields and Autocorrelation","text":"<p>CNNs are explicitly designed to exploit local autocorrelation via convolution operations. However, the interplay between the network's Effective Receptive Field (ERF) and the data's Spatial Correlation Length is complex.</p>"},{"location":"theory/spatial_autocorrelation/#71-the-erf-correlation-resonance","title":"7.1 The ERF-Correlation Resonance","text":"<ul> <li> <p>Case 1: ERF &lt;&lt; Correlation Length.     The network operates entirely within a locally homogenous region. It essentially acts as a texture filter. It struggles to learn global semantics (e.g., \"this texture is part of a large lake\").</p> </li> <li> <p>Case 2: ERF &gt;&gt; Correlation Length.     The feature map integrates uncorrelated noise. While this averages out noise (good), it may dilute sharp features.</p> </li> <li> <p>Case 3: ERF \\(\\approx\\) Correlation Length.     Optimal. The network context matches the physical scale of the objects.</p> </li> </ul> <p>Architectural implication: We must tune the depth and dilation rates of the network based on the average variogram range of the target features (e.g., average field size for crop mapping). Ununennium provides tools to calculate the <code>Mean Feature Diameter</code> to guide <code>Atrous</code> rate selection in DeepLab models.</p>"},{"location":"theory/spatial_autocorrelation/#72-spatially-dependent-label-noise","title":"7.2 Spatially Dependent Label Noise","text":"<p>In EO, label noise is rarely uniform (white noise). It is usually spatially structured: *   A cloud mask algorithm fails on an entire cloud (blob of error). *   A lazy annotator labels a whole region loosely.</p> <p>This structured noise is highly dangerous because a CNN can easily learn to model the spatial structure of the noise itself (e.g., \"blob-like error patterns\").</p> <p>Mitigation: <code>SpatialLabelSmoothing</code>. Instead of uniform smoothing \\(\\epsilon\\), we smooth based on the local variance of labels in the neighborhood.</p>"},{"location":"theory/spatial_autocorrelation/#8-ununennium-implementation-details","title":"8. Ununennium Implementation Details","text":""},{"location":"theory/spatial_autocorrelation/#81-the-ununenniumstatsautocorrelation-module","title":"8.1 The <code>ununennium.stats.autocorrelation</code> Module","text":"<p>We provide optimized, CUDA-accelerated implementations of Moran's I and Geary's C.</p> <pre><code>def moran_index(\n    tensor: torch.Tensor,\n    weights: torch.Tensor | None = None,\n    reduction: str = 'mean'\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes Global Moran's I for a batch of spatial tensors.\n\n    Args:\n        tensor: Input tensor (B, C, H, W)\n        weights: Spatial weight matrix (N, N). If None, \n                 a Gaussian kernel is computed on the fly.\n    ...\n    \"\"\"\n    # 1. Standardize (Z-score)\n    mean = tensor.mean(dim=(2, 3), keepdim=True)\n    var = tensor.var(dim=(2, 3), keepdim=True)\n    z = (tensor - mean) / (torch.sqrt(var) + 1e-6)\n\n    # 2. Compute W * Z (Spatial Lag)\n    # Implemented via convolution for speed on grids\n    kernel = _generate_gaussian_kernel()\n    z_lag = F.conv2d(z, kernel, padding='same')\n\n    # 3. Compute I = (Z * Z_lag).sum / Z^2.sum\n    numerator = (z * z_lag).sum(dim=(2, 3))\n    denominator = (z * z).sum(dim=(2, 3))\n\n    return (len(tensor) * numerator) / (W.sum() * denominator)\n</code></pre>"},{"location":"theory/spatial_autocorrelation/#82-spatially-weighted-loss-function","title":"8.2 Spatially-Weighted Loss Function","text":"<p>To counter the redundancy of autocorrelated samples, we re-weight the loss function using the inverse of Local Moran's \\(I\\).</p> \\[ \\mathcal{L}_{final} = \\frac{1}{B H W} \\sum_{b,h,w} \\omega_{bhw} \\cdot \\ell(y_{bhw}, \\hat{y}_{bhw}) \\] \\[ \\omega_{bhw} \\propto \\frac{1}{|I_i| + \\epsilon} \\] <ul> <li>High \\(I_i\\) (Redundant): \\(\\omega \\to 0\\). The model learns less from highly clustered, repetitive data.</li> <li>Low \\(I_i\\) (Unique): \\(\\omega \\to 1\\). The model focuses on edges, transitions, and outliers.</li> </ul>"},{"location":"theory/spatial_autocorrelation/#9-conclusion","title":"9. Conclusion","text":"<p>Spatial autocorrelation is not a nuisance to be ignored, but a fundamental property of the physical world. Ignoring it leads to: 1.  Leaked Test Sets: Overestimated performance. 2.  Inefficient Sampling: Wasted compute on redundant data. 3.  Biased Models: Failure to generalize to new geographies.</p> <p>By explicitly modeling autocorrelation via the Variogram, employing Block Cross-Validation, and using Spatially-Weighted Losses, Ununennium ensures that the \"State-of-the-Art\" metrics reported are not just statistical artifacts, but real-world capability.</p>"},{"location":"theory/spatial_autocorrelation/#10-references","title":"10. References","text":"<ol> <li>Tobler, W. R. (1970). \"A Computer Movie Simulating Urban Growth in the Detroit Region\". Economic Geography, 46(sup1), 234-240.</li> <li>Moran, P. A. P. (1950). \"Notes on Continuous Stochastic Phenomena\". Biometrika, 37(1/2), 17-23.</li> <li>Geary, R. C. (1954). \"The Contiguity Ratio and Statistical Mapping\". The Incorporated Statistician, 5(3), 115-145.</li> <li>Openshaw, S. (1984). The Modifiable Areal Unit Problem. CATMOG 38. Norwich: Geo Books.</li> <li>Anselin, L. (1995). \"Local Indicators of Spatial Association\u2014LISA\". Geographical Analysis, 27(2), 93-115.</li> <li>Roberts, D. R., et al. (2017). \"Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure\". Ecography, 40(8), 913-929.</li> <li>Wadoux, A., et al. (2021). \"Spatial sampling design for machine learning inference of soil properties\". Geoderma, 385, 114890.</li> </ol>"},{"location":"theory/uncertainty/","title":"Uncertainty Quantification in Geospatial AI: A Theoretical Treatise","text":""},{"location":"theory/uncertainty/#abstract","title":"Abstract","text":"<p>In safety-critical EO applications\u2014such as disaster response, autonomous aerial navigation, and climate policy enforcement\u2014a point prediction (\\(\\hat{y}\\)) is insufficient and potentially dangerous. Decision-makers require a rigorous quantification of confidence. However, standard Deep Neural Networks (DNNs) are notoriously overconfident, often assigning \\(99\\%\\) probability to incorrect predictions on out-of-distribution (OOD) data. This document explores the theoretical decomposition of uncertainty into Aleatoric and Epistemic components and details the advanced Bayesian and conformal prediction methods implemented in Ununennium to calibrate these estimates.</p>"},{"location":"theory/uncertainty/#1-the-anatomy-of-uncertainty","title":"1. The Anatomy of Uncertainty","text":"<p>Formally, given input \\(x\\) and output \\(y\\), we seek the predictive posterior distribution \\(P(y|x, D)\\), where \\(D\\) is the training data.</p> \\[ P(y|x, D) = \\int P(y|x, \\theta) P(\\theta|D) d\\theta \\] <p>This marginalization over parameters \\(\\theta\\) is intractable for modern DNNs. We approximate detailed components:</p>"},{"location":"theory/uncertainty/#11-aleatoric-uncertainty-data-noise","title":"1.1 Aleatoric Uncertainty (Data Noise)","text":"<p>The uncertainty inherent in the observation process. Even with infinite data and a perfect model, this cannot be reduced. *   Homoscedastic: Constant noise variance \\(\\sigma^2\\) (e.g., thermal sensor noise). *   Heteroscedastic: Noise variance \\(\\sigma(x)^2\\) depends on input.     *   Example: Clouds, shadows, or mixed pixels (land/water border) have higher ambiguity than clear pixels.</p>"},{"location":"theory/uncertainty/#12-epistemic-uncertainty-model-knowledge","title":"1.2 Epistemic Uncertainty (Model Knowledge)","text":"<p>The uncertainty in the model parameters \\(\\theta\\) due to lack of knowledge (finite data). *   Reducible: Can be reduced by observing more data in the sparse region of the manifold. *   Indicator of OOD: High epistemic uncertainty implies the input \\(x\\) is far from the training distribution \\(P_{train}(x)\\).     *   Example: A model trained on European cities receiving an input from the Sahara Desert.</p>"},{"location":"theory/uncertainty/#2-modeling-aleatoric-uncertainty","title":"2. Modeling Aleatoric Uncertainty","text":"<p>We model the output not as a deterministic value but as a distribution (e.g., Gaussian for regression).</p>"},{"location":"theory/uncertainty/#21-heteroscedastic-regression-loss","title":"2.1 Heteroscedastic Regression Loss","text":"<p>The network predicts two heads: mean \\(\\mu(x)\\) and variance \\(\\sigma^2(x)\\).</p> <p>The Negative Log Likelihood (NLL) for a Gaussian:</p> \\[ \\mathcal{L}_{NLL} = -\\log P(y | \\mu, \\sigma) = \\frac{1}{2\\sigma^2} \\| y - \\mu \\|^2 + \\frac{1}{2} \\log \\sigma^2 + \\text{const} \\] <ul> <li>Interpretation:<ul> <li>The first term is the MSE weighted by precision (\\(1/\\sigma^2\\)). This allows the model to \"attenuate\" the loss on noisy samples by predicting high \\(\\sigma^2\\).</li> <li>The second term (\\(\\log \\sigma^2\\)) prevents the model from predicting infinite variance (collapse).</li> </ul> </li> </ul> <p>Numerical Stability: To avoid division by zero or negative variance, we usually predict \\(s = \\log \\sigma^2\\) and optimize: $$ \\mathcal{L} = \\frac{1}{2} \\exp(-s) | y - \\mu |^2 + \\frac{1}{2} s $$</p>"},{"location":"theory/uncertainty/#3-modeling-epistemic-uncertainty-bayesian-dl","title":"3. Modeling Epistemic Uncertainty (Bayesian DL)","text":"<p>Since exact Bayesian inference is intractable, we use Variational Inference (VI) or functional approximations.</p>"},{"location":"theory/uncertainty/#31-monte-carlo-mc-dropout-gal-ghahramani-2016","title":"3.1 Monte Carlo (MC) Dropout (Gal &amp; Ghahramani, 2016)","text":"<p>Dropout, usually viewed as regularization, can be interpreted as a variational Bayesian approximation. *   Training: Dropout active. *   Inference: Dropout kept active. We perform \\(T\\) stochastic forward passes \\(\\{\\hat{y}_t\\}_{t=1}^T\\).</p> <p>Predictive Mean: $$ \\mathbb{E}[y] \\approx \\frac{1}{T} \\sum_{t=1}^T \\hat{y}_t $$</p> <p>Predictive Entropy (Classification): $$ H(y|x) = - \\sum_{c=1}^C p_{avg}(c) \\log p_{avg}(c) $$ Where \\(p_{avg}\\) is the numerical average of Softmax vectors.</p> <p>Predictive Variance (Regression): $$ \\text{Var}(y) \\approx \\underbrace{\\frac{1}{T} \\sum \\sigma^2_t}{\\text{Aleatoric}} + \\underbrace{\\frac{1}{T} \\sum (\\mu_t - \\bar{\\mu})^2}{\\text{Epistemic}} $$</p>"},{"location":"theory/uncertainty/#32-deep-ensembles-lakshminarayanan-et-al-2017","title":"3.2 Deep Ensembles (Lakshminarayanan et al., 2017)","text":"<p>Train \\(M\\) models independently with random initialization and shuffled data. *   Pros: Empirically the Gold Standard. Better calibration and OOD detection than MC Dropout. *   Cons: \\(M \\times\\) training and inference cost (computationally heavy). *   Ununennium Strategy: We support \"Snapshots Ensembles\" (Cyclic Learning Rate) to get ensemble diversity within a single training run.</p>"},{"location":"theory/uncertainty/#33-evidential-deep-learning-edl-sensoy-et-al-2018","title":"3.3 Evidential Deep Learning (EDL) (Sensoy et al., 2018)","text":"<p>This deterministic method places a distribution over the distribution (Dirichlet Distribution for classification). The network predicts evidence \\(e_k \\ge 0\\). $$ \\alpha_k = e_k + 1 $$ $$ S = \\sum \\alpha_k $$ $$ \\text{Expected Probability } \\hat{p}_k = \\alpha_k / S $$ $$ \\text{Uncertainty (Vacuity) } u = K / S $$</p> <ul> <li>Mechanism: High evidence (lots of support) \\(\\to\\) High \\(S\\) \\(\\to\\) Low \\(u\\).</li> <li>Effect: A pure OOD sample will have \\(e_k \\approx 0\\) for all classes, so \\(u \\to 1\\).</li> <li>Loss: KL-divergence between predicted Dirichlet and Uniform Dirichlet + Accuracy term.</li> </ul>"},{"location":"theory/uncertainty/#4-calibration","title":"4. Calibration","text":"<p>A model is Calibrated if its predicted confidence matches its empirical accuracy. $$ P(Y=y | \\hat{P}=p) = p $$</p>"},{"location":"theory/uncertainty/#41-reliability-diagrams","title":"4.1 Reliability Diagrams","text":"<p>Plot Accuracy vs Confidence in bins (e.g., 0-0.1, 0.1-0.2, ...). *   Perfect Calibration: \\(y=x\\) diagonal line. *   Under-confident: Above diagonal. *   Over-confident: Below diagonal (Typical DNN behavior).</p>"},{"location":"theory/uncertainty/#42-expected-calibration-error-ece","title":"4.2 Expected Calibration Error (ECE)","text":"<p>Weighted average gap.</p> \\[ \\text{ECE} = \\sum_{m=1}^M \\frac{|B_m|}{N} | \\text{acc}(B_m) - \\text{conf}(B_m) | \\]"},{"location":"theory/uncertainty/#43-temperature-scaling-platt-scaling","title":"4.3 Temperature Scaling (Platt Scaling)","text":"<p>A post-processing step to fix calibration on a validation set. $$ \\hat{q}i = \\max_k \\sigma{SM}(\\mathbf{z}_i / T)^{(k)} $$ We learn a single scalar parameter \\(T\\) (Temperature) that softens (\\(T&gt;1\\)) or sharpens (\\(T&lt;1\\)) the logits \\(\\mathbf{z}\\) to minimize NLL. *   Note: Does not change accuracy (rank preserving), only confidence values.</p>"},{"location":"theory/uncertainty/#5-conformal-prediction-cp","title":"5. Conformal Prediction (CP)","text":"<p>Bayesian methods provide a score of uncertainty but no guarantees. Conformal Prediction provides rigorous statistical guarantees of finite-sample validity.</p> <p>\"We guarantee with probability \\(1-\\alpha\\) that the true label \\(y\\) is contained in the predicted set \\(\\mathcal{C}(x)\\).\"</p>"},{"location":"theory/uncertainty/#51-inductive-conformal-prediction-icp","title":"5.1 Inductive Conformal Prediction (ICP)","text":"<ol> <li>Calibration Set: Hold out a set \\(\\{(x_i, y_i)\\}\\).</li> <li>Non-Conformity Score (\\(s_i\\)): e.g., \\(s_i = 1 - \\hat{P}(y_i | x_i)\\). (How \"weird\" is the true label given the model?).</li> <li>Quantile: Compute \\(\\hat{q}\\) as the \\(\\lceil (N+1)(1-\\alpha) \\rceil / N\\) quantile of scores \\(s_i\\).</li> <li>Prediction: For test \\(x_{new}\\), include all classes \\(k\\) where \\(1 - \\hat{P}(k|x_{new}) \\le \\hat{q}\\).</li> </ol> <p>Result: The model outputs a set of classes (e.g., <code>{\"Forest\", \"Shrub\"}</code>). *   Easy sample \\(\\to\\) Set size 1. *   Hard sample \\(\\to\\) Set size 5. *   OOD sample \\(\\to\\) Set size \\(C\\) (all classes).</p> <p>Ununennium implements <code>ConformalClassifier</code> wrappers that transform any PyTorch model into a set predictor with coverage guarantees.</p>"},{"location":"theory/uncertainty/#6-applications-in-earth-observation","title":"6. Applications in Earth Observation","text":""},{"location":"theory/uncertainty/#61-domain-adaptation","title":"6.1 Domain Adaptation","text":"<p>We use Epistemic Uncertainty heatmaps to detect when the satellite passes over a new biome. $$ \\text{if } \\text{mean}(H(y)) &gt; \\tau \\implies \\text{Trigger Retraining/Labeling} $$</p>"},{"location":"theory/uncertainty/#62-reliable-mapping","title":"6.2 Reliable Mapping","text":"<p>Flood mapping requires high precision. We threshold the uncertainty map: *   Mask: \\(y_{out} = \\text{bg}\\) if \\(u(x) &gt; \\tau\\) else \\(\\operatorname{argmax} P(y|x)\\). *   This creates \"No Data\" holes in the map where the model is unsure, rather than guessing.</p>"},{"location":"theory/uncertainty/#63-active-learning","title":"6.3 Active Learning","text":"<p>The <code>ununennium.active_learning</code> module prioritizes labeling patches with high Epistemic Uncertainty (maximizing information gain).</p>"},{"location":"theory/uncertainty/#7-ununennium-api","title":"7. Ununennium API","text":"<pre><code>from ununennium.models import DeepLabV3Plus\nfrom ununennium.uncertainty import BayesianWrapper\n\n# 1. Wrap model\nmodel = DeepLabV3Plus(..., dropout_rate=0.2)\nbayesian_model = BayesianWrapper(model, method=\"mc_dropout\", n_samples=30)\n\n# 2. Predict\nmean, entropy, variance = bayesian_model(input_tensor)\n\n# 3. Calibrate\ncalibrator = TemperatureScaling()\ncalibrator.fit(val_logits, val_labels)\ncalibrated_probs = calibrator.transform(test_logits)\n</code></pre>"},{"location":"theory/uncertainty/#8-references","title":"8. References","text":"<ol> <li>Gal, Y., &amp; Ghahramani, Z. (2016). \"Dropout as a bayesian approximation: Representing model uncertainty in deep learning\". ICML.</li> <li>Lakshminarayanan, B., et al. (2017). \"Simple and scalable predictive uncertainty estimation using deep ensembles\". NeurIPS.</li> <li>Kendall, A., &amp; Gal, Y. (2017). \"What uncertainties do we need in bayesian deep learning for computer vision?\". NeurIPS.</li> <li>Sensoy, M., et al. (2018). \"Evidential deep learning to quantify classification uncertainty\". NeurIPS.</li> <li>Angelopoulos, A. N., &amp; Bates, S. (2021). \"A gentle introduction to conformal prediction and distribution-free uncertainty quantification\". arXiv.</li> </ol>"},{"location":"tutorials/custom_dataset/","title":"Custom Dataset Tutorial","text":"<p>Create custom datasets for your satellite imagery.</p>"},{"location":"tutorials/custom_dataset/#inherit-from-geodataset","title":"Inherit from GeoDataset","text":"<pre><code>from ununennium.datasets.base import GeoDataset\nfrom ununennium.core import GeoTensor\nimport ununennium as uu\n\nclass MyDataset(GeoDataset):\n    def __init__(self, image_paths, label_paths):\n        self.image_paths = image_paths\n        self.label_paths = label_paths\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = uu.io.read_geotiff(self.image_paths[idx])\n        label = uu.io.read_geotiff(self.label_paths[idx])\n        return image, label.data.squeeze().long()\n\n    @property\n    def crs(self):\n        return \"EPSG:32632\"\n</code></pre>"},{"location":"tutorials/custom_dataset/#use-with-dataloader","title":"Use with DataLoader","text":"<pre><code>from torch.utils.data import DataLoader\n\ndataset = MyDataset(image_paths, label_paths)\nloader = DataLoader(dataset, batch_size=8, shuffle=True)\n</code></pre>"},{"location":"tutorials/distributed_training/","title":"Distributed Training Tutorial","text":"<p>Scale training across multiple GPUs.</p>"},{"location":"tutorials/distributed_training/#single-node-multi-gpu","title":"Single Node Multi-GPU","text":"<pre><code>torchrun --nproc_per_node=4 train.py\n</code></pre>"},{"location":"tutorials/distributed_training/#training-script","title":"Training Script","text":"<pre><code>import torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# Initialize\ndist.init_process_group(\"nccl\")\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\ntorch.cuda.set_device(local_rank)\n\n# Model\nmodel = create_model(\"unet_resnet50\", in_channels=12, num_classes=10)\nmodel = model.cuda()\nmodel = DDP(model, device_ids=[local_rank])\n\n# Train normally\ntrainer = Trainer(model=model, ...)\ntrainer.fit(epochs=100)\n</code></pre>"},{"location":"tutorials/gan_training/","title":"GAN Training Tutorial","text":"<p>Train GANs for image translation.</p>"},{"location":"tutorials/gan_training/#pix2pix-training","title":"Pix2Pix Training","text":"<pre><code>from ununennium.models.gan import Pix2Pix\nimport torch\n\nmodel = Pix2Pix(in_channels=12, out_channels=3)\nopt_g = torch.optim.Adam(model.generator.parameters(), lr=2e-4)\nopt_d = torch.optim.Adam(model.discriminator.parameters(), lr=2e-4)\n\nfor epoch in range(epochs):\n    for real_a, real_b in dataloader:\n        # Train generator\n        opt_g.zero_grad()\n        g_losses = model.compute_generator_loss(real_a, real_b)\n        g_losses[\"total\"].backward()\n        opt_g.step()\n\n        # Train discriminator\n        opt_d.zero_grad()\n        d_losses = model.compute_discriminator_loss(\n            real_a, real_b, g_losses[\"fake_b\"].detach()\n        )\n        d_losses[\"total\"].backward()\n        opt_d.step()\n</code></pre>"},{"location":"tutorials/pinn_training/","title":"PINN Training Tutorial","text":"<p>Train Physics-Informed Neural Networks.</p>"},{"location":"tutorials/pinn_training/#setup","title":"Setup","text":"<pre><code>from ununennium.models.pinn import PINN, DiffusionEquation, MLP, UniformSampler\nimport torch\n\n# Define equation\nequation = DiffusionEquation(diffusivity=0.1)\n\n# Create network\nnetwork = MLP([2, 64, 64, 64, 1], activation=\"tanh\")\n\n# Create PINN\npinn = PINN(\n    network=network,\n    equation=equation,\n    lambda_data=1.0,\n    lambda_pde=10.0,\n)\n\n# Collocation sampler\nsampler = UniformSampler(bounds=[(0, 1), (0, 1)])\n\n# Training\noptimizer = torch.optim.Adam(pinn.parameters(), lr=1e-3)\n\nfor epoch in range(epochs):\n    x_collocation = sampler.sample(1000)\n\n    losses = pinn.compute_loss(\n        x_data=x_data,\n        u_data=u_data,\n        x_collocation=x_collocation,\n    )\n\n    optimizer.zero_grad()\n    losses[\"total\"].backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/quickstart/","title":"Quickstart Tutorial","text":"<p>Get started with Ununennium in 5 minutes.</p>"},{"location":"tutorials/quickstart/#installation","title":"Installation","text":"<pre><code>pip install \"ununennium[all]\"\n</code></pre>"},{"location":"tutorials/quickstart/#load-satellite-imagery","title":"Load Satellite Imagery","text":"<pre><code>import ununennium as uu\n\ntensor = uu.io.read_geotiff(\"path/to/image.tif\")\nprint(f\"Shape: {tensor.shape}, CRS: {tensor.crs}\")\n</code></pre>"},{"location":"tutorials/quickstart/#create-a-model","title":"Create a Model","text":"<pre><code>model = uu.models.create(\"unet_resnet50\", in_channels=12, num_classes=10)\n</code></pre>"},{"location":"tutorials/quickstart/#make-predictions","title":"Make Predictions","text":"<pre><code>import torch\n\nwith torch.no_grad():\n    output = model(tensor.data.unsqueeze(0))\n    prediction = output.argmax(dim=1)\n</code></pre>"},{"location":"tutorials/training_segmentation/","title":"Semantic Segmentation Tutorial","text":"<p>Train a U-Net for land cover classification.</p>"},{"location":"tutorials/training_segmentation/#setup","title":"Setup","text":"<pre><code>from ununennium.models import create_model\nfrom ununennium.training import Trainer, CheckpointCallback\nfrom ununennium.datasets import SyntheticDataset\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\n\n# Dataset\ntrain_ds = SyntheticDataset(num_samples=1000, task=\"segmentation\")\nval_ds = SyntheticDataset(num_samples=200, task=\"segmentation\")\n\ntrain_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=8)\n\n# Model\nmodel = create_model(\"unet_resnet50\", in_channels=12, num_classes=10)\n\n# Training\ntrainer = Trainer(\n    model=model,\n    optimizer=torch.optim.AdamW(model.parameters(), lr=1e-4),\n    loss_fn=nn.CrossEntropyLoss(),\n    train_loader=train_loader,\n    val_loader=val_loader,\n)\n\ntrainer.fit(epochs=50)\n</code></pre>"}]}